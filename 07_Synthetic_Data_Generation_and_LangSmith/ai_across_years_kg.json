{
  "nodes": [
    {
      "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
      "properties": {
        "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
        "document_metadata": {
          "source": "data/2024_llms.html"
        },
        "headlines": [
          "The GPT-4 barrier was comprehensively broken",
          "Some of those GPT-4 models run on my laptop",
          "LLM prices crashed, thanks to competition and increased efficiency",
          "Multimodal vision is common, audio and video are starting to emerge",
          "Prompt driven app generation is a commodity already"
        ],
        "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
        "summary_embedding": [
          -0.014540364034473896,
          0.0007438954198732972,
          0.005369548685848713,
          -0.02401237189769745,
          -0.003787418594583869,
          0.017005302011966705,
          -0.005504566244781017,
          0.010136710479855537,
          -0.03810960054397583,
          -0.04378726705908775,
          0.029606949537992477,
          0.027322035282850266,
          -0.01417339313775301,
          -0.007976428605616093,
          0.005265688989311457,
          0.017213020473718643,
          0.021533586084842682,
          0.01747613213956356,
          0.013965672813355923,
          -0.004649454262107611,
          -0.014969650655984879,
          -0.005141057074069977,
          -0.014415732584893703,
          -0.02142280340194702,
          0.004763700067996979,
          0.0012904573231935501,
          0.02474631369113922,
          -0.04162698611617088,
          -0.01675603911280632,
          0.019317911937832832,
          0.011410723440349102,
          0.00557380635291338,
          -0.04154389724135399,
          0.00294788577593863,
          -0.017019148916006088,
          -0.009001177735626698,
          0.013342514634132385,
          -0.00898732990026474,
          0.015260457992553711,
          -0.022101353853940964,
          0.00837801955640316,
          0.018376249819993973,
          0.017628459259867668,
          -0.02135356329381466,
          -0.02052268572151661,
          0.017545372247695923,
          0.01775309257209301,
          -0.004822554066777229,
          -0.027502059936523438,
          0.009264289401471615,
          0.014609603211283684,
          0.035035353153944016,
          -0.009409692138433456,
          -0.01184693444520235,
          -0.007747936528176069,
          0.016077488660812378,
          -0.005400706548243761,
          0.011902326717972755,
          0.01841779425740242,
          -0.0061796545051038265,
          -0.0005028542364016175,
          0.004237477667629719,
          -0.01563435234129429,
          0.035589270293712616,
          -0.018736297264695168,
          -0.009063493460416794,
          -0.02701738104224205,
          0.02253063954412937,
          0.02163052186369896,
          -0.016936061903834343,
          0.002709008287638426,
          0.011071448214352131,
          0.014415732584893703,
          -0.015828223899006844,
          0.010967588983476162,
          0.004538670647889376,
          -0.005040659569203854,
          -0.011293016374111176,
          -0.03273659199476242,
          0.015163522213697433,
          0.01042751781642437,
          -0.008177223615348339,
          0.005459560547024012,
          0.02186593785881996,
          0.01730995625257492,
          0.01172922644764185,
          0.01231776550412178,
          -0.012047730386257172,
          -0.015038890764117241,
          -0.013716409914195538,
          -0.005975397303700447,
          0.0022537563927471638,
          0.017171477898955345,
          0.008544194512069225,
          -0.0064912340603768826,
          -0.0017413817113265395,
          0.003489687340334058,
          0.012303917668759823,
          -0.007664849050343037,
          -0.029136119410395622,
          -0.004846788011491299,
          -0.0055807302705943584,
          -0.037915728986263275,
          -0.007810252718627453,
          -0.02085503563284874,
          0.000979743548668921,
          0.01120300404727459,
          0.013557158410549164,
          0.009312757290899754,
          0.011293016374111176,
          -0.01658986322581768,
          0.017573067918419838,
          0.011556127108633518,
          -0.020993515849113464,
          -0.008551118895411491,
          -0.009319680742919445,
          -0.0009304101695306599,
          -0.020051853731274605,
          0.007526369299739599,
          0.004961033817380667,
          0.034813784062862396,
          0.004954109899699688,
          0.02185208909213543,
          0.0022537563927471638,
          0.013321742415428162,
          0.0008992522489279509,
          -0.0032663887832313776,
          -0.002058153972029686,
          -0.013564081862568855,
          0.004736004397273064,
          -0.005411092657595873,
          0.0060654086992144585,
          0.01201311033219099,
          0.004677150398492813,
          -0.030382435768842697,
          0.027322035282850266,
          -0.014471123926341534,
          -0.005947701167315245,
          -0.013709485530853271,
          -0.007858720608055592,
          0.001367486547678709,
          0.02258603274822235,
          -0.006678181234747171,
          0.014554211869835854,
          -0.004725618287920952,
          0.028776071965694427,
          0.004292869474738836,
          0.004767162259668112,
          0.005258765071630478,
          0.006896286737173796,
          0.0030188565142452717,
          -0.029219206422567368,
          -0.01686682179570198,
          0.020951971411705017,
          0.016506774351000786,
          0.004448658786714077,
          0.015329698100686073,
          0.03154566511511803,
          -0.031711842864751816,
          -0.014235708862543106,
          -0.013764877803623676,
          0.004497126676142216,
          -0.0020927737932652235,
          0.011860782280564308,
          0.014941954985260963,
          0.024261634796857834,
          0.025258690118789673,
          0.00151462119538337,
          -0.0046598403714597225,
          -0.010946816764771938,
          -0.010939892381429672,
          0.024372419342398643,
          -0.04076841101050377,
          0.010926044546067715,
          -0.016008248552680016,
          0.01132071204483509,
          0.0229045357555151,
          -0.007678696885704994,
          -0.008564966730773449,
          -0.03317972645163536,
          0.007512521464377642,
          0.013204035349190235,
          -0.003808190580457449,
          0.0019664110150188208,
          -0.0067958892323076725,
          -0.005321080796420574,
          0.017656156793236732,
          0.00022589493892155588,
          0.0013345977058634162,
          -0.013993369415402412,
          0.014789626933634281,
          0.006671257317066193,
          0.0022866453509777784,
          -0.025757215917110443,
          -0.6518514156341553,
          -0.019816439598798752,
          0.015218914486467838,
          -0.010385974310338497,
          0.0029773125424981117,
          0.003157336264848709,
          -0.005514952354133129,
          0.006473924033343792,
          -0.007131702266633511,
          0.03772185742855072,
          -0.017462285235524178,
          0.005511490162461996,
          -0.0173653494566679,
          -0.004805244039744139,
          0.010829108767211437,
          -0.007187094073742628,
          4.5249307731864974e-05,
          -0.0248017068952322,
          0.00014269896200858057,
          0.020771948620676994,
          -0.010171330533921719,
          0.017794635146856308,
          -0.014131848700344563,
          -0.0017431126907467842,
          -0.031767234206199646,
          9.926178609021008e-05,
          0.006269666366279125,
          0.013744105584919453,
          -0.0068374332040548325,
          0.00859958678483963,
          -0.00831570290029049,
          0.008281083777546883,
          -0.013162490911781788,
          -0.0008624686161056161,
          0.04891101270914078,
          -0.0033442836720496416,
          -0.019497934728860855,
          0.020314965397119522,
          0.0056534321047365665,
          0.012206981889903545,
          -0.038054209202528,
          -0.006089642643928528,
          -0.016769886016845703,
          0.020896580070257187,
          -0.00209450488910079,
          0.03154566511511803,
          0.022835295647382736,
          0.006023865193128586,
          -0.017046846449375153,
          -0.009742043912410736,
          0.00453520892187953,
          0.02351384609937668,
          0.0015076972777023911,
          0.004677150398492813,
          -0.00037908804370090365,
          0.019110191613435745,
          0.02390158921480179,
          -0.011923098005354404,
          0.01042751781642437,
          -0.007086696568876505,
          0.0005305501981638372,
          -0.011653062887489796,
          -0.025037121027708054,
          -0.01520506665110588,
          -0.033041246235370636,
          0.013986445032060146,
          -0.00019116683688480407,
          0.0032819679472595453,
          0.01179154310375452,
          -0.030908659100532532,
          -0.008862697519361973,
          0.038912784308195114,
          0.006657409481704235,
          -0.0033512075897306204,
          -0.00990129541605711,
          0.002290107309818268,
          -0.0004160880926065147,
          -0.014803474768996239,
          0.0009485856280662119,
          0.00638044998049736,
          0.018625514581799507,
          -0.01780848391354084,
          0.011784618720412254,
          -0.009465084411203861,
          0.036863286048173904,
          -0.011853858828544617,
          0.009340452961623669,
          -0.001243720413185656,
          -0.013273275457322598,
          -0.005643045995384455,
          0.013294046744704247,
          0.027446668595075607,
          -0.0162575114518404,
          -0.04375956952571869,
          -0.007844872772693634,
          0.014443428255617619,
          -0.005577268078923225,
          0.020979667082428932,
          0.0110160568729043,
          -0.013003239408135414,
          -0.004081687889993191,
          -0.00756098935380578,
          0.031185617670416832,
          0.028886856511235237,
          0.027003532275557518,
          0.025535648688673973,
          -0.009298908524215221,
          0.012186209671199322,
          0.01248394139111042,
          -0.02613111026585102,
          0.000608877744525671,
          0.01197849027812481,
          0.00284402584657073,
          -0.02434472367167473,
          -0.013681789860129356,
          -0.027100468054413795,
          -0.0005387724377214909,
          -0.006512005813419819,
          -0.0012982467887923121,
          -0.02424778789281845,
          0.015246610157191753,
          0.00241993204690516,
          0.022876838222146034,
          -0.0040886118076741695,
          -0.007464053574949503,
          0.014900410547852516,
          0.008994253352284431,
          -0.002598224440589547,
          -0.009894371032714844,
          0.024677075445652008,
          -0.009478932246565819,
          -0.029302295297384262,
          0.025480257347226143,
          -0.024774011224508286,
          0.029080728068947792,
          0.012906303629279137,
          0.040076013654470444,
          -0.0054941801354289055,
          0.018334707245230675,
          0.0025497565511614084,
          -0.029413077980279922,
          -0.004559442866593599,
          -0.004310179501771927,
          0.006286976393312216,
          -0.04245786368846893,
          -0.0262280460447073,
          -0.011479963548481464,
          0.004296331200748682,
          0.001073217368684709,
          -0.033428989350795746,
          -0.01897171325981617,
          -0.014014140702784061,
          -0.013674866408109665,
          0.02496788278222084,
          -0.002310879295691848,
          -0.006820123177021742,
          -0.015371241606771946,
          -0.027031227946281433,
          -0.014568059705197811,
          -0.03248732537031174,
          0.008156451396644115,
          0.0008096732199192047,
          0.0007932287990115583,
          0.012601648457348347,
          0.015080434270203114,
          -0.02629728615283966,
          0.000766398326959461,
          0.023153798654675484,
          -0.037306420505046844,
          -0.023056862875819206,
          0.003399675479158759,
          -0.041959334164857864,
          0.0031971491407603025,
          0.019456392154097557,
          -0.013681789860129356,
          0.012006185948848724,
          0.004704846069216728,
          0.0017898496007546782,
          0.015066586434841156,
          -0.010434442199766636,
          0.013529462739825249,
          0.026172654703259468,
          0.0126778120175004,
          -0.008343399502336979,
          0.023098407313227654,
          0.02275220677256584,
          0.04785856977105141,
          0.014159544371068478,
          -0.007221714127808809,
          0.01534354593604803,
          0.02196287363767624,
          0.03063170053064823,
          -0.013107099570333958,
          -0.000241690271650441,
          -0.012802444398403168,
          0.021325867623090744,
          -0.002593031618744135,
          -0.0012601648923009634,
          0.016742190346121788,
          0.013529462739825249,
          0.01075986959040165,
          0.028609896078705788,
          0.010912196710705757,
          -0.013771802186965942,
          0.00873806606978178,
          -0.0157451368868351,
          0.002091042697429657,
          -0.022392161190509796,
          -0.001155439647845924,
          0.018293162807822227,
          0.008710370399057865,
          0.0012558373855426908,
          -0.004178623668849468,
          -0.029413077980279922,
          -0.0009269482106901705,
          0.03334590047597885,
          0.0025687976740300655,
          0.013806421309709549,
          -0.02268296852707863,
          0.004098997917026281,
          -0.004767162259668112,
          0.0070590004324913025,
          0.018445489928126335,
          -0.008398790843784809,
          -0.014180316589772701,
          0.008571891114115715,
          -0.02395698055624962,
          0.02279375120997429,
          0.014291100203990936,
          -0.029357686638832092,
          0.0034065996296703815,
          0.017102237790822983,
          0.025424864143133163,
          0.0204119011759758,
          0.00998438335955143,
          0.004732542205601931,
          0.02019033394753933,
          -0.0009529131348244846,
          0.036863286048173904,
          -0.003777032718062401,
          -0.011563051491975784,
          0.0060723330825567245,
          0.004756776150316,
          -0.0062246606685221195,
          0.024926338344812393,
          -0.017393045127391815,
          0.035977013409137726,
          -0.00837801955640316,
          -0.0234169103205204,
          -0.004237477667629719,
          -0.024760162457823753,
          0.0067093390971422195,
          -0.01819622702896595,
          -0.0009269482106901705,
          0.008717294782400131,
          -0.019262520596385002,
          0.015038890764117241,
          0.0066816434264183044,
          0.020868884399533272,
          0.013494842685759068,
          -0.009257365018129349,
          0.0061831166967749596,
          0.01109914481639862,
          -0.010448290035128593,
          0.006235046312212944,
          -0.007581761106848717,
          -0.003593547036871314,
          -0.0045906007289886475,
          -0.0037527987733483315,
          -0.011957718059420586,
          -0.011881554499268532,
          -0.021602826192975044,
          0.010856805369257927,
          -0.038248080760240555,
          0.015094282105565071,
          0.006003092974424362,
          0.005639583803713322,
          0.008170299232006073,
          0.013501766137778759,
          0.02496788278222084,
          -0.004687536507844925,
          -0.04331643506884575,
          0.003326973645016551,
          -0.008481878787279129,
          -0.011313787661492825,
          -0.035256918519735336,
          -0.04425809904932976,
          0.014471123926341534,
          -0.007796404417604208,
          -0.013508690521121025,
          -0.0009598371107131243,
          0.008364170789718628,
          -0.011230699717998505,
          0.013661017641425133,
          -0.00041911733569577336,
          0.014664995484054089,
          0.026186503469944,
          -0.013820270076394081,
          -0.013100175186991692,
          -0.0215197391808033,
          0.0028942248318344355,
          0.011999262496829033,
          -0.004486741032451391,
          -0.027529755607247353,
          0.03960518166422844,
          -0.016797581687569618,
          -0.03185031935572624,
          0.011327635496854782,
          -0.0071732462383806705,
          -0.019096344709396362,
          -0.0036281670909374952,
          0.004684074316173792,
          0.0029236518312245607,
          0.007090158294886351,
          0.01581437699496746,
          0.021325867623090744,
          0.0047706239856779575,
          0.0012393929064273834,
          0.014706538990139961,
          0.019317911937832832,
          0.0014124924782663584,
          -0.03101944364607334,
          0.004974881652742624,
          0.020979667082428932,
          0.06973835825920105,
          0.02063346840441227,
          -0.012269297614693642,
          0.017213020473718643,
          -0.018099291250109673,
          -0.00990129541605711,
          -0.028097521513700485,
          -0.007858720608055592,
          0.011389951221644878,
          0.019220976158976555,
          -0.007651000749319792,
          -0.002856142818927765,
          0.019373303279280663,
          -0.009866675361990929,
          0.036530934274196625,
          0.008114907890558243,
          0.004871021956205368,
          -0.03218267112970352,
          -0.020010311156511307,
          -0.025369472801685333,
          0.007117854431271553,
          -0.011244548484683037,
          0.01636829599738121,
          0.019207127392292023,
          0.002006224123761058,
          -0.018680905923247337,
          0.028332937508821487,
          0.018763992935419083,
          -0.01570359244942665,
          -0.018320858478546143,
          0.0061831166967749596,
          0.0004245266900397837,
          0.0035589272156357765,
          0.012296993285417557,
          -0.0017777326283976436,
          0.02074425294995308,
          0.024164699018001556,
          -0.010074394755065441,
          0.015731288120150566,
          0.004057453945279121,
          0.02557719312608242,
          0.020841188728809357,
          0.0029738505836576223,
          -0.010462137870490551,
          0.005286460742354393,
          -0.02063346840441227,
          0.010115939192473888,
          0.039355918765068054,
          0.0024857097305357456,
          -0.030050085857510567,
          0.016215967014431953,
          0.007574837189167738,
          -0.030714787542819977,
          0.009423540905117989,
          0.015011195093393326,
          0.021879784762859344,
          0.009423540905117989,
          0.0012480479199439287,
          -0.010891424492001534,
          -0.011535354889929295,
          -0.015108130872249603,
          -0.028776071965694427,
          -0.006695491261780262,
          -0.020979667082428932,
          -0.020314965397119522,
          -0.010600618086755276,
          -0.00976281613111496,
          0.019373303279280663,
          -0.03185031935572624,
          0.019539479166269302,
          0.013730257749557495,
          -0.02585415169596672,
          -0.05409015342593193,
          -0.0064185322262346745,
          0.011293016374111176,
          -0.011743075214326382,
          0.016395991668105125,
          -0.013224807567894459,
          -0.010642161592841148,
          0.006501619704067707,
          -0.004524822812527418,
          -0.012387005612254143,
          -0.010295961983501911,
          -0.026172654703259468,
          0.00934737641364336,
          -0.010185178369283676,
          -0.037140242755413055,
          -0.014720387756824493,
          -0.020605772733688354,
          0.023001471534371376,
          0.009278137236833572,
          -0.010475985705852509,
          -0.0004245266900397837,
          0.011258396320044994,
          -0.011341484263539314,
          0.010185178369283676,
          0.021395105868577957,
          0.016492927446961403,
          0.003100213361904025,
          -0.029025336727499962,
          -0.014360340312123299,
          -0.022613728418946266,
          -0.027100468054413795,
          -0.020785795524716377,
          0.015066586434841156,
          0.009243517182767391,
          0.023943131789565086,
          0.016562167555093765,
          0.007533293217420578,
          -0.011466115713119507,
          0.006238508503884077,
          -0.005639583803713322,
          0.01581437699496746,
          -0.008675750344991684,
          0.014221861027181149,
          0.011355332098901272,
          0.02363847754895687,
          -0.0021291247103363276,
          0.014332644641399384,
          -0.022336767986416817,
          -0.0253556240350008,
          -0.03982675075531006,
          0.029662342742085457,
          0.00026635697577148676,
          -0.014706538990139961,
          0.02007954940199852,
          0.017240718007087708,
          0.006743959151208401,
          -0.049354150891304016,
          0.002184516517445445,
          -0.020882731303572655,
          0.035146135836839676,
          0.0035796992015093565,
          -0.015537417493760586,
          -0.01996876671910286,
          -0.002068539848551154,
          -0.019941071048378944,
          -0.0007542813546024263,
          -0.029136119410395622,
          -0.0049679577350616455,
          0.007754860911518335,
          0.015177370049059391,
          -0.01276089996099472,
          -0.018999408930540085,
          0.017116084694862366,
          -0.02485709823668003,
          -0.007657925132662058,
          0.006920520681887865,
          0.025203296914696693,
          0.040962282568216324,
          -0.0248432494699955,
          0.011701530776917934,
          -0.037527985870838165,
          -0.013287123292684555,
          0.002075463766232133,
          -0.02563258446753025,
          0.008931937627494335,
          -0.01375795342028141,
          0.031878016889095306,
          0.012026958167552948,
          0.04043605923652649,
          -0.01963641494512558,
          0.013688714243471622,
          0.013584854081273079,
          -0.0014592293882742524,
          -0.022766055539250374,
          0.0021862476132810116,
          0.007955656386911869,
          -0.015274305827915668,
          -0.00835032295435667,
          0.003552003065124154,
          -0.0017110892804339528,
          0.029551558196544647,
          0.001432398916222155,
          0.008101060055196285,
          -0.0020702709443867207,
          -0.01409030519425869,
          -0.0006365736480802298,
          -0.014360340312123299,
          -0.03193340823054314,
          -0.019027104601264,
          -0.005625735968351364,
          0.0018867852631956339,
          -0.0035693130921572447,
          -0.019331760704517365,
          -0.009797435253858566,
          0.011209928430616856,
          0.014014140702784061,
          0.010856805369257927,
          -0.0030153945554047823,
          0.02568797580897808,
          0.008980405516922474,
          -0.0017119547119364142,
          0.023167645558714867,
          0.009631260298192501,
          -0.03395521268248558,
          -0.012525484897196293,
          -0.0429563894867897,
          0.004060915671288967,
          0.01564820110797882,
          -0.015177370049059391,
          0.010032851248979568,
          0.019345607608556747,
          -0.021145842969417572,
          0.018376249819993973,
          0.00284402584657073,
          0.0032438859343528748,
          -0.008620359003543854,
          -0.023818500339984894,
          -0.034536827355623245,
          0.005355700850486755,
          -0.028042130172252655,
          -0.006885901093482971,
          -0.012310841120779514,
          -0.015274305827915668,
          0.004137079697102308,
          0.0003397079126443714,
          0.02462168224155903,
          -0.026975836604833603,
          -0.025701824575662613,
          0.03827577829360962,
          0.011611519381403923,
          0.02679581381380558,
          -0.010088242590427399,
          0.010240570642054081,
          0.007768708746880293,
          0.000310930103296414,
          -0.020591923967003822,
          0.01952563226222992,
          0.01703299768269062,
          -0.006252356339246035,
          0.01725456491112709,
          0.016908366233110428,
          -0.011112992651760578,
          -0.005418016575276852,
          0.01819622702896595,
          0.019054800271987915,
          -0.021062755957245827,
          -0.049021799117326736,
          0.013273275457322598,
          0.018113138154149055,
          -0.005874999333173037,
          -0.020550381392240524,
          -0.03356746956706047,
          -0.0245662909001112,
          0.014581907540559769,
          -0.009215821512043476,
          0.016991453245282173,
          -0.004223629366606474,
          -0.007457129657268524,
          0.016880670562386513,
          0.005293384660035372,
          0.023555388674139977,
          -0.012463169172406197,
          -0.0013094982132315636,
          -0.005864613223820925,
          0.019345607608556747,
          -0.016880670562386513,
          0.0075194453820586205,
          0.010171330533921719,
          -0.004112845752388239,
          0.018514730036258698,
          0.007401737384498119,
          0.012989391572773457,
          -0.009395844303071499,
          0.013404830358922482,
          0.009153504855930805,
          -0.008184147998690605,
          -0.0030361665412783623,
          0.027765171602368355,
          -0.020938124507665634,
          -0.0034221785608679056,
          0.0022866453509777784,
          -0.011604594998061657,
          0.0051895249634981155,
          -0.0234169103205204,
          -0.010884501039981842,
          0.00040764949517324567,
          0.0011805390240624547,
          0.007761784829199314,
          0.01969180628657341,
          0.010462137870490551,
          -0.005781525745987892,
          0.011078372597694397,
          -0.0038912782911211252,
          -0.03185031935572624,
          0.007076310459524393,
          -0.0031919560860842466,
          0.01858397014439106,
          -0.032985854893922806,
          0.0027142013423144817,
          0.01542663387954235,
          -0.008731142617762089,
          0.00689282501116395,
          0.002828446915373206,
          -0.021090451627969742,
          0.0028249849565327168,
          0.02541101723909378,
          -0.028277546167373657,
          0.012165437452495098,
          0.01663140580058098,
          -0.0063769882544875145,
          0.00260168663226068,
          0.019650263711810112,
          0.0023905050475150347,
          -0.0023506921716034412,
          0.014484971761703491,
          -0.02275220677256584,
          -0.02430317923426628,
          -0.003756260732188821,
          -0.017393045127391815,
          -0.008751913905143738,
          -0.0019473701249808073,
          0.012518560513854027,
          0.01231776550412178,
          -0.017129933461546898,
          0.0008983867592178285,
          -0.018376249819993973,
          0.00796257983893156,
          0.019664110615849495,
          -0.0066435616463422775,
          -0.006352754309773445,
          0.02423393912613392,
          -0.020924275740981102,
          0.003221383085474372,
          -0.015108130872249603,
          0.009395844303071499,
          -0.024261634796857834,
          -0.05035120248794556,
          0.0017110892804339528,
          0.023112254217267036,
          -0.0006486906204372644,
          0.006307748146355152,
          -0.012470092624425888,
          -0.026948140934109688,
          0.0031261781696230173,
          -0.004756776150316,
          0.018016202375292778,
          -0.004995653405785561,
          0.013321742415428162,
          0.009956687688827515,
          0.01797465980052948,
          0.028526809066534042,
          0.009825131855905056,
          -0.017517676576972008,
          -0.02125662751495838,
          -0.011992338113486767,
          -0.011050676926970482,
          -0.035256918519735336,
          -0.0165344700217247,
          -0.02307070977985859,
          0.041793160140514374,
          -0.002992891473695636,
          -0.01747613213956356,
          -0.008301855064928532,
          -0.003043090458959341,
          -0.043177954852581024,
          -0.022336767986416817,
          -0.003728564828634262,
          0.012428549118340015,
          0.004341337364166975,
          0.03594931960105896,
          0.0004967957502231002,
          0.019110191613435745,
          -0.0030240495689213276,
          0.0020494989585131407,
          -0.008731142617762089,
          -0.01642368733882904,
          -0.0024130078963935375,
          -0.0038912782911211252,
          0.01067678164690733,
          0.00859958678483963,
          -0.02435857057571411,
          -0.00021042415755800903,
          0.020093398168683052,
          -0.008281083777546883,
          -0.003510459326207638,
          0.029551558196544647,
          -0.0017638845602050424,
          -0.009562020190060139,
          0.006716263480484486,
          0.0075471410527825356,
          0.002277990337461233,
          0.005899233277887106,
          0.01042751781642437,
          -0.012255449779331684,
          0.01137610338628292,
          0.012947848066687584,
          -0.020730404183268547,
          -0.014955802820622921,
          -0.006927444599568844,
          0.014235708862543106,
          0.013439450412988663,
          -0.0009053107351064682,
          0.00028539792401716113,
          -0.026214199140667915,
          0.0033771726302802563,
          -0.017711548134684563,
          0.04796935245394707,
          0.003918974194675684,
          0.0023905050475150347,
          0.02561873570084572,
          -0.0023178032133728266,
          -0.009354300796985626,
          0.00445212097838521,
          0.004420963115990162,
          0.008066440001130104,
          0.003950132057070732,
          0.022004418075084686,
          -0.02269681543111801,
          -0.016963757574558258,
          -0.008004124276340008,
          0.0016435803845524788,
          -0.029800821095705032,
          -0.029246903955936432,
          -0.020619621500372887,
          -0.0017162822186946869,
          -0.026034174486994743,
          0.024040067568421364,
          0.012657040730118752,
          -0.014692691154778004,
          0.007166322320699692,
          -0.005663817748427391,
          0.0024476279504597187,
          -0.006712801288813353,
          -0.018542425706982613,
          0.007747936528176069,
          -0.018348554149270058,
          0.010012079030275345,
          0.013120947405695915,
          -0.013688714243471622,
          -0.00672664912417531,
          0.010275190696120262,
          0.016797581687569618,
          -0.028208306059241295,
          0.02301531843841076,
          0.21248318254947662,
          -0.03622627630829811,
          0.010912196710705757,
          0.025715671479701996,
          0.017019148916006088,
          -0.014166468754410744,
          0.029745429754257202,
          0.007339421659708023,
          -0.012490864843130112,
          0.01003977470099926,
          0.000831310695502907,
          0.008087212219834328,
          -0.02190748229622841,
          -0.0009027142659761012,
          0.008281083777546883,
          -0.0314071848988533,
          -0.04741543531417847,
          -0.026158807799220085,
          -0.0034585294779390097,
          0.017116084694862366,
          0.010725249536335468,
          0.004459044896066189,
          -0.004133617505431175,
          -0.026546550914645195,
          0.020439596846699715,
          -0.01006747130304575,
          -0.0026916982606053352,
          0.006342368200421333,
          0.006581245455890894,
          0.010316734202206135,
          0.0015994400018826127,
          0.0038324245251715183,
          0.006051560863852501,
          -0.0014999078121036291,
          0.0072563341818749905,
          -0.01697760634124279,
          0.007110930513590574,
          -0.010275190696120262,
          0.004690998233854771,
          0.017725395038723946,
          0.007387889549136162,
          0.0014471124159172177,
          0.029579253867268562,
          -0.03506304696202278,
          0.006878976710140705,
          0.025784911587834358,
          -0.005289922934025526,
          -0.010690629482269287,
          0.008620359003543854,
          0.0028821078594774008,
          -0.010808337479829788,
          0.03279198333621025,
          0.030991746112704277,
          0.038054209202528,
          -0.0020131480414420366,
          0.009236592799425125,
          -0.03921743854880333,
          0.01714378222823143,
          0.012262373231351376,
          0.008551118895411491,
          -0.0278482586145401,
          -0.009236592799425125,
          0.0053833965212106705,
          0.024220092222094536,
          -0.018126986920833588,
          0.001204772968776524,
          -0.009105036966502666,
          0.003498342353850603,
          0.009922067634761333,
          -0.02790364995598793,
          -0.007207866292446852,
          -0.012982468120753765,
          -0.032598111778497696,
          0.015523569658398628,
          -0.021547434851527214,
          -0.04724925756454468,
          0.03257041424512863,
          0.038192689418792725,
          0.011687682941555977,
          0.04054684191942215,
          -0.01245624478906393,
          -0.01051060575991869,
          -0.006255818530917168,
          -0.0220736563205719,
          -0.00790718849748373,
          -0.036641716957092285,
          0.017517676576972008,
          -0.023375365883111954,
          -0.016285207122564316,
          -0.013349439017474651,
          -0.012248525395989418,
          -0.014817323535680771,
          -0.009478932246565819,
          0.015468177385628223,
          0.00650508189573884,
          0.005604964215308428,
          0.006608941592276096,
          -0.015620505437254906,
          -0.0023662711028009653,
          -0.022669119760394096,
          -0.014415732584893703,
          0.053785499185323715,
          0.031130226328969002,
          0.0030188565142452717,
          0.011078372597694397,
          0.012975543737411499,
          -0.003593547036871314,
          0.011417647823691368,
          0.004988729488104582,
          -0.0008049129974097013,
          0.008488803170621395,
          -0.041793160140514374,
          0.015149674378335476,
          -0.005660356022417545,
          -0.013993369415402412,
          0.005057969596236944,
          -4.646641536965035e-05,
          -0.0039051263593137264,
          0.021118147298693657,
          0.00028496517916209996,
          0.0008905972936190665,
          -0.022544488310813904,
          0.006982836406677961,
          -0.0030205873772501945,
          -0.0033564006444066763,
          -0.023887740448117256,
          -0.017850028350949287,
          -0.005092589184641838,
          -0.040685322135686874,
          0.005089127458631992,
          0.007893340662121773,
          -0.020273420959711075,
          -0.004628682509064674,
          0.01858397014439106,
          -0.010171330533921719,
          -0.004694460425525904,
          0.008641130290925503,
          -0.011078372597694397,
          -0.0075471410527825356,
          -0.004195933695882559,
          0.0028665289282798767,
          0.018071595579385757,
          -0.002819791901856661,
          0.0009918605210259557,
          0.027972890064120293,
          0.0018487033667042851,
          0.010946816764771938,
          0.028969943523406982,
          -0.008177223615348339,
          -0.002456282963976264,
          -0.030077781528234482,
          0.0048571741208434105,
          0.004296331200748682,
          -0.005199911072850227,
          0.018708601593971252,
          0.009499704465270042,
          -0.035810839384794235,
          0.005968473386019468,
          0.010219798423349857,
          0.02264142408967018,
          -0.04644607752561569,
          0.001801966573111713,
          0.002335113240405917,
          -0.016215967014431953,
          -0.009714348241686821,
          -0.01603594422340393,
          -0.17869414389133453,
          -0.016229815781116486,
          -0.00013815509737469256,
          -0.04207012057304382,
          0.010282114148139954,
          0.021228931844234467,
          0.012525484897196293,
          -0.008855774067342281,
          -0.034370649605989456,
          -0.004684074316173792,
          0.0063389060087502,
          -0.0017846565460786223,
          0.0004937665071338415,
          -0.01203388161957264,
          -0.010808337479829788,
          0.004950647708028555,
          -0.007242485880851746,
          0.00973511952906847,
          -0.0013043052749708295,
          0.0028249849565327168,
          0.0319611057639122,
          -0.033041246235370636,
          -0.011479963548481464,
          -0.02552179992198944,
          0.010836033150553703,
          0.014831171371042728,
          -0.0072840298525989056,
          0.02368002198636532,
          -0.005376472603529692,
          -0.029579253867268562,
          -0.017448436468839645,
          -0.009305832907557487,
          0.030964050441980362,
          -0.014118000864982605,
          0.01168075855821371,
          0.011971565894782543,
          0.02129817195236683,
          -0.00027133358526043594,
          -0.024593986570835114,
          0.018265467137098312,
          0.021145842969417572,
          0.024275483563542366,
          0.020231878384947777,
          0.011078372597694397,
          -0.030659396201372147,
          0.006965526845306158,
          0.021755153313279152,
          -0.030437828972935677,
          0.002503019757568836,
          -0.013287123292684555,
          0.024164699018001556,
          -0.02445550635457039,
          0.005978859029710293,
          0.004053991753607988,
          -0.003846272360533476,
          0.02434472367167473,
          -0.0018088904907926917,
          0.008814229629933834,
          -0.00760253332555294,
          -0.001434995443560183,
          -0.019761046394705772,
          -0.013944901525974274,
          0.0011026442516595125,
          9.374423825647682e-05,
          -0.01808544248342514,
          -0.03522922471165657,
          -0.012573952786624432,
          0.003262926824390888,
          -0.024663226678967476,
          0.004597524646669626,
          -0.013079403899610043,
          0.002425124868750572,
          -0.008059515617787838,
          -0.005622274242341518,
          0.02723894827067852,
          0.018348554149270058,
          0.00865497812628746,
          0.02163052186369896,
          0.016132880002260208,
          -0.0012601648923009634,
          -0.018570121377706528,
          0.01814083568751812,
          -0.0004275559331290424,
          0.017725395038723946,
          -0.01941484771668911,
          -0.010822185315191746,
          -0.008101060055196285,
          0.014734235592186451,
          -0.02163052186369896,
          -0.010891424492001534,
          0.039245136082172394,
          -0.010102090425789356,
          -0.0038220384158194065,
          -0.013439450412988663,
          -0.004344799090176821,
          0.015578960999846458,
          0.00404706783592701,
          -0.017102237790822983,
          -0.003304470796138048,
          0.00030768447322770953,
          -0.005826531443744898,
          0.0026726573705673218,
          0.00034771376522257924,
          -0.004524822812527418,
          0.029136119410395622,
          0.008592662401497364,
          -0.04043605923652649,
          0.012532409280538559,
          0.036475542932748795,
          -0.026089567691087723,
          -0.025715671479701996,
          -0.01375795342028141,
          -0.011230699717998505,
          -0.003283698810264468,
          -0.01167383510619402,
          0.006162344478070736,
          0.0001670410856604576,
          -0.0319611057639122,
          0.01952563226222992,
          0.0018677443731576204,
          0.039688270539045334,
          -0.020314965397119522,
          -0.0027488211635500193,
          -0.006830508820712566,
          0.003352938685566187,
          -0.024261634796857834,
          -0.12164053320884705,
          -0.02613111026585102,
          0.01586976833641529,
          0.015163522213697433,
          -0.010600618086755276,
          0.034370649605989456,
          0.0038947402499616146,
          0.04619681462645531,
          -0.020148789510130882,
          0.04403652995824814,
          -0.02485709823668003,
          -0.0018677443731576204,
          0.013349439017474651,
          -0.002939230762422085,
          0.029329990968108177,
          -0.009873599745333195,
          0.002016610000282526,
          -0.015883617103099823,
          -0.040131404995918274,
          0.027086621150374413,
          0.019165584817528725,
          -0.012442396953701973,
          0.007394813466817141,
          -0.028914552181959152,
          -0.014581907540559769,
          -0.006629713345319033,
          -0.03556157648563385,
          0.02319534309208393,
          0.02096582017838955,
          0.005629198160022497,
          0.017545372247695923,
          -0.034647610038518906,
          -0.008564966730773449,
          -0.026089567691087723,
          0.024317028000950813,
          -0.005767677444964647,
          0.0016617558430880308,
          -0.012248525395989418,
          0.016119031235575676,
          -0.012206981889903545,
          -0.008004124276340008,
          0.007803328800946474,
          0.021658217534422874,
          -0.004542132839560509,
          0.004040143918246031,
          -0.030050085857510567,
          -0.007346345577389002,
          0.02030111663043499,
          0.009555095806717873,
          -0.016839126124978065,
          -0.03489687293767929,
          -0.013377134688198566,
          -0.05032350867986679,
          0.01825161837041378,
          0.016880670562386513,
          0.01034442987293005,
          0.009091189131140709,
          0.017946964129805565,
          -0.0074779014103114605,
          0.00890424195677042,
          -0.01071140170097351,
          0.0018002354772761464,
          -0.029606949537992477,
          0.006290438584983349,
          0.00895963329821825,
          0.0005703630740754306,
          0.0006188309635035694,
          -0.02602032758295536,
          0.003044821321964264,
          -0.0016738728154450655,
          -0.0020720018073916435,
          0.019151736050844193,
          -0.011196080595254898,
          0.006799350958317518,
          -0.0018002354772761464,
          -0.0038531965110450983,
          -0.020176485180854797,
          -0.029634647071361542,
          -0.005992707330733538,
          0.0009961880277842283,
          -0.007644076831638813,
          -0.016686799004673958,
          0.007024380378425121,
          -0.009866675361990929,
          -6.009800563333556e-05,
          -0.0037389507051557302,
          -0.015662049874663353,
          -0.005199911072850227,
          -0.008461106568574905,
          -0.01913788914680481,
          0.0049229515716433525,
          0.029579253867268562,
          0.004725618287920952,
          -0.019788742065429688,
          0.006082718726247549,
          0.02474631369113922,
          -0.019096344709396362,
          -0.004247863311320543,
          0.037361811846494675,
          0.006581245455890894,
          -0.00837801955640316,
          0.009285060688853264,
          -0.05325927585363388,
          0.02696198970079422,
          0.016354447230696678,
          -0.006986298598349094,
          0.005355700850486755,
          -0.012110046111047268,
          0.019677959382534027,
          -0.023652324452996254,
          0.01245624478906393,
          0.009638183750212193,
          -0.023043014109134674,
          0.022544488310813904,
          -0.006238508503884077,
          -0.010462137870490551,
          -0.02219828963279724,
          0.0026155344676226377,
          0.03107483498752117,
          -0.0011251472169533372,
          0.011909250169992447,
          -0.0016617558430880308,
          0.003901664400473237,
          -0.02280759997665882,
          0.0027886340394616127,
          0.011078372597694397,
          0.007734088692814112,
          -0.0036627869121730328,
          -0.02762669138610363,
          0.012774747796356678,
          -0.0016756037948653102,
          -0.014568059705197811,
          -0.009243517182767391,
          -0.01098143681883812,
          -0.011639215052127838,
          0.003991676028817892,
          -0.014568059705197811,
          -0.009928991086781025,
          0.027723627164959908,
          0.030742483213543892,
          0.01852857880294323,
          0.04564289376139641,
          -0.02229522541165352,
          -0.03301354870200157,
          0.0015371241606771946,
          -0.020841188728809357,
          -0.004902179818600416,
          -0.017268413677811623,
          0.0025999555364251137,
          -0.007228638045489788,
          0.028969943523406982,
          -0.0016046330565586686,
          0.04597524553537369,
          0.014941954985260963,
          -0.020868884399533272,
          -0.02434472367167473,
          -0.009928991086781025,
          0.01175692304968834,
          0.01969180628657341,
          0.012040806002914906,
          0.011867706663906574,
          0.02057807706296444,
          0.02046729251742363,
          0.005521876271814108,
          0.008869621902704239,
          -0.012795520015060902,
          0.0030967514030635357,
          -0.009478932246565819,
          -0.014249556697905064,
          -0.007117854431271553,
          0.0037493365816771984,
          -0.002913265721872449,
          -0.0005448309239000082,
          0.005965011194348335,
          0.018445489928126335,
          0.006944754626601934,
          0.007110930513590574,
          -0.014900410547852516,
          0.004292869474738836,
          0.005445712246000767,
          -0.01062831375747919,
          0.010233646258711815,
          0.00032412895234301686,
          -0.007616381160914898,
          0.0019525631796568632,
          0.00658816983923316,
          0.018390098586678505,
          0.0019871830008924007,
          -0.004420963115990162,
          0.002729780273512006,
          -0.013259426690638065,
          -0.007311725988984108,
          0.0008633341058157384,
          -0.002729780273512006,
          -0.006273128557950258,
          0.026061872020363808,
          0.013107099570333958,
          0.0037943425122648478,
          -0.007000146433711052,
          0.005739981774240732,
          0.019456392154097557,
          0.0021498966962099075,
          0.010489833541214466,
          -0.022267527878284454,
          -0.009423540905117989,
          -0.010621389374136925,
          -0.028526809066534042,
          0.01179154310375452,
          -0.04373187571763992,
          -0.022544488310813904,
          0.03489687293767929,
          0.023666173219680786,
          0.010974512435495853,
          -0.010607541538774967,
          0.007484825327992439,
          0.013210958801209927,
          -0.027225099503993988,
          0.03428756445646286,
          -0.009555095806717873,
          -0.014886562712490559,
          -0.030520915985107422,
          0.027308188378810883,
          0.040962282568216324,
          0.0035866231191903353,
          0.050572771579027176,
          0.00012722818064503372,
          0.020675012841820717,
          -0.0014445158885791898,
          0.017268413677811623,
          -0.029551558196544647,
          0.017850028350949287,
          -0.0024701307993382215,
          -0.012643192894756794,
          -0.0038116525392979383,
          -0.014568059705197811,
          -0.0065985554829239845,
          -0.008232615888118744,
          -0.002944423584267497,
          -0.017116084694862366,
          0.029773125424981117,
          0.011936946772038937,
          0.058493807911872864,
          0.014817323535680771,
          -0.007948732003569603,
          0.028055978938937187,
          -0.007221714127808809,
          0.02269681543111801,
          0.014831171371042728,
          0.026200350373983383,
          -0.003037897404283285,
          -0.025369472801685333,
          -0.0016782003222033381,
          0.003309663850814104,
          0.005951163358986378,
          -0.029496166855096817,
          -0.0033200497273355722,
          -0.015662049874663353,
          0.003621242940425873,
          0.02478785812854767,
          -0.00934737641364336,
          -0.01109914481639862,
          0.03755568340420723,
          -0.005881923250854015,
          0.015468177385628223,
          0.013612549751996994,
          -0.014595755375921726,
          0.00790718849748373,
          0.018487034365534782,
          0.009319680742919445,
          -0.004746390040963888,
          0.001062831375747919,
          0.025881847366690636,
          0.021270474418997765,
          -0.031324099749326706,
          -0.015094282105565071,
          0.009548172354698181,
          0.007817176170647144,
          0.005646508187055588,
          -0.025757215917110443,
          -0.001264492399059236,
          -0.007692544721066952,
          -0.0018573583802208304,
          0.007810252718627453,
          -0.0038220384158194065,
          -0.02264142408967018,
          -0.0032975468784570694,
          0.0016150189330801368,
          -0.0009840710554271936,
          -0.008357247337698936,
          -0.01563435234129429
        ]
      },
      "type": "document"
    },
    {
      "id": "1d35c58e-1af7-4136-bfeb-f0894fc21db0",
      "properties": {
        "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere’s my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey’re actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don’t yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n\nThe ethics of this space remain diabolically complex\n\nMy blog in 2023\n\nHere’s the sequel to this post: Things we learned about LLMs in 2024.\n\nLarge Language Models\n\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\n\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\n\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\n\nSo far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\n\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\n\nThey’re actually quite easy to build\n\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\n\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\n\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\n\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\n\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\n\nThe training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing.\n\nSo training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\n\nYou can run LLMs on your own devices\n\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\n\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\n\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\n\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use.\n\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\n\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\n\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\n\nHobbyists can build their own fine-tuned models\n\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\n\nThere’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\n\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours.\n\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\n\nThis is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\n\nWe don’t yet know how to build GPT-4\n\nFrustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4.\n\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\n\nThis may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out.\n\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then.\n\nStill, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet.\n\nVibes Based Development\n\nAs a computer scientist and software engineer, LLMs are infuriating.\n\nEven the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\n\nI’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\n\nThe worst part is the challenge of evaluating them.\n\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task.\n\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself!\n\nThe most frustrating thing for me is at the level of individual prompting.\n\nSometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out.\n\nWe’re left with what’s effectively Vibes Based Development. It’s vibes all the way down.\n\nI’d love to see us move beyond vibes in 2024!\n\nLLMs are really smart, and also really, really dumb\n\nOn the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun!\n\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\n\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\n\nThe honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\n\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead.\n\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works!\n\nGullibility is the biggest unsolved problem\n\nI coined the term prompt injection in September last year.\n\n15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem.\n\nI’ve written a ton about this already.\n\nBeyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility.\n\nLanguage Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt.\n\nIn order to be useful tools for us, we need them to believe what we feed them!\n\nBut it turns out a lot of the things we want to build need them not to be gullible.\n\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\n\nA lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\n\nI think this is because of gullibility.\n\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\n\nCode may be the best application\n\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.\n\nIf you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\n\nIt’s still astonishing to me how effective they are though.\n\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\n\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\n\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\n\nHow should we feel about this as software engineers?\n\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\n\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\n\nThe ethics of this space remain diabolically complex\n\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\n\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\n\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\n\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\n\nLaw is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\n\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\n\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\n\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\n\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\n\nMy blog in 2023\n\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\n\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\n\nI’ve written a lot about this stuff!\n\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\n\nArticle Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k\n\nI also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\n\nPrompt injection explained, with video, slides, and a transcript\n\nCatching up on the weird world of LLMs\n\nMaking Large Language Models work for you\n\nOpen questions for AI engineering\n\nEmbeddings: What they are and why they matter\n\nFinancial sustainability for open source projects at GitHub Universe\n\nAnd in podcasts:\n\nWhat AI can do for you on the Theory of Change\n\nWorking in public on Path to Citus Con\n\nLLMs break the internet on the Changelog\n\nTalking Large Language Models on Rooftop Ruby\n\nThoughts on the OpenAI board situation on Newsroom Robots\n\nIndustry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1089\n\ngenerative-ai\n            934\n\nllms\n            922\n\nNext: Tom Scott, and the formidable power of escalating streaks\n\nPrevious: Last weeknotes of 2023\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
        "document_metadata": {
          "source": "data/2023_llms.html"
        },
        "headlines": [
          "Large Language Models",
          "Vibes Based Development",
          "LLMs are really smart, and also really, really dumb",
          "Gullibility is the biggest unsolved problem",
          "Code may be the best application"
        ],
        "summary": "In 2023, Large Language Models (LLMs) emerged as a significant development in AI, with advancements making them easier to build and run on personal devices. Despite their capabilities, LLMs face challenges like gullibility and ethical concerns over unlicensed training data. They excel in code generation, where errors can be iteratively corrected. The year saw a proliferation of LLMs from various organizations, though none surpassed GPT-4. The complexity of LLMs remains a barrier, with 'Vibes Based Development' highlighting the difficulty in evaluating their performance. The ethical and legal implications of AI continue to be complex and unresolved.",
        "summary_embedding": [
          -0.01449595857411623,
          -0.005809996742755175,
          0.0007078905473463237,
          -0.03407438471913338,
          -0.0034515000879764557,
          0.01598517596721649,
          -0.015042460523545742,
          0.02121793106198311,
          -0.023909451439976692,
          -0.04071437940001488,
          0.025603607296943665,
          0.020726079121232033,
          -0.023991426452994347,
          -0.0034241750836372375,
          0.0019861923065036535,
          0.0180345568805933,
          0.021983033046126366,
          0.0025992989540100098,
          0.017529044300317764,
          -0.007432423997670412,
          -0.0009734562481753528,
          -0.0001246707106474787,
          -0.014441308565437794,
          -0.017406079918146133,
          -0.005376210901886225,
          0.006428226828575134,
          0.022898422554135323,
          -0.03661561757326126,
          -0.014618922024965286,
          -0.0006575098959729075,
          0.02032986469566822,
          -0.0041534132324159145,
          -0.03789989650249481,
          0.007480242755264044,
          -0.029647719115018845,
          -0.00886699091643095,
          0.013819662854075432,
          -0.007883287966251373,
          0.020575791597366333,
          0.005519667640328407,
          0.028800642117857933,
          0.02869134210050106,
          0.015138098038733006,
          -0.02002928964793682,
          -0.029073892161250114,
          0.017815956845879555,
          0.004792137071490288,
          -0.01644970290362835,
          -0.03341858088970184,
          0.012070856988430023,
          0.019318837672472,
          0.037872571498155594,
          -0.011421886272728443,
          -0.014536946080625057,
          -0.00784913171082735,
          0.002877673367038369,
          -0.014113407582044601,
          0.01066361553966999,
          0.017706656828522682,
          0.00035437222686596215,
          -0.006291601341217756,
          -0.01573925092816353,
          -0.002004978246986866,
          0.024879492819309235,
          -0.006978144403547049,
          -0.010670446790754795,
          -0.027584675699472427,
          0.03314533084630966,
          0.033965084701776505,
          -0.013266329653561115,
          -0.0027427556924521923,
          -0.008095056749880314,
          0.019154885783791542,
          -0.011121310293674469,
          0.017925256863236427,
          0.007992587983608246,
          -0.006277938839048147,
          -0.015261061489582062,
          -0.0275436881929636,
          0.004747733939439058,
          -0.0019281264394521713,
          -0.0006651951116509736,
          -0.003910903353244066,
          0.013150198385119438,
          0.013744519092142582,
          0.004597445949912071,
          -0.005208844784647226,
          -0.017692994326353073,
          -0.0015805854927748442,
          -0.013437111862003803,
          -0.005540161393582821,
          -0.005652877502143383,
          0.016531677916646004,
          0.017228467389941216,
          -0.0062813544645905495,
          -0.0023106776643544436,
          -0.008962628431618214,
          0.002288476098328829,
          -0.005642630625516176,
          -0.026764923706650734,
          -0.00512003805488348,
          0.0024695047177374363,
          -0.040905654430389404,
          -0.008313657715916634,
          -0.023608876392245293,
          0.0037196276243776083,
          0.015684600919485092,
          0.021245256066322327,
          0.015124435536563396,
          0.018949948251247406,
          -0.00663658045232296,
          0.027652988210320473,
          0.016518015414476395,
          -0.02136821858584881,
          -0.0024438875261694193,
          -0.015807563439011574,
          0.0029476936906576157,
          -0.0006071293028071523,
          0.005079050548374653,
          0.008095056749880314,
          0.031095949932932854,
          0.010404027067124844,
          0.013778675347566605,
          -0.0019042170606553555,
          0.019701387733221054,
          -0.015151760540902615,
          0.005929544102400541,
          -0.020726079121232033,
          -0.01234410796314478,
          -0.0003955733263865113,
          0.0020237641874700785,
          -0.0027290931902825832,
          0.010294727049767971,
          0.0027188463136553764,
          -0.026901548728346825,
          0.031369201838970184,
          -0.023704513907432556,
          -0.0034737016540020704,
          0.009386167861521244,
          0.010397195816040039,
          0.0009649171261116862,
          0.02467455342411995,
          -0.022761797532439232,
          0.017515381798148155,
          -0.004358351230621338,
          0.018717685714364052,
          0.009413492865860462,
          0.013361968100070953,
          -0.0024353484623134136,
          -0.000552052166312933,
          0.004450573585927486,
          -0.03153315186500549,
          -0.01802089437842369,
          0.02321266196668148,
          0.004679421428591013,
          0.004518886562436819,
          0.00696106581017375,
          0.023267311975359917,
          -0.031369201838970184,
          -0.010349377058446407,
          -0.014741884544491768,
          0.009563780389726162,
          -0.0032328993547707796,
          0.019797025248408318,
          0.01363521907478571,
          0.021245256066322327,
          0.028800642117857933,
          0.0026932288892567158,
          -0.006749296560883522,
          -0.014687234535813332,
          -0.014427646063268185,
          0.030604097992181778,
          -0.03085002303123474,
          0.011968388222157955,
          -0.015138098038733006,
          0.020111264660954475,
          0.023321961984038353,
          -0.0037025492638349533,
          -0.024783853441476822,
          -0.018553733825683594,
          0.01804821938276291,
          0.009666250087320805,
          0.01613546349108219,
          -0.009270035661756992,
          -0.013505424372851849,
          0.0011843717657029629,
          0.010499664582312107,
          0.0041397507302463055,
          0.012214314192533493,
          -0.02293941006064415,
          0.027652988210320473,
          0.018581058830022812,
          -0.005468433257192373,
          -0.028800642117857933,
          -0.6571137309074402,
          -0.0369708426296711,
          0.015821225941181183,
          -0.010690940544009209,
          0.002980142366141081,
          -0.0036342365201562643,
          0.0009486929047852755,
          -0.01284279115498066,
          -0.015343036502599716,
          0.03306335583329201,
          -0.013525918126106262,
          0.006315510720014572,
          -0.003108228789642453,
          -0.019646737724542618,
          0.011216948740184307,
          -0.009782381355762482,
          0.0042900387197732925,
          -0.026478009298443794,
          0.007610036991536617,
          0.018376121297478676,
          -0.005273741669952869,
          0.019072910770773888,
          -0.0071181850507855415,
          -0.006609255447983742,
          -0.028445415198802948,
          0.006571683567017317,
          0.009010447189211845,
          0.015206411480903625,
          -0.006018350366503,
          0.004180738236755133,
          -0.01097102276980877,
          0.00577242486178875,
          0.0032414384186267853,
          0.016258427873253822,
          0.04273643717169762,
          -0.001592540298588574,
          -0.014004107564687729,
          0.027311425656080246,
          -0.001619865302927792,
          -0.00018519151490181684,
          -0.044758494943380356,
          -0.010943697765469551,
          -0.01646336540579796,
          0.03147850185632706,
          -0.005205429159104824,
          0.03186105191707611,
          0.024401303380727768,
          -0.001023836899548769,
          -0.013819662854075432,
          -0.007972094230353832,
          -0.0005725459777750075,
          0.010205919854342937,
          0.00799941923469305,
          0.00587830925360322,
          -0.008259007707238197,
          0.008183863945305347,
          0.014837522059679031,
          -0.010984685271978378,
          -0.000806943979114294,
          -0.019578425213694572,
          0.006585346069186926,
          -0.016408715397119522,
          -0.021381881088018417,
          -0.03128722682595253,
          -0.013484930619597435,
          0.011059829033911228,
          0.007773987483233213,
          0.012542215175926685,
          0.0005225922795943916,
          -0.024838505312800407,
          -0.00268468982540071,
          0.043337587267160416,
          0.004126088228076696,
          -0.01181126944720745,
          -0.0068859220482409,
          0.01528838649392128,
          0.00994633138179779,
          -0.010615796782076359,
          -0.006687815301120281,
          0.008989953435957432,
          0.006428226828575134,
          -0.018362458795309067,
          0.0042251418344676495,
          -0.0011211824603378773,
          0.03861035034060478,
          -0.006137897726148367,
          0.01658632792532444,
          -0.006120819598436356,
          0.0015626534586772323,
          0.009119748137891293,
          0.010848059318959713,
          0.02349957451224327,
          -0.017501719295978546,
          -0.05607108026742935,
          0.004952671937644482,
          0.009119748137891293,
          -0.013450774364173412,
          0.011449211277067661,
          0.017242129892110825,
          -0.02093101665377617,
          -0.006954234559088945,
          -0.012077688239514828,
          0.03505808860063553,
          0.023704513907432556,
          0.011633655987679958,
          0.0232399869710207,
          0.0023499575909227133,
          0.005625552497804165,
          0.015479662455618382,
          -0.02410072647035122,
          -0.006503370590507984,
          0.002655657008290291,
          0.003606911515817046,
          -0.00566312437877059,
          -0.0068859220482409,
          -0.029620394110679626,
          -0.01053382083773613,
          -0.0029306155629456043,
          0.012446577660739422,
          -0.02683323621749878,
          0.012637852691113949,
          -0.0044130017049610615,
          0.026887886226177216,
          0.00035330484388396144,
          -0.005079050548374653,
          0.004785305820405483,
          -0.002153558423742652,
          -0.006141313351690769,
          -0.005796334240585566,
          0.023745501413941383,
          -0.01643604040145874,
          -0.027775950729846954,
          0.012064025737345219,
          -0.016395052894949913,
          0.028773317113518715,
          0.01629941537976265,
          0.04265446215867996,
          -0.001146799768321216,
          0.006032012868672609,
          0.015343036502599716,
          -0.023690851405262947,
          0.0001412152050761506,
          0.0016659764805808663,
          0.0137718440964818,
          -0.04216260835528374,
          -0.02538500539958477,
          -0.005628968123346567,
          -0.004952671937644482,
          0.011920569464564323,
          -0.016477027907967567,
          -0.017911594361066818,
          -0.008621064946055412,
          -0.016340402886271477,
          0.03415635973215103,
          0.0009871188085526228,
          -0.009433986619114876,
          -0.015520649962127209,
          -0.02378648892045021,
          -0.003485656576231122,
          -0.026300396770238876,
          0.0016958632040768862,
          0.006954234559088945,
          0.0024438875261694193,
          0.006513617932796478,
          0.02468821592628956,
          -0.02064410410821438,
          -0.0076510244980454445,
          0.02710648626089096,
          -0.03358253091573715,
          -0.022966735064983368,
          0.009461311623454094,
          -0.03284475579857826,
          0.012446577660739422,
          0.030084921047091484,
          -0.010704603046178818,
          0.018130196258425713,
          -0.0034515000879764557,
          0.001164731802418828,
          0.008211188949644566,
          -0.011319417506456375,
          0.004358351230621338,
          0.03057677298784256,
          0.014632584527134895,
          -0.004300285596400499,
          0.022447559982538223,
          0.02337661199271679,
          0.03514006361365318,
          0.014058757573366165,
          -0.006414564326405525,
          0.01001464482396841,
          0.00311847566626966,
          0.032407552003860474,
          -0.011640487238764763,
          0.000776630244217813,
          -0.006387239322066307,
          0.022119658067822456,
          -0.0013158736983314157,
          -0.009953162632882595,
          0.009755056351423264,
          0.023841138929128647,
          0.014113407582044601,
          0.024278340861201286,
          0.018225833773612976,
          -0.010902710258960724,
          0.0012535384157672524,
          -0.01814385876059532,
          -0.002088661305606365,
          -0.014782872051000595,
          -0.00263003958389163,
          0.016545340418815613,
          0.009645755402743816,
          0.003593249013647437,
          -0.013034067116677761,
          -0.02250220999121666,
          -0.0016779311699792743,
          0.02855471707880497,
          0.005219091661274433,
          0.011790774762630463,
          -0.014605259522795677,
          -3.9333182940026745e-05,
          0.0129042724147439,
          0.011797606945037842,
          0.018840648233890533,
          -0.012337276712059975,
          -0.0029630642384290695,
          0.019578425213694572,
          -0.025999819859862328,
          0.014332008548080921,
          -0.0006276231142692268,
          -0.0360691137611866,
          0.009843862615525723,
          0.015028798021376133,
          0.024551590904593468,
          0.015588962472975254,
          0.007760324981063604,
          -0.010007813572883606,
          0.026751261204481125,
          -0.016108138486742973,
          0.024278340861201286,
          -0.0026095458306372166,
          -0.003579586511477828,
          0.008696208707988262,
          0.0004265275492798537,
          -0.010547483339905739,
          0.03287208080291748,
          -0.0005921858828514814,
          0.0325988307595253,
          -0.011729293502867222,
          -0.02636870928108692,
          0.005953453481197357,
          -0.037571996450424194,
          0.0070157162845134735,
          -0.02453792840242386,
          0.004792137071490288,
          0.002737632254138589,
          -0.017091842368245125,
          0.010110282339155674,
          0.0030723644886165857,
          0.016982542350888252,
          0.009563780389726162,
          -0.013512255623936653,
          0.009673081338405609,
          0.004829709418118,
          0.0012287750141695142,
          0.0158485509455204,
          -0.007951600477099419,
          -0.008259007707238197,
          0.0018222418148070574,
          -0.012924766167998314,
          -0.015411349013447762,
          -0.022092333063483238,
          -0.023144349455833435,
          0.00914707314223051,
          -0.03057677298784256,
          0.03473018482327461,
          -0.002833270002156496,
          0.0007988318684510887,
          0.006264276336878538,
          0.012692502699792385,
          0.046234048902988434,
          -0.022379247471690178,
          -0.037408046424388885,
          0.013812831602990627,
          0.0038972406182438135,
          -0.006595592945814133,
          -0.016408715397119522,
          -0.050688035786151886,
          0.01008978858590126,
          -0.0005913319764658809,
          -0.013621555641293526,
          -0.004966334439814091,
          0.0068790907971560955,
          -0.001078487024642527,
          0.006527280434966087,
          -0.013621555641293526,
          0.022310933098196983,
          0.025425994768738747,
          -0.005365964025259018,
          -0.007965262979269028,
          -0.020097602158784866,
          0.007582711987197399,
          0.009181229397654533,
          -0.017802294343709946,
          -0.025207392871379852,
          0.031642451882362366,
          -0.012849622406065464,
          -0.04139750823378563,
          -0.00045854912605136633,
          -0.003917734604328871,
          -0.01182493194937706,
          -0.009256373159587383,
          0.006824440788477659,
          0.013013572432100773,
          0.004498392343521118,
          0.016955217346549034,
          0.0161491259932518,
          0.014386658556759357,
          0.002249196171760559,
          0.012275795452296734,
          0.006069585215300322,
          0.013225342147052288,
          -0.043911416083574295,
          0.0020425503607839346,
          0.015028798021376133,
          0.07498004287481308,
          0.006001272238790989,
          -0.02192838303744793,
          0.003606911515817046,
          -0.021381881088018417,
          -0.009290529415011406,
          -0.02020690217614174,
          -0.00872353371232748,
          0.0076510244980454445,
          0.022597847506403923,
          -0.013669375330209732,
          -0.00907192938029766,
          0.015903200954198837,
          -0.004320779349654913,
          0.03451158478856087,
          0.025303030386567116,
          0.01161999348551035,
          -0.03161512687802315,
          -0.023704513907432556,
          -0.014974148012697697,
          -4.317577258916572e-05,
          0.005721190012991428,
          0.02295307256281376,
          0.004822877701371908,
          0.015547974966466427,
          -0.017337767407298088,
          0.027147473767399788,
          0.0159988384693861,
          -0.03197035193443298,
          -0.01657266542315483,
          0.0087918471544981,
          0.012132339179515839,
          0.004013372119516134,
          0.012747153639793396,
          -0.006629749201238155,
          0.021299906075000763,
          0.010581640526652336,
          0.0010195673676207662,
          0.012883778661489487,
          -0.0031748334877192974,
          0.031669776886701584,
          0.024141713976860046,
          0.0024917065165936947,
          -0.003480532905086875,
          0.008798678405582905,
          -0.02451060339808464,
          -0.0019452046835795045,
          0.040304504334926605,
          0.016039825975894928,
          -0.03404705971479416,
          0.015370361506938934,
          0.005150779150426388,
          -0.030959324911236763,
          -0.010854890570044518,
          0.015588962472975254,
          0.025207392871379852,
          0.008634727448225021,
          -0.005082466173917055,
          -0.0008543359581381083,
          -0.026013482362031937,
          -0.018526408821344376,
          -0.03270813077688217,
          -0.013409786857664585,
          -0.02005661465227604,
          -0.01905924826860428,
          -0.0008052361663430929,
          -0.025316692888736725,
          0.008259007707238197,
          -0.03604178875684738,
          0.00815653894096613,
          0.0029203686863183975,
          -0.025781219825148582,
          -0.044375941157341,
          -0.003400265472009778,
          0.006110572721809149,
          -0.010199088603258133,
          0.028500067070126534,
          -0.006315510720014572,
          -0.003753783879801631,
          0.01269933395087719,
          -0.00019896079902537167,
          -0.0115448497235775,
          -0.016367727890610695,
          -0.009153904393315315,
          0.0038733312394469976,
          -0.0018529824446886778,
          -0.024920480325818062,
          -0.020821716636419296,
          0.0037571995053440332,
          0.01226896420121193,
          0.01254904642701149,
          -0.0077534937299788,
          0.00034455227432772517,
          0.009864356368780136,
          -0.007521230261772871,
          0.011654149740934372,
          0.025917844846844673,
          0.020589454099535942,
          -0.004597445949912071,
          -0.01890896074473858,
          -0.007295798510313034,
          -0.027516363188624382,
          -0.022761797532439232,
          -0.031642451882362366,
          0.005792918615043163,
          0.012562708929181099,
          0.028008215129375458,
          0.02306237444281578,
          0.002141603734344244,
          -0.010076126083731651,
          0.012091350741684437,
          -0.004249051213264465,
          0.022133320569992065,
          -0.006650242954492569,
          0.020944679155945778,
          0.010841228067874908,
          0.019291510805487633,
          -0.006274523213505745,
          0.01703719235956669,
          -0.02609545923769474,
          -0.01804821938276291,
          -0.047791577875614166,
          0.03464820981025696,
          0.006356498692184687,
          -0.0139221316203475,
          0.017692994326353073,
          0.014427646063268185,
          0.013211679644882679,
          -0.03328195586800575,
          0.012747153639793396,
          0.009987319819629192,
          0.024715540930628777,
          0.007582711987197399,
          -0.0014243201585486531,
          -0.011503862217068672,
          -0.02016591466963291,
          -0.01886797323822975,
          0.012938428670167923,
          -0.01527472399175167,
          -0.013799169100821018,
          0.011592668481171131,
          0.023035049438476562,
          2.3068885639077052e-05,
          -0.02352689951658249,
          0.016668302938342094,
          -0.025043442845344543,
          -0.008286332711577415,
          0.01886797323822975,
          0.010567978024482727,
          0.04686252400279045,
          -0.02468821592628956,
          0.026286734268069267,
          -0.03989462926983833,
          -0.026163771748542786,
          -7.952027772262227e-06,
          -0.028172165155410767,
          0.0026112536434084177,
          -0.0014098037499934435,
          0.030604097992181778,
          0.0159988384693861,
          0.04257248714566231,
          -0.014550609514117241,
          0.013355136848986149,
          0.00901727844029665,
          -0.005635799374431372,
          -0.008860159665346146,
          0.007172835525125265,
          -0.002059628488495946,
          -0.006042259745299816,
          -0.0038869937416166067,
          -0.0027120148297399282,
          -0.009413492865860462,
          0.025125417858362198,
          -0.004730655811727047,
          -0.00631209509447217,
          0.0005102106370031834,
          -0.015261061489582062,
          -0.012303120456635952,
          -0.02220163308084011,
          -0.02755735069513321,
          -0.02770763821899891,
          0.0022526117973029613,
          0.001700132736004889,
          0.015247398987412453,
          -0.019960977137088776,
          0.00017280982865486294,
          0.01601250097155571,
          0.012876947410404682,
          0.003238022793084383,
          -0.0032465618569403887,
          0.014263696037232876,
          0.006049090996384621,
          -0.00045129089266993105,
          0.023035049438476562,
          0.005946622230112553,
          -0.04199865832924843,
          -0.00526007916778326,
          -0.04279108718037605,
          0.0049629188142716885,
          0.02783060073852539,
          0.007466580253094435,
          0.017843281850218773,
          0.016777602955698967,
          -0.010171763598918915,
          0.00979604385793209,
          0.009058266878128052,
          0.003535183146595955,
          -0.008040406741201878,
          -0.011162297800183296,
          -0.03344590589404106,
          -0.0023123854771256447,
          -0.028172165155410767,
          -0.012583202682435513,
          -0.019127560779452324,
          -0.015124435536563396,
          -0.0017342891078442335,
          0.004621355328708887,
          0.030904673039913177,
          -0.029948296025395393,
          -0.02871866710484028,
          0.029237844049930573,
          0.018785998225212097,
          0.027161136269569397,
          -0.012863284908235073,
          0.024729203432798386,
          0.024578915908932686,
          0.013792337849736214,
          -0.01830780878663063,
          0.011845425702631474,
          0.027448050677776337,
          -0.007200160529464483,
          0.009024109691381454,
          0.003511273767799139,
          -0.012521721422672272,
          -0.00786279421299696,
          0.025029780343174934,
          0.01815752126276493,
          -0.0319976769387722,
          -0.04101495444774628,
          0.016053488478064537,
          0.008887484669685364,
          0.007125016767531633,
          -0.01631307788193226,
          -0.03270813077688217,
          -0.021231593564152718,
          0.012596865184605122,
          -0.007610036991536617,
          0.03218895196914673,
          -0.008696208707988262,
          -0.023581549525260925,
          0.01506978552788496,
          0.0005311314016580582,
          0.015343036502599716,
          -0.016504352912306786,
          -0.006489708088338375,
          0.014441308565437794,
          0.022420234978199005,
          -0.010984685271978378,
          -0.007172835525125265,
          0.007985756732523441,
          0.0003590687410905957,
          0.023253649473190308,
          0.013546411879360676,
          0.006076416466385126,
          -0.007384604774415493,
          0.019414475187659264,
          0.016668302938342094,
          -0.00907192938029766,
          0.0002085672749672085,
          0.030330847948789597,
          -0.02970236912369728,
          -0.01355324313044548,
          -0.008163370192050934,
          -0.015534312464296818,
          0.005953453481197357,
          -0.011292092502117157,
          -0.005482095759361982,
          -0.01616278849542141,
          0.01211867667734623,
          0.01240558922290802,
          0.0239231139421463,
          0.01601250097155571,
          -0.0061071570962667465,
          0.002361912280321121,
          -0.013881144113838673,
          -0.04790087789297104,
          0.001551552675664425,
          -0.001618157490156591,
          0.006438473705202341,
          -0.01933250017464161,
          0.011524355970323086,
          0.02594516985118389,
          -0.023704513907432556,
          0.005311314016580582,
          -0.0016770772635936737,
          -0.024742865934967995,
          -0.005164441652595997,
          0.013116042129695415,
          -0.02262517251074314,
          0.004631602205336094,
          0.019824350252747536,
          0.00814287643879652,
          -0.004973165690898895,
          0.010246907360851765,
          -0.00988485012203455,
          0.0012202359503135085,
          0.009037773124873638,
          -0.00577242486178875,
          -0.02449694089591503,
          -0.006493123713880777,
          -0.013928962871432304,
          0.009270035661756992,
          -0.0058714780025184155,
          -0.0016950092976912856,
          0.0070157162845134735,
          -0.010943697765469551,
          0.004594030324369669,
          -0.009850693866610527,
          -0.00801308173686266,
          0.022980399429798126,
          -0.009386167861521244,
          -0.0002309823757968843,
          0.013642050325870514,
          -0.0221879705786705,
          0.010000982321798801,
          -0.016395052894949913,
          0.001566922990605235,
          -0.019879000261425972,
          -0.03809117153286934,
          -0.012706165201961994,
          0.018813323229551315,
          0.002317508915439248,
          -0.005055141169577837,
          0.0019076326861977577,
          -0.015315711498260498,
          0.0025822208262979984,
          -0.009987319819629192,
          0.01859472133219242,
          0.0011211824603378773,
          0.014755547046661377,
          0.0015558222075924277,
          0.030658748000860214,
          0.019428137689828873,
          0.006547774188220501,
          -0.017665669322013855,
          -0.011742956005036831,
          -0.014304683543741703,
          -0.015206411480903625,
          -0.03273545578122139,
          -0.0071181850507855415,
          -0.03317265585064888,
          0.04918515682220459,
          -0.00222016335465014,
          -0.019660400226712227,
          -0.0016155957709997892,
          -0.01414073258638382,
          -0.030221546068787575,
          -0.011421886272728443,
          -0.008771353401243687,
          0.022857435047626495,
          0.012036700733006,
          0.02781693823635578,
          0.01045867707580328,
          0.02017957717180252,
          -0.008826003409922123,
          0.008163370192050934,
          -0.010731928050518036,
          -0.01132624875754118,
          0.006271107587963343,
          -0.00504489429295063,
          -0.006014934740960598,
          0.004358351230621338,
          -0.028664017096161842,
          -0.002032303484156728,
          0.0155069874599576,
          -0.0175836943089962,
          -0.006349666975438595,
          0.016217438504099846,
          -0.013088717125356197,
          -0.008259007707238197,
          0.017214804887771606,
          -0.01058847177773714,
          -0.0115448497235775,
          0.006233535706996918,
          0.007029378786683083,
          -0.0115448497235775,
          0.004423248581588268,
          0.002957940800115466,
          -0.025057105347514153,
          0.0007245417800731957,
          -0.013095548376441002,
          0.008607402443885803,
          0.005601643119007349,
          -9.211542783305049e-05,
          0.013218510895967484,
          -0.005243001040071249,
          -0.0011946186423301697,
          -0.01253538392484188,
          0.05079733580350876,
          0.002647117944434285,
          0.010246907360851765,
          0.020220564678311348,
          -0.004703330807387829,
          0.004696499556303024,
          -0.0071045225486159325,
          0.009966826066374779,
          0.007282136008143425,
          0.002141603734344244,
          0.028062865138053894,
          -0.025589944794774055,
          -0.01746073178946972,
          -0.012371432967483997,
          -0.0018615216249600053,
          -0.024401303380727768,
          -0.021860070526599884,
          -0.015834888443350792,
          -0.001762468134984374,
          -0.03153315186500549,
          0.01804821938276291,
          0.010861721821129322,
          -0.01491949800401926,
          -0.0008299995097331703,
          -0.007760324981063604,
          0.006435058079659939,
          -0.010363039560616016,
          -0.02959306910634041,
          0.012637852691113949,
          -0.018389783799648285,
          0.0009461311274208128,
          0.0019161717500537634,
          -0.007418761029839516,
          -0.011428717523813248,
          0.012487565167248249,
          0.009693575091660023,
          -0.022010358050465584,
          0.00306382542476058,
          0.2013312429189682,
          -0.026027144864201546,
          -0.005676786880940199,
          0.012385095469653606,
          -0.00025275704683735967,
          -0.020370852202177048,
          0.04128820821642876,
          0.0023516654036939144,
          -0.004604277200996876,
          0.013737687841057777,
          -0.007685180753469467,
          0.004529133439064026,
          -0.024469615891575813,
          -0.0006989245302975178,
          0.0065170335583388805,
          -0.024114388972520828,
          -0.04820145294070244,
          -0.03371915966272354,
          -0.01434567105025053,
          0.008914809674024582,
          -0.012733491137623787,
          0.012931597419083118,
          -0.016244765371084213,
          -0.026792248710989952,
          0.020261552184820175,
          -0.002981850178912282,
          0.004013372119516134,
          0.01978336274623871,
          0.0012586618540808558,
          0.011838594451546669,
          -0.0016173035837709904,
          0.005628968123346567,
          -0.0016036410816013813,
          0.0033439076505601406,
          0.0052498322911560535,
          -0.01818484626710415,
          0.00663658045232296,
          -0.0036888867616653442,
          0.010438183322548866,
          0.022816447541117668,
          0.01848542131483555,
          0.0011544849257916212,
          0.028090190142393112,
          -0.03158780187368393,
          0.011920569464564323,
          0.044184666126966476,
          -0.006349666975438595,
          -0.0016249887412413955,
          -0.008552752435207367,
          -0.0019503281218931079,
          -0.022297270596027374,
          0.01147653628140688,
          0.017351429909467697,
          0.03145117685198784,
          0.0017829620046541095,
          0.006920078303664923,
          -0.02612278424203396,
          0.01743340492248535,
          0.0008654367411509156,
          -0.005232754163444042,
          -0.03404705971479416,
          -0.024578915908932686,
          0.001741120358929038,
          0.010718265548348427,
          -0.012350939214229584,
          -0.006762959063053131,
          -0.009352011606097221,
          0.0062301200814545155,
          -0.0026932288892567158,
          -0.014577934518456459,
          -0.006834687665104866,
          -0.004682837054133415,
          -0.023117024451494217,
          0.012781309895217419,
          -0.026450684294104576,
          -0.05205428972840309,
          0.04033182933926582,
          0.033254630863666534,
          -0.008190695196390152,
          0.03131455183029175,
          -0.004798968322575092,
          -0.023253649473190308,
          0.0016975710168480873,
          -0.026764923706650734,
          -0.006383823696523905,
          -0.02885529212653637,
          0.018266821280121803,
          -0.020999329164624214,
          -0.020411839708685875,
          -0.007022547535598278,
          -0.007869625464081764,
          -0.018239496275782585,
          0.0033661092165857553,
          0.01132624875754118,
          0.002440471900627017,
          0.009597936645150185,
          0.020001964643597603,
          -0.005007322411984205,
          0.004337857477366924,
          -0.016900567337870598,
          -0.021422868594527245,
          0.05268276855349541,
          0.02841809019446373,
          -0.00823168270289898,
          0.01196155697107315,
          0.016559002920985222,
          -0.017679331824183464,
          -0.002886212430894375,
          0.0035693396348506212,
          -0.016039825975894928,
          -0.01543867401778698,
          -0.034675534814596176,
          0.01759735681116581,
          0.007329954765737057,
          -0.006862012669444084,
          0.01030838955193758,
          -0.0034070967230945826,
          -0.0061959633603692055,
          0.027898915112018585,
          0.0005285696825012565,
          -8.635154517833143e-05,
          -0.015903200954198837,
          0.0047750589437782764,
          0.0025736817624419928,
          0.005485511384904385,
          -0.02410072647035122,
          -0.014168057590723038,
          0.0013303902233019471,
          -0.036998167634010315,
          0.000961501500569284,
          0.00674588093534112,
          -0.036998167634010315,
          0.00573826814070344,
          0.012794972397387028,
          -0.004734071437269449,
          -0.004949256312102079,
          0.010547483339905739,
          -0.0025122002698481083,
          -0.009215385653078556,
          -0.006547774188220501,
          -0.0036888867616653442,
          0.011285261251032352,
          -0.008853328414261341,
          -0.005365964025259018,
          0.01800723187625408,
          -0.0040543596260249615,
          0.014113407582044601,
          0.01991998963057995,
          -0.01284279115498066,
          -0.00771933700889349,
          -0.019510112702846527,
          -0.0015686308033764362,
          0.004559874068945646,
          -0.0018137026345357299,
          0.019113898277282715,
          0.02032986469566822,
          -0.0248248428106308,
          0.013109210878610611,
          0.006226704455912113,
          0.024128051474690437,
          -0.04298236221075058,
          0.017050854861736298,
          0.01743340492248535,
          -0.012296289205551147,
          -0.011230611242353916,
          -0.01776130683720112,
          -0.17575496435165405,
          -0.005991025362163782,
          0.003934812732040882,
          -0.05298334360122681,
          0.02783060073852539,
          0.0221879705786705,
          0.019236860796809196,
          -0.013573736883699894,
          -0.032407552003860474,
          -0.0009435694082640111,
          0.008375138975679874,
          -0.008976290933787823,
          -0.017652006819844246,
          -0.009194891899824142,
          -0.015629950910806656,
          0.013034067116677761,
          -0.005348885897547007,
          0.01989266276359558,
          0.02091735415160656,
          0.0015250814612954855,
          0.037544671446084976,
          -0.026942536234855652,
          -0.01670929044485092,
          -0.017201142385601997,
          0.011777112260460854,
          0.007145510520786047,
          -0.004149997606873512,
          0.014413983561098576,
          -0.012282626703381538,
          -0.022160645574331284,
          -0.010294727049767971,
          -0.014427646063268185,
          0.03544063866138458,
          -0.007493905257433653,
          0.024742865934967995,
          0.015261061489582062,
          0.032653480768203735,
          0.0010340837761759758,
          -0.03401973471045494,
          0.002466089092195034,
          0.015793900936841965,
          0.036123763769865036,
          0.02105397917330265,
          0.002672735136002302,
          -0.011203286238014698,
          0.014195382595062256,
          0.016326740384101868,
          -0.03462088480591774,
          0.016258427873253822,
          -0.01573925092816353,
          0.027338750660419464,
          -0.02508443035185337,
          0.0233902744948864,
          -0.00411584135144949,
          0.010861721821129322,
          0.025125417858362198,
          0.0027444635052233934,
          0.004932178184390068,
          -0.010697771795094013,
          -0.019113898277282715,
          -0.012378264218568802,
          -0.011517524719238281,
          0.0038528372533619404,
          -0.009386167861521244,
          -0.015534312464296818,
          -0.028199490159749985,
          -0.013375630602240562,
          0.010950529016554356,
          -0.02681957371532917,
          0.007657855749130249,
          -0.020958341658115387,
          -0.0034634547773748636,
          -0.011162297800183296,
          -0.015889538452029228,
          0.031068624928593636,
          0.023117024451494217,
          0.0005482095875777304,
          0.015124435536563396,
          0.027297763153910637,
          1.6437747035524808e-05,
          -0.008545921184122562,
          0.01916854828596115,
          -0.0032619324047118425,
          0.024852167814970016,
          -0.016627315431833267,
          -0.008101888000965118,
          0.008443452417850494,
          0.014577934518456459,
          -0.028773317113518715,
          -0.010656784288585186,
          0.02522105537354946,
          -0.02090369164943695,
          -0.008040406741201878,
          -0.01508344803005457,
          -0.015302048996090889,
          0.004727240186184645,
          0.00987118761986494,
          0.00013353001850191504,
          -0.0030450394842773676,
          -0.0009785797446966171,
          0.013573736883699894,
          0.00886699091643095,
          -0.0013431988190859556,
          -0.011879581958055496,
          0.037544671446084976,
          0.0018137026345357299,
          -0.030604097992181778,
          0.011134972795844078,
          0.03874697536230087,
          -0.019564762711524963,
          -0.02855471707880497,
          -0.008306826464831829,
          -0.008033575490117073,
          -0.0007518668426200747,
          -0.008771353401243687,
          0.015684600919485092,
          -0.0137718440964818,
          -0.019414475187659264,
          0.016982542350888252,
          0.001952035934664309,
          0.03718944266438484,
          0.00609691021963954,
          -0.001953743863850832,
          0.005277157295495272,
          0.00015156884910538793,
          -0.011196454986929893,
          -0.11706067621707916,
          -0.021805420517921448,
          0.0030843191780149937,
          0.015179086476564407,
          -0.012214314192533493,
          0.0316971018910408,
          -0.00508929742500186,
          0.04546894505620003,
          -0.029046567156910896,
          0.04560557007789612,
          -0.01434567105025053,
          -0.008853328414261341,
          0.007145510520786047,
          -0.0026915210764855146,
          0.014332008548080921,
          -0.00014996776008047163,
          -0.0023875294718891382,
          -0.01815752126276493,
          -0.03587783873081207,
          0.023540562018752098,
          0.019441800191998482,
          -0.0012697626370936632,
          0.007657855749130249,
          -0.03044014796614647,
          -0.0058475686237216,
          -0.01299307867884636,
          -0.03937545046210289,
          0.02798089012503624,
          0.023567887023091316,
          0.0017385586397722363,
          0.009030940942466259,
          -0.028664017096161842,
          0.005666540004312992,
          -0.011339911259710789,
          0.010076126083731651,
          -0.01024007610976696,
          0.010889047756791115,
          -0.014195382595062256,
          0.01802089437842369,
          -0.012986247427761555,
          -0.002857179380953312,
          0.0079242754727602,
          0.013676206581294537,
          -0.010226413607597351,
          -0.007125016767531633,
          -0.02319899946451187,
          -0.008532258681952953,
          0.011674643494188786,
          0.007370942272245884,
          -0.0155069874599576,
          -0.03951207548379898,
          -0.02064410410821438,
          -0.04453989118337631,
          0.013525918126106262,
          0.01875867322087288,
          0.0036205740179866552,
          0.018130196258425713,
          0.016121800988912582,
          -0.013560074381530285,
          -0.00030975547269918025,
          -0.011380898766219616,
          -0.004508639220148325,
          -0.027174798771739006,
          -0.00344296102412045,
          0.01833513379096985,
          -0.0027308010030537844,
          6.217524787643924e-05,
          -0.02494780533015728,
          0.009816537611186504,
          -0.00959110539406538,
          0.006933740805834532,
          0.022283608093857765,
          -0.005854399874806404,
          0.007610036991536617,
          -0.01916854828596115,
          0.0009085591300390661,
          -0.013355136848986149,
          -0.03500343859195709,
          -0.0031270147301256657,
          0.0089421346783638,
          -0.010513327084481716,
          -0.01325949840247631,
          0.007097691297531128,
          -0.0041534132324159145,
          6.009384378558025e-05,
          -0.0029237843118608,
          -0.02020690217614174,
          0.0007377773872576654,
          0.0009128287201747298,
          -0.014413983561098576,
          0.012631021440029144,
          0.028636692091822624,
          0.004518886562436819,
          -0.01414073258638382,
          0.0012347523588687181,
          0.010725096799433231,
          -0.019414475187659264,
          -0.005748515482991934,
          0.028472740203142166,
          0.003120183479040861,
          -0.02724311128258705,
          0.009468142874538898,
          -0.055879805237054825,
          0.042927712202072144,
          0.0014055342180654407,
          -0.010759253054857254,
          -0.0011988881742581725,
          -0.02031620219349861,
          0.01613546349108219,
          -0.020083939656615257,
          0.005410367157310247,
          0.008340982720255852,
          -0.01053382083773613,
          0.016340402886271477,
          -0.01434567105025053,
          -0.0005213114200159907,
          -0.021586818620562553,
          -0.005140532273799181,
          0.018116533756256104,
          0.009679912589490414,
          0.007603205740451813,
          -0.0014644538750872016,
          0.007603205740451813,
          -0.0177339818328619,
          0.022966735064983368,
          0.017925256863236427,
          0.010889047756791115,
          -0.01508344803005457,
          -0.02583586983382702,
          0.015247398987412453,
          -0.009167566895484924,
          -0.010772915557026863,
          -0.01586221344769001,
          -0.01161999348551035,
          -0.006800530944019556,
          0.0078013124875724316,
          -0.019414475187659264,
          -0.010608965530991554,
          0.024633565917611122,
          0.032653480768203735,
          0.026505334302783012,
          0.04626137390732765,
          -0.021231593564152718,
          -0.04254516214132309,
          0.010062463581562042,
          -0.02046648971736431,
          -0.0089421346783638,
          -0.023103361949324608,
          -0.007876456715166569,
          -0.000488435965962708,
          0.02090369164943695,
          0.003811849746853113,
          0.037134792655706406,
          0.015479662455618382,
          -0.029155869036912918,
          -0.03016689606010914,
          -0.008238513953983784,
          0.013778675347566605,
          0.005266910418868065,
          -0.00022436458675656468,
          -0.0013218511594459414,
          0.025207392871379852,
          0.029620394110679626,
          0.005741683766245842,
          0.007077197544276714,
          -0.004891190677881241,
          -0.0018478590063750744,
          -0.006817609537392855,
          -0.007125016767531633,
          0.004573536571115255,
          0.0034258828964084387,
          0.008545921184122562,
          -0.006499954964965582,
          0.011859088204801083,
          0.00382209662348032,
          -0.001369670033454895,
          0.012385095469653606,
          -0.021231593564152718,
          0.016340402886271477,
          0.016477027907967567,
          -0.01804821938276291,
          0.006503370590507984,
          0.008026744239032269,
          -0.016968879848718643,
          0.01081390306353569,
          0.002855471568182111,
          0.023882126435637474,
          0.006042259745299816,
          -0.015957850962877274,
          0.0022594432812184095,
          -0.008921640925109386,
          -0.005417198408395052,
          0.0008667176007293165,
          -0.0036000802647322416,
          -0.011893244460225105,
          0.01670929044485092,
          0.021996695548295975,
          -0.009693575091660023,
          0.00428662309423089,
          0.011988881975412369,
          0.017091842368245125,
          -0.0033251214772462845,
          0.010199088603258133,
          -0.01978336274623871,
          0.007575880270451307,
          -0.010151269845664501,
          -0.033254630863666534,
          0.005994440987706184,
          -0.03153315186500549,
          -0.010410858318209648,
          0.03000294603407383,
          0.014837522059679031,
          0.0156982634216547,
          -0.0036957180127501488,
          0.006708309054374695,
          0.00843662116676569,
          -0.01399044506251812,
          0.020111264660954475,
          -0.010281064547598362,
          -0.0038050184957683086,
          -0.032407552003860474,
          0.039867304265499115,
          0.044922444969415665,
          0.017474394291639328,
          0.040741704404354095,
          -0.016791265457868576,
          0.005994440987706184,
          0.0014337131287902594,
          0.024319328367710114,
          -0.028008215129375458,
          0.014058757573366165,
          -0.01089587900787592,
          -0.013314148411154747,
          -0.008224851451814175,
          -0.012972584925591946,
          0.005082466173917055,
          -0.006113988347351551,
          1.228828386956593e-05,
          -0.012289457954466343,
          0.02205134555697441,
          -0.0017044023843482137,
          0.05563387647271156,
          0.016490690410137177,
          0.0022696901578456163,
          0.02665562368929386,
          -0.014618922024965286,
          0.015302048996090889,
          0.00849810242652893,
          0.025740232318639755,
          -0.00522933853790164,
          -0.017488056793808937,
          -0.0015566761139780283,
          -0.004143166355788708,
          -0.005755346734076738,
          -0.028336115181446075,
          -0.013006741181015968,
          -0.00310310535132885,
          -0.007562217768281698,
          0.013621555641293526,
          -0.0073777735233306885,
          -0.007917444221675396,
          0.02378648892045021,
          0.005051725544035435,
          0.023089699447155,
          0.007507567759603262,
          -0.0051473635248839855,
          0.0007343617035076022,
          0.028308790177106857,
          0.001484947744756937,
          0.0008744028164073825,
          -0.006810777820646763,
          0.03429298475384712,
          0.01873134821653366,
          -0.0335005559027195,
          -0.017952581867575645,
          -0.001010174280963838,
          -0.0016497521428391337,
          0.014045095071196556,
          -0.04008590430021286,
          -0.005161026027053595,
          -0.009638924151659012,
          -0.006711724679917097,
          0.008894315920770168,
          -0.0020562128629535437,
          -0.02263883501291275,
          0.0010853182757273316,
          0.00042780840885825455,
          -0.016326740384101868,
          -0.004112425725907087,
          -0.015821225941181183
        ]
      },
      "type": "document"
    },
    {
      "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
      "properties": {
        "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
        "themes": [
          "Prompt driven app generation",
          "Universal access to AI models",
          "Agents in AI",
          "Evaluations in AI",
          "Apple's AI capabilities",
          "Inference-scaling reasoning models",
          "Environmental impact of AI",
          "Synthetic training data",
          "Knowledge distribution in AI",
          "Criticism of LLMs",
          "GPT-4 advancements",
          "Chatbot Arena Leaderboard",
          "Increased context lengths in LLMs",
          "Anthropic's Claude series",
          "Competition in AI model development",
          "Running LLMs on personal devices",
          "Model efficiency improvements",
          "LLM pricing and cost reduction"
        ],
        "entities": [
          "Apple",
          "China",
          "OpenAI",
          "Google",
          "Gemini 1.5 Pro",
          "Claude 3",
          "Anthropic",
          "GPT-4",
          "Meta",
          "Llama 3.2"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
      "properties": {
        "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
        "entities": [
          "OpenAI",
          "GPT-4o",
          "GPT-3.5",
          "Anthropic",
          "Claude 3",
          "Google",
          "Gemini 1.5 Flash",
          "California Academy of Sciences",
          "Mistral",
          "Meta"
        ],
        "themes": [
          "Pricing changes in AI models",
          "Increased competition",
          "Increased efficiency",
          "Environmental impact of AI",
          "Multi-modal LLMs",
          "Voice and live video modes",
          "Advanced Voice mode",
          "Audio and video input/output",
          "OpenAI's Whisper and tts-1 models",
          "Google's Gemini models"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "f3588744-0d04-4d05-8670-70021648e3e8",
      "properties": {
        "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
        "entities": [
          "Google Gemini",
          "ChatGPT",
          "OpenAI",
          "WebSocket API",
          "WebRTC API",
          "GPT-4",
          "Anthropic",
          "Claude Artifacts",
          "Claude 3.5 Sonnet",
          "GitHub Spark"
        ],
        "themes": [
          "Real-time camera feed analysis",
          "Google Gemini",
          "OpenAI API",
          "WebRTC API",
          "Prompt-driven app generation",
          "Claude Artifacts",
          "Interactive applications",
          "Universal access to AI models",
          "Subscription services for AI models",
          "AI agents and autonomy"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
      "properties": {
        "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
        "themes": [
          "AGI and gullibility",
          "Test-driven development for system prompts",
          "Automated evaluations for LLM systems",
          "Apple's MLX library",
          "Inference-scaling reasoning models",
          "OpenAI's o1 and o3 models",
          "Google's gemini-2.0-flash-thinking-exp",
          "Alibaba's Qwen and QvQ models",
          "DeepSeek's models",
          "LLM architecture advancements"
        ],
        "entities": [
          "AGI",
          "Anthropic",
          "Amanda Askell",
          "Claude",
          "Vercel",
          "Malte Ubl",
          "ASML",
          "Apple",
          "NVIDIA",
          "Hugging Face"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
      "properties": {
        "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
        "entities": [
          "DeepSeek v3",
          "Meta",
          "Llama",
          "Claude 3.5 Sonnet",
          "Gemini 2.0",
          "OpenAI",
          "Google",
          "Amazon",
          "New York",
          "London"
        ],
        "themes": [
          "DeepSeek v3",
          "Large language models",
          "Training cost",
          "Environmental impact",
          "Infrastructure buildout",
          "AI-generated content",
          "Slop",
          "Synthetic training data",
          "Model collapse",
          "Energy efficiency"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
      "properties": {
        "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
        "themes": [
          "Reasoning patterns",
          "Training data creation",
          "Large language models (LLMs)",
          "User experience with LLMs",
          "Capabilities and limitations of LLMs",
          "Knowledge gap in AI technology",
          "Criticism of LLMs",
          "Ethics and environmental impact",
          "Responsible use of AI tools",
          "Hype and misinformation in AI"
        ],
        "entities": [
          "DeepSeek v3",
          "DeepSeek-R1",
          "Meta",
          "Llama 3.3",
          "OpenAI",
          "ChatGPT",
          "Claude",
          "Python",
          "JavaScript",
          "GPT-4o"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
      "properties": {
        "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
        "themes": [
          "Large Language Models",
          "GPT-4",
          "Technological advancements",
          "Price reduction",
          "Competition",
          "Efficiency",
          "Multimodal vision",
          "Audio and video integration",
          "Voice technology",
          "Live camera mode"
        ],
        "entities": [
          "Simon Willison",
          "Weblog",
          "LLMs",
          "2024",
          "31st December 2024",
          "Large Language Models",
          "2023",
          "GPT-4"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
      "properties": {
        "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
        "themes": [
          "Large Language Models (LLMs)",
          "Artificial Intelligence",
          "Open Source",
          "Prompt engineering",
          "AI-generated content",
          "AI for Data Journalism",
          "AI engineering challenges",
          "AI in software development",
          "AI in multimedia processing",
          "AI in misinformation tracking"
        ],
        "entities": [
          "Artificial Intelligence",
          "Open Source LLMs",
          "Gemini Pro",
          "GPT-4",
          "Claude",
          "ChatGPT",
          "Llama 3",
          "WWDC 2024",
          "PyCon US 2024",
          "OpenAI"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
      "properties": {
        "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
        "themes": [
          "Large Language Models (LLMs)",
          "Training data",
          "OpenAI",
          "Model training costs",
          "Local deployment of LLMs",
          "Fine-tuning models",
          "Open vs closed models",
          "GPT-4",
          "Innovation in AI",
          "Ethics of AI"
        ],
        "entities": [
          "2023",
          "2024",
          "OpenAI",
          "Anthropic",
          "Mistral",
          "Google",
          "Meta",
          "EleutherAI",
          "Stability AI",
          "Microsoft Research"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
      "properties": {
        "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
        "themes": [
          "LLMs as black boxes",
          "Prompting challenges",
          "Evaluation of LLMs",
          "Vibes Based Development",
          "LLM capabilities and limitations",
          "Gullibility in language models",
          "AI personal assistants",
          "Code generation by LLMs",
          "Hallucination in LLMs",
          "Ethics of training data"
        ],
        "entities": [
          "LLMs",
          "ChatGPT",
          "September",
          "AGI",
          "Python",
          "JavaScript",
          "Chinese",
          "Spanish",
          "English",
          "Andy Baio"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
      "properties": {
        "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
        "themes": [
          "Large Language Models",
          "Artificial Intelligence",
          "Breakthrough year",
          "Academic development",
          "Building LLMs",
          "Running LLMs on personal devices",
          "Hobbyist model building",
          "GPT-4",
          "Vibes Based Development",
          "Gullibility problem"
        ],
        "entities": [
          "Simon Willison",
          "Weblog",
          "AI",
          "2023",
          "Large Language Models",
          "LLMs",
          "Artificial Intelligence",
          "1950s",
          "GPT-4"
        ]
      },
      "type": "chunk"
    },
    {
      "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
      "properties": {
        "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
        "themes": [
          "Large Language Models (LLMs)",
          "AI ethics",
          "Legal implications of AI",
          "Impact of AI on employment",
          "Generative AI",
          "Prompt injection",
          "AI-enhanced development",
          "OpenAI",
          "AI journalism",
          "AI in 2023"
        ],
        "entities": [
          "LLMs",
          "AI",
          "ChatGPT",
          "OpenAI",
          "Google",
          "Stable Diffusion",
          "DALL-E 3",
          "Stanford Alpaca",
          "Llama 2",
          "Simon Willison"
        ]
      },
      "type": "chunk"
    }
  ],
  "relationships": [
    {
      "id": "78363733-b056-4be3-9f97-9bd95a5e1d5a",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "ab74f400-7267-48cb-90a4-5abb8d8b1f36",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "2cce2760-e1cd-43e6-9dc3-e077e9d81acf",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "265bce7c-72b2-4a4f-8a08-6cfbb28e66e9",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "3b288afa-ef9f-4933-a3e7-de90356e963c",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "7322d087-4f2d-4c2f-85e2-3d5b44b8622b",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "631da3aa-1e52-41bf-9335-09b09b17d726",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "7fc77626-ba7a-4765-aa74-48a561675bb5",
      "type": "child",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "6156be9d-949b-4533-bc7e-65b3e2d739a3",
      "type": "next",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "c5455272-3735-42f6-996e-d1cb65d305df",
      "type": "next",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "aad4ea93-5d5b-4567-9a03-e44ba7530923",
      "type": "next",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "2d79fa59-d8b5-4195-8d9d-bc1c9ce19e17",
      "type": "next",
      "source": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "898f9ddf-f797-483e-8e92-f7ea94cbe652",
      "type": "next",
      "source": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "15e3d6ec-7c2b-4363-ad5f-59d4e9945446",
      "type": "next",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "8791b190-6d83-492a-a369-3282087d60e8",
      "type": "next",
      "source": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "1607365b-3d00-4db5-8808-cae8150388a9",
      "type": "child",
      "source": {
        "id": "1d35c58e-1af7-4136-bfeb-f0894fc21db0",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere’s my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey’re actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don’t yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n\nThe ethics of this space remain diabolically complex\n\nMy blog in 2023\n\nHere’s the sequel to this post: Things we learned about LLMs in 2024.\n\nLarge Language Models\n\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\n\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\n\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\n\nSo far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\n\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\n\nThey’re actually quite easy to build\n\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\n\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\n\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\n\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\n\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\n\nThe training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing.\n\nSo training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\n\nYou can run LLMs on your own devices\n\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\n\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\n\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\n\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use.\n\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\n\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\n\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\n\nHobbyists can build their own fine-tuned models\n\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\n\nThere’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\n\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours.\n\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\n\nThis is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\n\nWe don’t yet know how to build GPT-4\n\nFrustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4.\n\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\n\nThis may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out.\n\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then.\n\nStill, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet.\n\nVibes Based Development\n\nAs a computer scientist and software engineer, LLMs are infuriating.\n\nEven the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\n\nI’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\n\nThe worst part is the challenge of evaluating them.\n\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task.\n\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself!\n\nThe most frustrating thing for me is at the level of individual prompting.\n\nSometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out.\n\nWe’re left with what’s effectively Vibes Based Development. It’s vibes all the way down.\n\nI’d love to see us move beyond vibes in 2024!\n\nLLMs are really smart, and also really, really dumb\n\nOn the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun!\n\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\n\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\n\nThe honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\n\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead.\n\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works!\n\nGullibility is the biggest unsolved problem\n\nI coined the term prompt injection in September last year.\n\n15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem.\n\nI’ve written a ton about this already.\n\nBeyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility.\n\nLanguage Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt.\n\nIn order to be useful tools for us, we need them to believe what we feed them!\n\nBut it turns out a lot of the things we want to build need them not to be gullible.\n\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\n\nA lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\n\nI think this is because of gullibility.\n\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\n\nCode may be the best application\n\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.\n\nIf you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\n\nIt’s still astonishing to me how effective they are though.\n\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\n\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\n\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\n\nHow should we feel about this as software engineers?\n\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\n\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\n\nThe ethics of this space remain diabolically complex\n\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\n\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\n\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\n\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\n\nLaw is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\n\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\n\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\n\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\n\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\n\nMy blog in 2023\n\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\n\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\n\nI’ve written a lot about this stuff!\n\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\n\nArticle Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k\n\nI also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\n\nPrompt injection explained, with video, slides, and a transcript\n\nCatching up on the weird world of LLMs\n\nMaking Large Language Models work for you\n\nOpen questions for AI engineering\n\nEmbeddings: What they are and why they matter\n\nFinancial sustainability for open source projects at GitHub Universe\n\nAnd in podcasts:\n\nWhat AI can do for you on the Theory of Change\n\nWorking in public on Path to Citus Con\n\nLLMs break the internet on the Changelog\n\nTalking Large Language Models on Rooftop Ruby\n\nThoughts on the OpenAI board situation on Newsroom Robots\n\nIndustry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1089\n\ngenerative-ai\n            934\n\nllms\n            922\n\nNext: Tom Scott, and the formidable power of escalating streaks\n\nPrevious: Last weeknotes of 2023\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2023_llms.html"
          },
          "headlines": [
            "Large Language Models",
            "Vibes Based Development",
            "LLMs are really smart, and also really, really dumb",
            "Gullibility is the biggest unsolved problem",
            "Code may be the best application"
          ],
          "summary": "In 2023, Large Language Models (LLMs) emerged as a significant development in AI, with advancements making them easier to build and run on personal devices. Despite their capabilities, LLMs face challenges like gullibility and ethical concerns over unlicensed training data. They excel in code generation, where errors can be iteratively corrected. The year saw a proliferation of LLMs from various organizations, though none surpassed GPT-4. The complexity of LLMs remains a barrier, with 'Vibes Based Development' highlighting the difficulty in evaluating their performance. The ethical and legal implications of AI continue to be complex and unresolved.",
          "summary_embedding": [
            -0.01449595857411623,
            -0.005809996742755175,
            0.0007078905473463237,
            -0.03407438471913338,
            -0.0034515000879764557,
            0.01598517596721649,
            -0.015042460523545742,
            0.02121793106198311,
            -0.023909451439976692,
            -0.04071437940001488,
            0.025603607296943665,
            0.020726079121232033,
            -0.023991426452994347,
            -0.0034241750836372375,
            0.0019861923065036535,
            0.0180345568805933,
            0.021983033046126366,
            0.0025992989540100098,
            0.017529044300317764,
            -0.007432423997670412,
            -0.0009734562481753528,
            -0.0001246707106474787,
            -0.014441308565437794,
            -0.017406079918146133,
            -0.005376210901886225,
            0.006428226828575134,
            0.022898422554135323,
            -0.03661561757326126,
            -0.014618922024965286,
            -0.0006575098959729075,
            0.02032986469566822,
            -0.0041534132324159145,
            -0.03789989650249481,
            0.007480242755264044,
            -0.029647719115018845,
            -0.00886699091643095,
            0.013819662854075432,
            -0.007883287966251373,
            0.020575791597366333,
            0.005519667640328407,
            0.028800642117857933,
            0.02869134210050106,
            0.015138098038733006,
            -0.02002928964793682,
            -0.029073892161250114,
            0.017815956845879555,
            0.004792137071490288,
            -0.01644970290362835,
            -0.03341858088970184,
            0.012070856988430023,
            0.019318837672472,
            0.037872571498155594,
            -0.011421886272728443,
            -0.014536946080625057,
            -0.00784913171082735,
            0.002877673367038369,
            -0.014113407582044601,
            0.01066361553966999,
            0.017706656828522682,
            0.00035437222686596215,
            -0.006291601341217756,
            -0.01573925092816353,
            -0.002004978246986866,
            0.024879492819309235,
            -0.006978144403547049,
            -0.010670446790754795,
            -0.027584675699472427,
            0.03314533084630966,
            0.033965084701776505,
            -0.013266329653561115,
            -0.0027427556924521923,
            -0.008095056749880314,
            0.019154885783791542,
            -0.011121310293674469,
            0.017925256863236427,
            0.007992587983608246,
            -0.006277938839048147,
            -0.015261061489582062,
            -0.0275436881929636,
            0.004747733939439058,
            -0.0019281264394521713,
            -0.0006651951116509736,
            -0.003910903353244066,
            0.013150198385119438,
            0.013744519092142582,
            0.004597445949912071,
            -0.005208844784647226,
            -0.017692994326353073,
            -0.0015805854927748442,
            -0.013437111862003803,
            -0.005540161393582821,
            -0.005652877502143383,
            0.016531677916646004,
            0.017228467389941216,
            -0.0062813544645905495,
            -0.0023106776643544436,
            -0.008962628431618214,
            0.002288476098328829,
            -0.005642630625516176,
            -0.026764923706650734,
            -0.00512003805488348,
            0.0024695047177374363,
            -0.040905654430389404,
            -0.008313657715916634,
            -0.023608876392245293,
            0.0037196276243776083,
            0.015684600919485092,
            0.021245256066322327,
            0.015124435536563396,
            0.018949948251247406,
            -0.00663658045232296,
            0.027652988210320473,
            0.016518015414476395,
            -0.02136821858584881,
            -0.0024438875261694193,
            -0.015807563439011574,
            0.0029476936906576157,
            -0.0006071293028071523,
            0.005079050548374653,
            0.008095056749880314,
            0.031095949932932854,
            0.010404027067124844,
            0.013778675347566605,
            -0.0019042170606553555,
            0.019701387733221054,
            -0.015151760540902615,
            0.005929544102400541,
            -0.020726079121232033,
            -0.01234410796314478,
            -0.0003955733263865113,
            0.0020237641874700785,
            -0.0027290931902825832,
            0.010294727049767971,
            0.0027188463136553764,
            -0.026901548728346825,
            0.031369201838970184,
            -0.023704513907432556,
            -0.0034737016540020704,
            0.009386167861521244,
            0.010397195816040039,
            0.0009649171261116862,
            0.02467455342411995,
            -0.022761797532439232,
            0.017515381798148155,
            -0.004358351230621338,
            0.018717685714364052,
            0.009413492865860462,
            0.013361968100070953,
            -0.0024353484623134136,
            -0.000552052166312933,
            0.004450573585927486,
            -0.03153315186500549,
            -0.01802089437842369,
            0.02321266196668148,
            0.004679421428591013,
            0.004518886562436819,
            0.00696106581017375,
            0.023267311975359917,
            -0.031369201838970184,
            -0.010349377058446407,
            -0.014741884544491768,
            0.009563780389726162,
            -0.0032328993547707796,
            0.019797025248408318,
            0.01363521907478571,
            0.021245256066322327,
            0.028800642117857933,
            0.0026932288892567158,
            -0.006749296560883522,
            -0.014687234535813332,
            -0.014427646063268185,
            0.030604097992181778,
            -0.03085002303123474,
            0.011968388222157955,
            -0.015138098038733006,
            0.020111264660954475,
            0.023321961984038353,
            -0.0037025492638349533,
            -0.024783853441476822,
            -0.018553733825683594,
            0.01804821938276291,
            0.009666250087320805,
            0.01613546349108219,
            -0.009270035661756992,
            -0.013505424372851849,
            0.0011843717657029629,
            0.010499664582312107,
            0.0041397507302463055,
            0.012214314192533493,
            -0.02293941006064415,
            0.027652988210320473,
            0.018581058830022812,
            -0.005468433257192373,
            -0.028800642117857933,
            -0.6571137309074402,
            -0.0369708426296711,
            0.015821225941181183,
            -0.010690940544009209,
            0.002980142366141081,
            -0.0036342365201562643,
            0.0009486929047852755,
            -0.01284279115498066,
            -0.015343036502599716,
            0.03306335583329201,
            -0.013525918126106262,
            0.006315510720014572,
            -0.003108228789642453,
            -0.019646737724542618,
            0.011216948740184307,
            -0.009782381355762482,
            0.0042900387197732925,
            -0.026478009298443794,
            0.007610036991536617,
            0.018376121297478676,
            -0.005273741669952869,
            0.019072910770773888,
            -0.0071181850507855415,
            -0.006609255447983742,
            -0.028445415198802948,
            0.006571683567017317,
            0.009010447189211845,
            0.015206411480903625,
            -0.006018350366503,
            0.004180738236755133,
            -0.01097102276980877,
            0.00577242486178875,
            0.0032414384186267853,
            0.016258427873253822,
            0.04273643717169762,
            -0.001592540298588574,
            -0.014004107564687729,
            0.027311425656080246,
            -0.001619865302927792,
            -0.00018519151490181684,
            -0.044758494943380356,
            -0.010943697765469551,
            -0.01646336540579796,
            0.03147850185632706,
            -0.005205429159104824,
            0.03186105191707611,
            0.024401303380727768,
            -0.001023836899548769,
            -0.013819662854075432,
            -0.007972094230353832,
            -0.0005725459777750075,
            0.010205919854342937,
            0.00799941923469305,
            0.00587830925360322,
            -0.008259007707238197,
            0.008183863945305347,
            0.014837522059679031,
            -0.010984685271978378,
            -0.000806943979114294,
            -0.019578425213694572,
            0.006585346069186926,
            -0.016408715397119522,
            -0.021381881088018417,
            -0.03128722682595253,
            -0.013484930619597435,
            0.011059829033911228,
            0.007773987483233213,
            0.012542215175926685,
            0.0005225922795943916,
            -0.024838505312800407,
            -0.00268468982540071,
            0.043337587267160416,
            0.004126088228076696,
            -0.01181126944720745,
            -0.0068859220482409,
            0.01528838649392128,
            0.00994633138179779,
            -0.010615796782076359,
            -0.006687815301120281,
            0.008989953435957432,
            0.006428226828575134,
            -0.018362458795309067,
            0.0042251418344676495,
            -0.0011211824603378773,
            0.03861035034060478,
            -0.006137897726148367,
            0.01658632792532444,
            -0.006120819598436356,
            0.0015626534586772323,
            0.009119748137891293,
            0.010848059318959713,
            0.02349957451224327,
            -0.017501719295978546,
            -0.05607108026742935,
            0.004952671937644482,
            0.009119748137891293,
            -0.013450774364173412,
            0.011449211277067661,
            0.017242129892110825,
            -0.02093101665377617,
            -0.006954234559088945,
            -0.012077688239514828,
            0.03505808860063553,
            0.023704513907432556,
            0.011633655987679958,
            0.0232399869710207,
            0.0023499575909227133,
            0.005625552497804165,
            0.015479662455618382,
            -0.02410072647035122,
            -0.006503370590507984,
            0.002655657008290291,
            0.003606911515817046,
            -0.00566312437877059,
            -0.0068859220482409,
            -0.029620394110679626,
            -0.01053382083773613,
            -0.0029306155629456043,
            0.012446577660739422,
            -0.02683323621749878,
            0.012637852691113949,
            -0.0044130017049610615,
            0.026887886226177216,
            0.00035330484388396144,
            -0.005079050548374653,
            0.004785305820405483,
            -0.002153558423742652,
            -0.006141313351690769,
            -0.005796334240585566,
            0.023745501413941383,
            -0.01643604040145874,
            -0.027775950729846954,
            0.012064025737345219,
            -0.016395052894949913,
            0.028773317113518715,
            0.01629941537976265,
            0.04265446215867996,
            -0.001146799768321216,
            0.006032012868672609,
            0.015343036502599716,
            -0.023690851405262947,
            0.0001412152050761506,
            0.0016659764805808663,
            0.0137718440964818,
            -0.04216260835528374,
            -0.02538500539958477,
            -0.005628968123346567,
            -0.004952671937644482,
            0.011920569464564323,
            -0.016477027907967567,
            -0.017911594361066818,
            -0.008621064946055412,
            -0.016340402886271477,
            0.03415635973215103,
            0.0009871188085526228,
            -0.009433986619114876,
            -0.015520649962127209,
            -0.02378648892045021,
            -0.003485656576231122,
            -0.026300396770238876,
            0.0016958632040768862,
            0.006954234559088945,
            0.0024438875261694193,
            0.006513617932796478,
            0.02468821592628956,
            -0.02064410410821438,
            -0.0076510244980454445,
            0.02710648626089096,
            -0.03358253091573715,
            -0.022966735064983368,
            0.009461311623454094,
            -0.03284475579857826,
            0.012446577660739422,
            0.030084921047091484,
            -0.010704603046178818,
            0.018130196258425713,
            -0.0034515000879764557,
            0.001164731802418828,
            0.008211188949644566,
            -0.011319417506456375,
            0.004358351230621338,
            0.03057677298784256,
            0.014632584527134895,
            -0.004300285596400499,
            0.022447559982538223,
            0.02337661199271679,
            0.03514006361365318,
            0.014058757573366165,
            -0.006414564326405525,
            0.01001464482396841,
            0.00311847566626966,
            0.032407552003860474,
            -0.011640487238764763,
            0.000776630244217813,
            -0.006387239322066307,
            0.022119658067822456,
            -0.0013158736983314157,
            -0.009953162632882595,
            0.009755056351423264,
            0.023841138929128647,
            0.014113407582044601,
            0.024278340861201286,
            0.018225833773612976,
            -0.010902710258960724,
            0.0012535384157672524,
            -0.01814385876059532,
            -0.002088661305606365,
            -0.014782872051000595,
            -0.00263003958389163,
            0.016545340418815613,
            0.009645755402743816,
            0.003593249013647437,
            -0.013034067116677761,
            -0.02250220999121666,
            -0.0016779311699792743,
            0.02855471707880497,
            0.005219091661274433,
            0.011790774762630463,
            -0.014605259522795677,
            -3.9333182940026745e-05,
            0.0129042724147439,
            0.011797606945037842,
            0.018840648233890533,
            -0.012337276712059975,
            -0.0029630642384290695,
            0.019578425213694572,
            -0.025999819859862328,
            0.014332008548080921,
            -0.0006276231142692268,
            -0.0360691137611866,
            0.009843862615525723,
            0.015028798021376133,
            0.024551590904593468,
            0.015588962472975254,
            0.007760324981063604,
            -0.010007813572883606,
            0.026751261204481125,
            -0.016108138486742973,
            0.024278340861201286,
            -0.0026095458306372166,
            -0.003579586511477828,
            0.008696208707988262,
            0.0004265275492798537,
            -0.010547483339905739,
            0.03287208080291748,
            -0.0005921858828514814,
            0.0325988307595253,
            -0.011729293502867222,
            -0.02636870928108692,
            0.005953453481197357,
            -0.037571996450424194,
            0.0070157162845134735,
            -0.02453792840242386,
            0.004792137071490288,
            0.002737632254138589,
            -0.017091842368245125,
            0.010110282339155674,
            0.0030723644886165857,
            0.016982542350888252,
            0.009563780389726162,
            -0.013512255623936653,
            0.009673081338405609,
            0.004829709418118,
            0.0012287750141695142,
            0.0158485509455204,
            -0.007951600477099419,
            -0.008259007707238197,
            0.0018222418148070574,
            -0.012924766167998314,
            -0.015411349013447762,
            -0.022092333063483238,
            -0.023144349455833435,
            0.00914707314223051,
            -0.03057677298784256,
            0.03473018482327461,
            -0.002833270002156496,
            0.0007988318684510887,
            0.006264276336878538,
            0.012692502699792385,
            0.046234048902988434,
            -0.022379247471690178,
            -0.037408046424388885,
            0.013812831602990627,
            0.0038972406182438135,
            -0.006595592945814133,
            -0.016408715397119522,
            -0.050688035786151886,
            0.01008978858590126,
            -0.0005913319764658809,
            -0.013621555641293526,
            -0.004966334439814091,
            0.0068790907971560955,
            -0.001078487024642527,
            0.006527280434966087,
            -0.013621555641293526,
            0.022310933098196983,
            0.025425994768738747,
            -0.005365964025259018,
            -0.007965262979269028,
            -0.020097602158784866,
            0.007582711987197399,
            0.009181229397654533,
            -0.017802294343709946,
            -0.025207392871379852,
            0.031642451882362366,
            -0.012849622406065464,
            -0.04139750823378563,
            -0.00045854912605136633,
            -0.003917734604328871,
            -0.01182493194937706,
            -0.009256373159587383,
            0.006824440788477659,
            0.013013572432100773,
            0.004498392343521118,
            0.016955217346549034,
            0.0161491259932518,
            0.014386658556759357,
            0.002249196171760559,
            0.012275795452296734,
            0.006069585215300322,
            0.013225342147052288,
            -0.043911416083574295,
            0.0020425503607839346,
            0.015028798021376133,
            0.07498004287481308,
            0.006001272238790989,
            -0.02192838303744793,
            0.003606911515817046,
            -0.021381881088018417,
            -0.009290529415011406,
            -0.02020690217614174,
            -0.00872353371232748,
            0.0076510244980454445,
            0.022597847506403923,
            -0.013669375330209732,
            -0.00907192938029766,
            0.015903200954198837,
            -0.004320779349654913,
            0.03451158478856087,
            0.025303030386567116,
            0.01161999348551035,
            -0.03161512687802315,
            -0.023704513907432556,
            -0.014974148012697697,
            -4.317577258916572e-05,
            0.005721190012991428,
            0.02295307256281376,
            0.004822877701371908,
            0.015547974966466427,
            -0.017337767407298088,
            0.027147473767399788,
            0.0159988384693861,
            -0.03197035193443298,
            -0.01657266542315483,
            0.0087918471544981,
            0.012132339179515839,
            0.004013372119516134,
            0.012747153639793396,
            -0.006629749201238155,
            0.021299906075000763,
            0.010581640526652336,
            0.0010195673676207662,
            0.012883778661489487,
            -0.0031748334877192974,
            0.031669776886701584,
            0.024141713976860046,
            0.0024917065165936947,
            -0.003480532905086875,
            0.008798678405582905,
            -0.02451060339808464,
            -0.0019452046835795045,
            0.040304504334926605,
            0.016039825975894928,
            -0.03404705971479416,
            0.015370361506938934,
            0.005150779150426388,
            -0.030959324911236763,
            -0.010854890570044518,
            0.015588962472975254,
            0.025207392871379852,
            0.008634727448225021,
            -0.005082466173917055,
            -0.0008543359581381083,
            -0.026013482362031937,
            -0.018526408821344376,
            -0.03270813077688217,
            -0.013409786857664585,
            -0.02005661465227604,
            -0.01905924826860428,
            -0.0008052361663430929,
            -0.025316692888736725,
            0.008259007707238197,
            -0.03604178875684738,
            0.00815653894096613,
            0.0029203686863183975,
            -0.025781219825148582,
            -0.044375941157341,
            -0.003400265472009778,
            0.006110572721809149,
            -0.010199088603258133,
            0.028500067070126534,
            -0.006315510720014572,
            -0.003753783879801631,
            0.01269933395087719,
            -0.00019896079902537167,
            -0.0115448497235775,
            -0.016367727890610695,
            -0.009153904393315315,
            0.0038733312394469976,
            -0.0018529824446886778,
            -0.024920480325818062,
            -0.020821716636419296,
            0.0037571995053440332,
            0.01226896420121193,
            0.01254904642701149,
            -0.0077534937299788,
            0.00034455227432772517,
            0.009864356368780136,
            -0.007521230261772871,
            0.011654149740934372,
            0.025917844846844673,
            0.020589454099535942,
            -0.004597445949912071,
            -0.01890896074473858,
            -0.007295798510313034,
            -0.027516363188624382,
            -0.022761797532439232,
            -0.031642451882362366,
            0.005792918615043163,
            0.012562708929181099,
            0.028008215129375458,
            0.02306237444281578,
            0.002141603734344244,
            -0.010076126083731651,
            0.012091350741684437,
            -0.004249051213264465,
            0.022133320569992065,
            -0.006650242954492569,
            0.020944679155945778,
            0.010841228067874908,
            0.019291510805487633,
            -0.006274523213505745,
            0.01703719235956669,
            -0.02609545923769474,
            -0.01804821938276291,
            -0.047791577875614166,
            0.03464820981025696,
            0.006356498692184687,
            -0.0139221316203475,
            0.017692994326353073,
            0.014427646063268185,
            0.013211679644882679,
            -0.03328195586800575,
            0.012747153639793396,
            0.009987319819629192,
            0.024715540930628777,
            0.007582711987197399,
            -0.0014243201585486531,
            -0.011503862217068672,
            -0.02016591466963291,
            -0.01886797323822975,
            0.012938428670167923,
            -0.01527472399175167,
            -0.013799169100821018,
            0.011592668481171131,
            0.023035049438476562,
            2.3068885639077052e-05,
            -0.02352689951658249,
            0.016668302938342094,
            -0.025043442845344543,
            -0.008286332711577415,
            0.01886797323822975,
            0.010567978024482727,
            0.04686252400279045,
            -0.02468821592628956,
            0.026286734268069267,
            -0.03989462926983833,
            -0.026163771748542786,
            -7.952027772262227e-06,
            -0.028172165155410767,
            0.0026112536434084177,
            -0.0014098037499934435,
            0.030604097992181778,
            0.0159988384693861,
            0.04257248714566231,
            -0.014550609514117241,
            0.013355136848986149,
            0.00901727844029665,
            -0.005635799374431372,
            -0.008860159665346146,
            0.007172835525125265,
            -0.002059628488495946,
            -0.006042259745299816,
            -0.0038869937416166067,
            -0.0027120148297399282,
            -0.009413492865860462,
            0.025125417858362198,
            -0.004730655811727047,
            -0.00631209509447217,
            0.0005102106370031834,
            -0.015261061489582062,
            -0.012303120456635952,
            -0.02220163308084011,
            -0.02755735069513321,
            -0.02770763821899891,
            0.0022526117973029613,
            0.001700132736004889,
            0.015247398987412453,
            -0.019960977137088776,
            0.00017280982865486294,
            0.01601250097155571,
            0.012876947410404682,
            0.003238022793084383,
            -0.0032465618569403887,
            0.014263696037232876,
            0.006049090996384621,
            -0.00045129089266993105,
            0.023035049438476562,
            0.005946622230112553,
            -0.04199865832924843,
            -0.00526007916778326,
            -0.04279108718037605,
            0.0049629188142716885,
            0.02783060073852539,
            0.007466580253094435,
            0.017843281850218773,
            0.016777602955698967,
            -0.010171763598918915,
            0.00979604385793209,
            0.009058266878128052,
            0.003535183146595955,
            -0.008040406741201878,
            -0.011162297800183296,
            -0.03344590589404106,
            -0.0023123854771256447,
            -0.028172165155410767,
            -0.012583202682435513,
            -0.019127560779452324,
            -0.015124435536563396,
            -0.0017342891078442335,
            0.004621355328708887,
            0.030904673039913177,
            -0.029948296025395393,
            -0.02871866710484028,
            0.029237844049930573,
            0.018785998225212097,
            0.027161136269569397,
            -0.012863284908235073,
            0.024729203432798386,
            0.024578915908932686,
            0.013792337849736214,
            -0.01830780878663063,
            0.011845425702631474,
            0.027448050677776337,
            -0.007200160529464483,
            0.009024109691381454,
            0.003511273767799139,
            -0.012521721422672272,
            -0.00786279421299696,
            0.025029780343174934,
            0.01815752126276493,
            -0.0319976769387722,
            -0.04101495444774628,
            0.016053488478064537,
            0.008887484669685364,
            0.007125016767531633,
            -0.01631307788193226,
            -0.03270813077688217,
            -0.021231593564152718,
            0.012596865184605122,
            -0.007610036991536617,
            0.03218895196914673,
            -0.008696208707988262,
            -0.023581549525260925,
            0.01506978552788496,
            0.0005311314016580582,
            0.015343036502599716,
            -0.016504352912306786,
            -0.006489708088338375,
            0.014441308565437794,
            0.022420234978199005,
            -0.010984685271978378,
            -0.007172835525125265,
            0.007985756732523441,
            0.0003590687410905957,
            0.023253649473190308,
            0.013546411879360676,
            0.006076416466385126,
            -0.007384604774415493,
            0.019414475187659264,
            0.016668302938342094,
            -0.00907192938029766,
            0.0002085672749672085,
            0.030330847948789597,
            -0.02970236912369728,
            -0.01355324313044548,
            -0.008163370192050934,
            -0.015534312464296818,
            0.005953453481197357,
            -0.011292092502117157,
            -0.005482095759361982,
            -0.01616278849542141,
            0.01211867667734623,
            0.01240558922290802,
            0.0239231139421463,
            0.01601250097155571,
            -0.0061071570962667465,
            0.002361912280321121,
            -0.013881144113838673,
            -0.04790087789297104,
            0.001551552675664425,
            -0.001618157490156591,
            0.006438473705202341,
            -0.01933250017464161,
            0.011524355970323086,
            0.02594516985118389,
            -0.023704513907432556,
            0.005311314016580582,
            -0.0016770772635936737,
            -0.024742865934967995,
            -0.005164441652595997,
            0.013116042129695415,
            -0.02262517251074314,
            0.004631602205336094,
            0.019824350252747536,
            0.00814287643879652,
            -0.004973165690898895,
            0.010246907360851765,
            -0.00988485012203455,
            0.0012202359503135085,
            0.009037773124873638,
            -0.00577242486178875,
            -0.02449694089591503,
            -0.006493123713880777,
            -0.013928962871432304,
            0.009270035661756992,
            -0.0058714780025184155,
            -0.0016950092976912856,
            0.0070157162845134735,
            -0.010943697765469551,
            0.004594030324369669,
            -0.009850693866610527,
            -0.00801308173686266,
            0.022980399429798126,
            -0.009386167861521244,
            -0.0002309823757968843,
            0.013642050325870514,
            -0.0221879705786705,
            0.010000982321798801,
            -0.016395052894949913,
            0.001566922990605235,
            -0.019879000261425972,
            -0.03809117153286934,
            -0.012706165201961994,
            0.018813323229551315,
            0.002317508915439248,
            -0.005055141169577837,
            0.0019076326861977577,
            -0.015315711498260498,
            0.0025822208262979984,
            -0.009987319819629192,
            0.01859472133219242,
            0.0011211824603378773,
            0.014755547046661377,
            0.0015558222075924277,
            0.030658748000860214,
            0.019428137689828873,
            0.006547774188220501,
            -0.017665669322013855,
            -0.011742956005036831,
            -0.014304683543741703,
            -0.015206411480903625,
            -0.03273545578122139,
            -0.0071181850507855415,
            -0.03317265585064888,
            0.04918515682220459,
            -0.00222016335465014,
            -0.019660400226712227,
            -0.0016155957709997892,
            -0.01414073258638382,
            -0.030221546068787575,
            -0.011421886272728443,
            -0.008771353401243687,
            0.022857435047626495,
            0.012036700733006,
            0.02781693823635578,
            0.01045867707580328,
            0.02017957717180252,
            -0.008826003409922123,
            0.008163370192050934,
            -0.010731928050518036,
            -0.01132624875754118,
            0.006271107587963343,
            -0.00504489429295063,
            -0.006014934740960598,
            0.004358351230621338,
            -0.028664017096161842,
            -0.002032303484156728,
            0.0155069874599576,
            -0.0175836943089962,
            -0.006349666975438595,
            0.016217438504099846,
            -0.013088717125356197,
            -0.008259007707238197,
            0.017214804887771606,
            -0.01058847177773714,
            -0.0115448497235775,
            0.006233535706996918,
            0.007029378786683083,
            -0.0115448497235775,
            0.004423248581588268,
            0.002957940800115466,
            -0.025057105347514153,
            0.0007245417800731957,
            -0.013095548376441002,
            0.008607402443885803,
            0.005601643119007349,
            -9.211542783305049e-05,
            0.013218510895967484,
            -0.005243001040071249,
            -0.0011946186423301697,
            -0.01253538392484188,
            0.05079733580350876,
            0.002647117944434285,
            0.010246907360851765,
            0.020220564678311348,
            -0.004703330807387829,
            0.004696499556303024,
            -0.0071045225486159325,
            0.009966826066374779,
            0.007282136008143425,
            0.002141603734344244,
            0.028062865138053894,
            -0.025589944794774055,
            -0.01746073178946972,
            -0.012371432967483997,
            -0.0018615216249600053,
            -0.024401303380727768,
            -0.021860070526599884,
            -0.015834888443350792,
            -0.001762468134984374,
            -0.03153315186500549,
            0.01804821938276291,
            0.010861721821129322,
            -0.01491949800401926,
            -0.0008299995097331703,
            -0.007760324981063604,
            0.006435058079659939,
            -0.010363039560616016,
            -0.02959306910634041,
            0.012637852691113949,
            -0.018389783799648285,
            0.0009461311274208128,
            0.0019161717500537634,
            -0.007418761029839516,
            -0.011428717523813248,
            0.012487565167248249,
            0.009693575091660023,
            -0.022010358050465584,
            0.00306382542476058,
            0.2013312429189682,
            -0.026027144864201546,
            -0.005676786880940199,
            0.012385095469653606,
            -0.00025275704683735967,
            -0.020370852202177048,
            0.04128820821642876,
            0.0023516654036939144,
            -0.004604277200996876,
            0.013737687841057777,
            -0.007685180753469467,
            0.004529133439064026,
            -0.024469615891575813,
            -0.0006989245302975178,
            0.0065170335583388805,
            -0.024114388972520828,
            -0.04820145294070244,
            -0.03371915966272354,
            -0.01434567105025053,
            0.008914809674024582,
            -0.012733491137623787,
            0.012931597419083118,
            -0.016244765371084213,
            -0.026792248710989952,
            0.020261552184820175,
            -0.002981850178912282,
            0.004013372119516134,
            0.01978336274623871,
            0.0012586618540808558,
            0.011838594451546669,
            -0.0016173035837709904,
            0.005628968123346567,
            -0.0016036410816013813,
            0.0033439076505601406,
            0.0052498322911560535,
            -0.01818484626710415,
            0.00663658045232296,
            -0.0036888867616653442,
            0.010438183322548866,
            0.022816447541117668,
            0.01848542131483555,
            0.0011544849257916212,
            0.028090190142393112,
            -0.03158780187368393,
            0.011920569464564323,
            0.044184666126966476,
            -0.006349666975438595,
            -0.0016249887412413955,
            -0.008552752435207367,
            -0.0019503281218931079,
            -0.022297270596027374,
            0.01147653628140688,
            0.017351429909467697,
            0.03145117685198784,
            0.0017829620046541095,
            0.006920078303664923,
            -0.02612278424203396,
            0.01743340492248535,
            0.0008654367411509156,
            -0.005232754163444042,
            -0.03404705971479416,
            -0.024578915908932686,
            0.001741120358929038,
            0.010718265548348427,
            -0.012350939214229584,
            -0.006762959063053131,
            -0.009352011606097221,
            0.0062301200814545155,
            -0.0026932288892567158,
            -0.014577934518456459,
            -0.006834687665104866,
            -0.004682837054133415,
            -0.023117024451494217,
            0.012781309895217419,
            -0.026450684294104576,
            -0.05205428972840309,
            0.04033182933926582,
            0.033254630863666534,
            -0.008190695196390152,
            0.03131455183029175,
            -0.004798968322575092,
            -0.023253649473190308,
            0.0016975710168480873,
            -0.026764923706650734,
            -0.006383823696523905,
            -0.02885529212653637,
            0.018266821280121803,
            -0.020999329164624214,
            -0.020411839708685875,
            -0.007022547535598278,
            -0.007869625464081764,
            -0.018239496275782585,
            0.0033661092165857553,
            0.01132624875754118,
            0.002440471900627017,
            0.009597936645150185,
            0.020001964643597603,
            -0.005007322411984205,
            0.004337857477366924,
            -0.016900567337870598,
            -0.021422868594527245,
            0.05268276855349541,
            0.02841809019446373,
            -0.00823168270289898,
            0.01196155697107315,
            0.016559002920985222,
            -0.017679331824183464,
            -0.002886212430894375,
            0.0035693396348506212,
            -0.016039825975894928,
            -0.01543867401778698,
            -0.034675534814596176,
            0.01759735681116581,
            0.007329954765737057,
            -0.006862012669444084,
            0.01030838955193758,
            -0.0034070967230945826,
            -0.0061959633603692055,
            0.027898915112018585,
            0.0005285696825012565,
            -8.635154517833143e-05,
            -0.015903200954198837,
            0.0047750589437782764,
            0.0025736817624419928,
            0.005485511384904385,
            -0.02410072647035122,
            -0.014168057590723038,
            0.0013303902233019471,
            -0.036998167634010315,
            0.000961501500569284,
            0.00674588093534112,
            -0.036998167634010315,
            0.00573826814070344,
            0.012794972397387028,
            -0.004734071437269449,
            -0.004949256312102079,
            0.010547483339905739,
            -0.0025122002698481083,
            -0.009215385653078556,
            -0.006547774188220501,
            -0.0036888867616653442,
            0.011285261251032352,
            -0.008853328414261341,
            -0.005365964025259018,
            0.01800723187625408,
            -0.0040543596260249615,
            0.014113407582044601,
            0.01991998963057995,
            -0.01284279115498066,
            -0.00771933700889349,
            -0.019510112702846527,
            -0.0015686308033764362,
            0.004559874068945646,
            -0.0018137026345357299,
            0.019113898277282715,
            0.02032986469566822,
            -0.0248248428106308,
            0.013109210878610611,
            0.006226704455912113,
            0.024128051474690437,
            -0.04298236221075058,
            0.017050854861736298,
            0.01743340492248535,
            -0.012296289205551147,
            -0.011230611242353916,
            -0.01776130683720112,
            -0.17575496435165405,
            -0.005991025362163782,
            0.003934812732040882,
            -0.05298334360122681,
            0.02783060073852539,
            0.0221879705786705,
            0.019236860796809196,
            -0.013573736883699894,
            -0.032407552003860474,
            -0.0009435694082640111,
            0.008375138975679874,
            -0.008976290933787823,
            -0.017652006819844246,
            -0.009194891899824142,
            -0.015629950910806656,
            0.013034067116677761,
            -0.005348885897547007,
            0.01989266276359558,
            0.02091735415160656,
            0.0015250814612954855,
            0.037544671446084976,
            -0.026942536234855652,
            -0.01670929044485092,
            -0.017201142385601997,
            0.011777112260460854,
            0.007145510520786047,
            -0.004149997606873512,
            0.014413983561098576,
            -0.012282626703381538,
            -0.022160645574331284,
            -0.010294727049767971,
            -0.014427646063268185,
            0.03544063866138458,
            -0.007493905257433653,
            0.024742865934967995,
            0.015261061489582062,
            0.032653480768203735,
            0.0010340837761759758,
            -0.03401973471045494,
            0.002466089092195034,
            0.015793900936841965,
            0.036123763769865036,
            0.02105397917330265,
            0.002672735136002302,
            -0.011203286238014698,
            0.014195382595062256,
            0.016326740384101868,
            -0.03462088480591774,
            0.016258427873253822,
            -0.01573925092816353,
            0.027338750660419464,
            -0.02508443035185337,
            0.0233902744948864,
            -0.00411584135144949,
            0.010861721821129322,
            0.025125417858362198,
            0.0027444635052233934,
            0.004932178184390068,
            -0.010697771795094013,
            -0.019113898277282715,
            -0.012378264218568802,
            -0.011517524719238281,
            0.0038528372533619404,
            -0.009386167861521244,
            -0.015534312464296818,
            -0.028199490159749985,
            -0.013375630602240562,
            0.010950529016554356,
            -0.02681957371532917,
            0.007657855749130249,
            -0.020958341658115387,
            -0.0034634547773748636,
            -0.011162297800183296,
            -0.015889538452029228,
            0.031068624928593636,
            0.023117024451494217,
            0.0005482095875777304,
            0.015124435536563396,
            0.027297763153910637,
            1.6437747035524808e-05,
            -0.008545921184122562,
            0.01916854828596115,
            -0.0032619324047118425,
            0.024852167814970016,
            -0.016627315431833267,
            -0.008101888000965118,
            0.008443452417850494,
            0.014577934518456459,
            -0.028773317113518715,
            -0.010656784288585186,
            0.02522105537354946,
            -0.02090369164943695,
            -0.008040406741201878,
            -0.01508344803005457,
            -0.015302048996090889,
            0.004727240186184645,
            0.00987118761986494,
            0.00013353001850191504,
            -0.0030450394842773676,
            -0.0009785797446966171,
            0.013573736883699894,
            0.00886699091643095,
            -0.0013431988190859556,
            -0.011879581958055496,
            0.037544671446084976,
            0.0018137026345357299,
            -0.030604097992181778,
            0.011134972795844078,
            0.03874697536230087,
            -0.019564762711524963,
            -0.02855471707880497,
            -0.008306826464831829,
            -0.008033575490117073,
            -0.0007518668426200747,
            -0.008771353401243687,
            0.015684600919485092,
            -0.0137718440964818,
            -0.019414475187659264,
            0.016982542350888252,
            0.001952035934664309,
            0.03718944266438484,
            0.00609691021963954,
            -0.001953743863850832,
            0.005277157295495272,
            0.00015156884910538793,
            -0.011196454986929893,
            -0.11706067621707916,
            -0.021805420517921448,
            0.0030843191780149937,
            0.015179086476564407,
            -0.012214314192533493,
            0.0316971018910408,
            -0.00508929742500186,
            0.04546894505620003,
            -0.029046567156910896,
            0.04560557007789612,
            -0.01434567105025053,
            -0.008853328414261341,
            0.007145510520786047,
            -0.0026915210764855146,
            0.014332008548080921,
            -0.00014996776008047163,
            -0.0023875294718891382,
            -0.01815752126276493,
            -0.03587783873081207,
            0.023540562018752098,
            0.019441800191998482,
            -0.0012697626370936632,
            0.007657855749130249,
            -0.03044014796614647,
            -0.0058475686237216,
            -0.01299307867884636,
            -0.03937545046210289,
            0.02798089012503624,
            0.023567887023091316,
            0.0017385586397722363,
            0.009030940942466259,
            -0.028664017096161842,
            0.005666540004312992,
            -0.011339911259710789,
            0.010076126083731651,
            -0.01024007610976696,
            0.010889047756791115,
            -0.014195382595062256,
            0.01802089437842369,
            -0.012986247427761555,
            -0.002857179380953312,
            0.0079242754727602,
            0.013676206581294537,
            -0.010226413607597351,
            -0.007125016767531633,
            -0.02319899946451187,
            -0.008532258681952953,
            0.011674643494188786,
            0.007370942272245884,
            -0.0155069874599576,
            -0.03951207548379898,
            -0.02064410410821438,
            -0.04453989118337631,
            0.013525918126106262,
            0.01875867322087288,
            0.0036205740179866552,
            0.018130196258425713,
            0.016121800988912582,
            -0.013560074381530285,
            -0.00030975547269918025,
            -0.011380898766219616,
            -0.004508639220148325,
            -0.027174798771739006,
            -0.00344296102412045,
            0.01833513379096985,
            -0.0027308010030537844,
            6.217524787643924e-05,
            -0.02494780533015728,
            0.009816537611186504,
            -0.00959110539406538,
            0.006933740805834532,
            0.022283608093857765,
            -0.005854399874806404,
            0.007610036991536617,
            -0.01916854828596115,
            0.0009085591300390661,
            -0.013355136848986149,
            -0.03500343859195709,
            -0.0031270147301256657,
            0.0089421346783638,
            -0.010513327084481716,
            -0.01325949840247631,
            0.007097691297531128,
            -0.0041534132324159145,
            6.009384378558025e-05,
            -0.0029237843118608,
            -0.02020690217614174,
            0.0007377773872576654,
            0.0009128287201747298,
            -0.014413983561098576,
            0.012631021440029144,
            0.028636692091822624,
            0.004518886562436819,
            -0.01414073258638382,
            0.0012347523588687181,
            0.010725096799433231,
            -0.019414475187659264,
            -0.005748515482991934,
            0.028472740203142166,
            0.003120183479040861,
            -0.02724311128258705,
            0.009468142874538898,
            -0.055879805237054825,
            0.042927712202072144,
            0.0014055342180654407,
            -0.010759253054857254,
            -0.0011988881742581725,
            -0.02031620219349861,
            0.01613546349108219,
            -0.020083939656615257,
            0.005410367157310247,
            0.008340982720255852,
            -0.01053382083773613,
            0.016340402886271477,
            -0.01434567105025053,
            -0.0005213114200159907,
            -0.021586818620562553,
            -0.005140532273799181,
            0.018116533756256104,
            0.009679912589490414,
            0.007603205740451813,
            -0.0014644538750872016,
            0.007603205740451813,
            -0.0177339818328619,
            0.022966735064983368,
            0.017925256863236427,
            0.010889047756791115,
            -0.01508344803005457,
            -0.02583586983382702,
            0.015247398987412453,
            -0.009167566895484924,
            -0.010772915557026863,
            -0.01586221344769001,
            -0.01161999348551035,
            -0.006800530944019556,
            0.0078013124875724316,
            -0.019414475187659264,
            -0.010608965530991554,
            0.024633565917611122,
            0.032653480768203735,
            0.026505334302783012,
            0.04626137390732765,
            -0.021231593564152718,
            -0.04254516214132309,
            0.010062463581562042,
            -0.02046648971736431,
            -0.0089421346783638,
            -0.023103361949324608,
            -0.007876456715166569,
            -0.000488435965962708,
            0.02090369164943695,
            0.003811849746853113,
            0.037134792655706406,
            0.015479662455618382,
            -0.029155869036912918,
            -0.03016689606010914,
            -0.008238513953983784,
            0.013778675347566605,
            0.005266910418868065,
            -0.00022436458675656468,
            -0.0013218511594459414,
            0.025207392871379852,
            0.029620394110679626,
            0.005741683766245842,
            0.007077197544276714,
            -0.004891190677881241,
            -0.0018478590063750744,
            -0.006817609537392855,
            -0.007125016767531633,
            0.004573536571115255,
            0.0034258828964084387,
            0.008545921184122562,
            -0.006499954964965582,
            0.011859088204801083,
            0.00382209662348032,
            -0.001369670033454895,
            0.012385095469653606,
            -0.021231593564152718,
            0.016340402886271477,
            0.016477027907967567,
            -0.01804821938276291,
            0.006503370590507984,
            0.008026744239032269,
            -0.016968879848718643,
            0.01081390306353569,
            0.002855471568182111,
            0.023882126435637474,
            0.006042259745299816,
            -0.015957850962877274,
            0.0022594432812184095,
            -0.008921640925109386,
            -0.005417198408395052,
            0.0008667176007293165,
            -0.0036000802647322416,
            -0.011893244460225105,
            0.01670929044485092,
            0.021996695548295975,
            -0.009693575091660023,
            0.00428662309423089,
            0.011988881975412369,
            0.017091842368245125,
            -0.0033251214772462845,
            0.010199088603258133,
            -0.01978336274623871,
            0.007575880270451307,
            -0.010151269845664501,
            -0.033254630863666534,
            0.005994440987706184,
            -0.03153315186500549,
            -0.010410858318209648,
            0.03000294603407383,
            0.014837522059679031,
            0.0156982634216547,
            -0.0036957180127501488,
            0.006708309054374695,
            0.00843662116676569,
            -0.01399044506251812,
            0.020111264660954475,
            -0.010281064547598362,
            -0.0038050184957683086,
            -0.032407552003860474,
            0.039867304265499115,
            0.044922444969415665,
            0.017474394291639328,
            0.040741704404354095,
            -0.016791265457868576,
            0.005994440987706184,
            0.0014337131287902594,
            0.024319328367710114,
            -0.028008215129375458,
            0.014058757573366165,
            -0.01089587900787592,
            -0.013314148411154747,
            -0.008224851451814175,
            -0.012972584925591946,
            0.005082466173917055,
            -0.006113988347351551,
            1.228828386956593e-05,
            -0.012289457954466343,
            0.02205134555697441,
            -0.0017044023843482137,
            0.05563387647271156,
            0.016490690410137177,
            0.0022696901578456163,
            0.02665562368929386,
            -0.014618922024965286,
            0.015302048996090889,
            0.00849810242652893,
            0.025740232318639755,
            -0.00522933853790164,
            -0.017488056793808937,
            -0.0015566761139780283,
            -0.004143166355788708,
            -0.005755346734076738,
            -0.028336115181446075,
            -0.013006741181015968,
            -0.00310310535132885,
            -0.007562217768281698,
            0.013621555641293526,
            -0.0073777735233306885,
            -0.007917444221675396,
            0.02378648892045021,
            0.005051725544035435,
            0.023089699447155,
            0.007507567759603262,
            -0.0051473635248839855,
            0.0007343617035076022,
            0.028308790177106857,
            0.001484947744756937,
            0.0008744028164073825,
            -0.006810777820646763,
            0.03429298475384712,
            0.01873134821653366,
            -0.0335005559027195,
            -0.017952581867575645,
            -0.001010174280963838,
            -0.0016497521428391337,
            0.014045095071196556,
            -0.04008590430021286,
            -0.005161026027053595,
            -0.009638924151659012,
            -0.006711724679917097,
            0.008894315920770168,
            -0.0020562128629535437,
            -0.02263883501291275,
            0.0010853182757273316,
            0.00042780840885825455,
            -0.016326740384101868,
            -0.004112425725907087,
            -0.015821225941181183
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "00a24722-a812-4e1d-8741-d9c969bf5e6a",
      "type": "child",
      "source": {
        "id": "1d35c58e-1af7-4136-bfeb-f0894fc21db0",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere’s my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey’re actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don’t yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n\nThe ethics of this space remain diabolically complex\n\nMy blog in 2023\n\nHere’s the sequel to this post: Things we learned about LLMs in 2024.\n\nLarge Language Models\n\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\n\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\n\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\n\nSo far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\n\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\n\nThey’re actually quite easy to build\n\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\n\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\n\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\n\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\n\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\n\nThe training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing.\n\nSo training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\n\nYou can run LLMs on your own devices\n\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\n\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\n\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\n\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use.\n\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\n\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\n\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\n\nHobbyists can build their own fine-tuned models\n\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\n\nThere’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\n\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours.\n\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\n\nThis is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\n\nWe don’t yet know how to build GPT-4\n\nFrustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4.\n\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\n\nThis may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out.\n\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then.\n\nStill, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet.\n\nVibes Based Development\n\nAs a computer scientist and software engineer, LLMs are infuriating.\n\nEven the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\n\nI’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\n\nThe worst part is the challenge of evaluating them.\n\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task.\n\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself!\n\nThe most frustrating thing for me is at the level of individual prompting.\n\nSometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out.\n\nWe’re left with what’s effectively Vibes Based Development. It’s vibes all the way down.\n\nI’d love to see us move beyond vibes in 2024!\n\nLLMs are really smart, and also really, really dumb\n\nOn the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun!\n\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\n\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\n\nThe honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\n\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead.\n\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works!\n\nGullibility is the biggest unsolved problem\n\nI coined the term prompt injection in September last year.\n\n15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem.\n\nI’ve written a ton about this already.\n\nBeyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility.\n\nLanguage Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt.\n\nIn order to be useful tools for us, we need them to believe what we feed them!\n\nBut it turns out a lot of the things we want to build need them not to be gullible.\n\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\n\nA lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\n\nI think this is because of gullibility.\n\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\n\nCode may be the best application\n\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.\n\nIf you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\n\nIt’s still astonishing to me how effective they are though.\n\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\n\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\n\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\n\nHow should we feel about this as software engineers?\n\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\n\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\n\nThe ethics of this space remain diabolically complex\n\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\n\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\n\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\n\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\n\nLaw is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\n\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\n\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\n\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\n\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\n\nMy blog in 2023\n\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\n\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\n\nI’ve written a lot about this stuff!\n\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\n\nArticle Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k\n\nI also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\n\nPrompt injection explained, with video, slides, and a transcript\n\nCatching up on the weird world of LLMs\n\nMaking Large Language Models work for you\n\nOpen questions for AI engineering\n\nEmbeddings: What they are and why they matter\n\nFinancial sustainability for open source projects at GitHub Universe\n\nAnd in podcasts:\n\nWhat AI can do for you on the Theory of Change\n\nWorking in public on Path to Citus Con\n\nLLMs break the internet on the Changelog\n\nTalking Large Language Models on Rooftop Ruby\n\nThoughts on the OpenAI board situation on Newsroom Robots\n\nIndustry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1089\n\ngenerative-ai\n            934\n\nllms\n            922\n\nNext: Tom Scott, and the formidable power of escalating streaks\n\nPrevious: Last weeknotes of 2023\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2023_llms.html"
          },
          "headlines": [
            "Large Language Models",
            "Vibes Based Development",
            "LLMs are really smart, and also really, really dumb",
            "Gullibility is the biggest unsolved problem",
            "Code may be the best application"
          ],
          "summary": "In 2023, Large Language Models (LLMs) emerged as a significant development in AI, with advancements making them easier to build and run on personal devices. Despite their capabilities, LLMs face challenges like gullibility and ethical concerns over unlicensed training data. They excel in code generation, where errors can be iteratively corrected. The year saw a proliferation of LLMs from various organizations, though none surpassed GPT-4. The complexity of LLMs remains a barrier, with 'Vibes Based Development' highlighting the difficulty in evaluating their performance. The ethical and legal implications of AI continue to be complex and unresolved.",
          "summary_embedding": [
            -0.01449595857411623,
            -0.005809996742755175,
            0.0007078905473463237,
            -0.03407438471913338,
            -0.0034515000879764557,
            0.01598517596721649,
            -0.015042460523545742,
            0.02121793106198311,
            -0.023909451439976692,
            -0.04071437940001488,
            0.025603607296943665,
            0.020726079121232033,
            -0.023991426452994347,
            -0.0034241750836372375,
            0.0019861923065036535,
            0.0180345568805933,
            0.021983033046126366,
            0.0025992989540100098,
            0.017529044300317764,
            -0.007432423997670412,
            -0.0009734562481753528,
            -0.0001246707106474787,
            -0.014441308565437794,
            -0.017406079918146133,
            -0.005376210901886225,
            0.006428226828575134,
            0.022898422554135323,
            -0.03661561757326126,
            -0.014618922024965286,
            -0.0006575098959729075,
            0.02032986469566822,
            -0.0041534132324159145,
            -0.03789989650249481,
            0.007480242755264044,
            -0.029647719115018845,
            -0.00886699091643095,
            0.013819662854075432,
            -0.007883287966251373,
            0.020575791597366333,
            0.005519667640328407,
            0.028800642117857933,
            0.02869134210050106,
            0.015138098038733006,
            -0.02002928964793682,
            -0.029073892161250114,
            0.017815956845879555,
            0.004792137071490288,
            -0.01644970290362835,
            -0.03341858088970184,
            0.012070856988430023,
            0.019318837672472,
            0.037872571498155594,
            -0.011421886272728443,
            -0.014536946080625057,
            -0.00784913171082735,
            0.002877673367038369,
            -0.014113407582044601,
            0.01066361553966999,
            0.017706656828522682,
            0.00035437222686596215,
            -0.006291601341217756,
            -0.01573925092816353,
            -0.002004978246986866,
            0.024879492819309235,
            -0.006978144403547049,
            -0.010670446790754795,
            -0.027584675699472427,
            0.03314533084630966,
            0.033965084701776505,
            -0.013266329653561115,
            -0.0027427556924521923,
            -0.008095056749880314,
            0.019154885783791542,
            -0.011121310293674469,
            0.017925256863236427,
            0.007992587983608246,
            -0.006277938839048147,
            -0.015261061489582062,
            -0.0275436881929636,
            0.004747733939439058,
            -0.0019281264394521713,
            -0.0006651951116509736,
            -0.003910903353244066,
            0.013150198385119438,
            0.013744519092142582,
            0.004597445949912071,
            -0.005208844784647226,
            -0.017692994326353073,
            -0.0015805854927748442,
            -0.013437111862003803,
            -0.005540161393582821,
            -0.005652877502143383,
            0.016531677916646004,
            0.017228467389941216,
            -0.0062813544645905495,
            -0.0023106776643544436,
            -0.008962628431618214,
            0.002288476098328829,
            -0.005642630625516176,
            -0.026764923706650734,
            -0.00512003805488348,
            0.0024695047177374363,
            -0.040905654430389404,
            -0.008313657715916634,
            -0.023608876392245293,
            0.0037196276243776083,
            0.015684600919485092,
            0.021245256066322327,
            0.015124435536563396,
            0.018949948251247406,
            -0.00663658045232296,
            0.027652988210320473,
            0.016518015414476395,
            -0.02136821858584881,
            -0.0024438875261694193,
            -0.015807563439011574,
            0.0029476936906576157,
            -0.0006071293028071523,
            0.005079050548374653,
            0.008095056749880314,
            0.031095949932932854,
            0.010404027067124844,
            0.013778675347566605,
            -0.0019042170606553555,
            0.019701387733221054,
            -0.015151760540902615,
            0.005929544102400541,
            -0.020726079121232033,
            -0.01234410796314478,
            -0.0003955733263865113,
            0.0020237641874700785,
            -0.0027290931902825832,
            0.010294727049767971,
            0.0027188463136553764,
            -0.026901548728346825,
            0.031369201838970184,
            -0.023704513907432556,
            -0.0034737016540020704,
            0.009386167861521244,
            0.010397195816040039,
            0.0009649171261116862,
            0.02467455342411995,
            -0.022761797532439232,
            0.017515381798148155,
            -0.004358351230621338,
            0.018717685714364052,
            0.009413492865860462,
            0.013361968100070953,
            -0.0024353484623134136,
            -0.000552052166312933,
            0.004450573585927486,
            -0.03153315186500549,
            -0.01802089437842369,
            0.02321266196668148,
            0.004679421428591013,
            0.004518886562436819,
            0.00696106581017375,
            0.023267311975359917,
            -0.031369201838970184,
            -0.010349377058446407,
            -0.014741884544491768,
            0.009563780389726162,
            -0.0032328993547707796,
            0.019797025248408318,
            0.01363521907478571,
            0.021245256066322327,
            0.028800642117857933,
            0.0026932288892567158,
            -0.006749296560883522,
            -0.014687234535813332,
            -0.014427646063268185,
            0.030604097992181778,
            -0.03085002303123474,
            0.011968388222157955,
            -0.015138098038733006,
            0.020111264660954475,
            0.023321961984038353,
            -0.0037025492638349533,
            -0.024783853441476822,
            -0.018553733825683594,
            0.01804821938276291,
            0.009666250087320805,
            0.01613546349108219,
            -0.009270035661756992,
            -0.013505424372851849,
            0.0011843717657029629,
            0.010499664582312107,
            0.0041397507302463055,
            0.012214314192533493,
            -0.02293941006064415,
            0.027652988210320473,
            0.018581058830022812,
            -0.005468433257192373,
            -0.028800642117857933,
            -0.6571137309074402,
            -0.0369708426296711,
            0.015821225941181183,
            -0.010690940544009209,
            0.002980142366141081,
            -0.0036342365201562643,
            0.0009486929047852755,
            -0.01284279115498066,
            -0.015343036502599716,
            0.03306335583329201,
            -0.013525918126106262,
            0.006315510720014572,
            -0.003108228789642453,
            -0.019646737724542618,
            0.011216948740184307,
            -0.009782381355762482,
            0.0042900387197732925,
            -0.026478009298443794,
            0.007610036991536617,
            0.018376121297478676,
            -0.005273741669952869,
            0.019072910770773888,
            -0.0071181850507855415,
            -0.006609255447983742,
            -0.028445415198802948,
            0.006571683567017317,
            0.009010447189211845,
            0.015206411480903625,
            -0.006018350366503,
            0.004180738236755133,
            -0.01097102276980877,
            0.00577242486178875,
            0.0032414384186267853,
            0.016258427873253822,
            0.04273643717169762,
            -0.001592540298588574,
            -0.014004107564687729,
            0.027311425656080246,
            -0.001619865302927792,
            -0.00018519151490181684,
            -0.044758494943380356,
            -0.010943697765469551,
            -0.01646336540579796,
            0.03147850185632706,
            -0.005205429159104824,
            0.03186105191707611,
            0.024401303380727768,
            -0.001023836899548769,
            -0.013819662854075432,
            -0.007972094230353832,
            -0.0005725459777750075,
            0.010205919854342937,
            0.00799941923469305,
            0.00587830925360322,
            -0.008259007707238197,
            0.008183863945305347,
            0.014837522059679031,
            -0.010984685271978378,
            -0.000806943979114294,
            -0.019578425213694572,
            0.006585346069186926,
            -0.016408715397119522,
            -0.021381881088018417,
            -0.03128722682595253,
            -0.013484930619597435,
            0.011059829033911228,
            0.007773987483233213,
            0.012542215175926685,
            0.0005225922795943916,
            -0.024838505312800407,
            -0.00268468982540071,
            0.043337587267160416,
            0.004126088228076696,
            -0.01181126944720745,
            -0.0068859220482409,
            0.01528838649392128,
            0.00994633138179779,
            -0.010615796782076359,
            -0.006687815301120281,
            0.008989953435957432,
            0.006428226828575134,
            -0.018362458795309067,
            0.0042251418344676495,
            -0.0011211824603378773,
            0.03861035034060478,
            -0.006137897726148367,
            0.01658632792532444,
            -0.006120819598436356,
            0.0015626534586772323,
            0.009119748137891293,
            0.010848059318959713,
            0.02349957451224327,
            -0.017501719295978546,
            -0.05607108026742935,
            0.004952671937644482,
            0.009119748137891293,
            -0.013450774364173412,
            0.011449211277067661,
            0.017242129892110825,
            -0.02093101665377617,
            -0.006954234559088945,
            -0.012077688239514828,
            0.03505808860063553,
            0.023704513907432556,
            0.011633655987679958,
            0.0232399869710207,
            0.0023499575909227133,
            0.005625552497804165,
            0.015479662455618382,
            -0.02410072647035122,
            -0.006503370590507984,
            0.002655657008290291,
            0.003606911515817046,
            -0.00566312437877059,
            -0.0068859220482409,
            -0.029620394110679626,
            -0.01053382083773613,
            -0.0029306155629456043,
            0.012446577660739422,
            -0.02683323621749878,
            0.012637852691113949,
            -0.0044130017049610615,
            0.026887886226177216,
            0.00035330484388396144,
            -0.005079050548374653,
            0.004785305820405483,
            -0.002153558423742652,
            -0.006141313351690769,
            -0.005796334240585566,
            0.023745501413941383,
            -0.01643604040145874,
            -0.027775950729846954,
            0.012064025737345219,
            -0.016395052894949913,
            0.028773317113518715,
            0.01629941537976265,
            0.04265446215867996,
            -0.001146799768321216,
            0.006032012868672609,
            0.015343036502599716,
            -0.023690851405262947,
            0.0001412152050761506,
            0.0016659764805808663,
            0.0137718440964818,
            -0.04216260835528374,
            -0.02538500539958477,
            -0.005628968123346567,
            -0.004952671937644482,
            0.011920569464564323,
            -0.016477027907967567,
            -0.017911594361066818,
            -0.008621064946055412,
            -0.016340402886271477,
            0.03415635973215103,
            0.0009871188085526228,
            -0.009433986619114876,
            -0.015520649962127209,
            -0.02378648892045021,
            -0.003485656576231122,
            -0.026300396770238876,
            0.0016958632040768862,
            0.006954234559088945,
            0.0024438875261694193,
            0.006513617932796478,
            0.02468821592628956,
            -0.02064410410821438,
            -0.0076510244980454445,
            0.02710648626089096,
            -0.03358253091573715,
            -0.022966735064983368,
            0.009461311623454094,
            -0.03284475579857826,
            0.012446577660739422,
            0.030084921047091484,
            -0.010704603046178818,
            0.018130196258425713,
            -0.0034515000879764557,
            0.001164731802418828,
            0.008211188949644566,
            -0.011319417506456375,
            0.004358351230621338,
            0.03057677298784256,
            0.014632584527134895,
            -0.004300285596400499,
            0.022447559982538223,
            0.02337661199271679,
            0.03514006361365318,
            0.014058757573366165,
            -0.006414564326405525,
            0.01001464482396841,
            0.00311847566626966,
            0.032407552003860474,
            -0.011640487238764763,
            0.000776630244217813,
            -0.006387239322066307,
            0.022119658067822456,
            -0.0013158736983314157,
            -0.009953162632882595,
            0.009755056351423264,
            0.023841138929128647,
            0.014113407582044601,
            0.024278340861201286,
            0.018225833773612976,
            -0.010902710258960724,
            0.0012535384157672524,
            -0.01814385876059532,
            -0.002088661305606365,
            -0.014782872051000595,
            -0.00263003958389163,
            0.016545340418815613,
            0.009645755402743816,
            0.003593249013647437,
            -0.013034067116677761,
            -0.02250220999121666,
            -0.0016779311699792743,
            0.02855471707880497,
            0.005219091661274433,
            0.011790774762630463,
            -0.014605259522795677,
            -3.9333182940026745e-05,
            0.0129042724147439,
            0.011797606945037842,
            0.018840648233890533,
            -0.012337276712059975,
            -0.0029630642384290695,
            0.019578425213694572,
            -0.025999819859862328,
            0.014332008548080921,
            -0.0006276231142692268,
            -0.0360691137611866,
            0.009843862615525723,
            0.015028798021376133,
            0.024551590904593468,
            0.015588962472975254,
            0.007760324981063604,
            -0.010007813572883606,
            0.026751261204481125,
            -0.016108138486742973,
            0.024278340861201286,
            -0.0026095458306372166,
            -0.003579586511477828,
            0.008696208707988262,
            0.0004265275492798537,
            -0.010547483339905739,
            0.03287208080291748,
            -0.0005921858828514814,
            0.0325988307595253,
            -0.011729293502867222,
            -0.02636870928108692,
            0.005953453481197357,
            -0.037571996450424194,
            0.0070157162845134735,
            -0.02453792840242386,
            0.004792137071490288,
            0.002737632254138589,
            -0.017091842368245125,
            0.010110282339155674,
            0.0030723644886165857,
            0.016982542350888252,
            0.009563780389726162,
            -0.013512255623936653,
            0.009673081338405609,
            0.004829709418118,
            0.0012287750141695142,
            0.0158485509455204,
            -0.007951600477099419,
            -0.008259007707238197,
            0.0018222418148070574,
            -0.012924766167998314,
            -0.015411349013447762,
            -0.022092333063483238,
            -0.023144349455833435,
            0.00914707314223051,
            -0.03057677298784256,
            0.03473018482327461,
            -0.002833270002156496,
            0.0007988318684510887,
            0.006264276336878538,
            0.012692502699792385,
            0.046234048902988434,
            -0.022379247471690178,
            -0.037408046424388885,
            0.013812831602990627,
            0.0038972406182438135,
            -0.006595592945814133,
            -0.016408715397119522,
            -0.050688035786151886,
            0.01008978858590126,
            -0.0005913319764658809,
            -0.013621555641293526,
            -0.004966334439814091,
            0.0068790907971560955,
            -0.001078487024642527,
            0.006527280434966087,
            -0.013621555641293526,
            0.022310933098196983,
            0.025425994768738747,
            -0.005365964025259018,
            -0.007965262979269028,
            -0.020097602158784866,
            0.007582711987197399,
            0.009181229397654533,
            -0.017802294343709946,
            -0.025207392871379852,
            0.031642451882362366,
            -0.012849622406065464,
            -0.04139750823378563,
            -0.00045854912605136633,
            -0.003917734604328871,
            -0.01182493194937706,
            -0.009256373159587383,
            0.006824440788477659,
            0.013013572432100773,
            0.004498392343521118,
            0.016955217346549034,
            0.0161491259932518,
            0.014386658556759357,
            0.002249196171760559,
            0.012275795452296734,
            0.006069585215300322,
            0.013225342147052288,
            -0.043911416083574295,
            0.0020425503607839346,
            0.015028798021376133,
            0.07498004287481308,
            0.006001272238790989,
            -0.02192838303744793,
            0.003606911515817046,
            -0.021381881088018417,
            -0.009290529415011406,
            -0.02020690217614174,
            -0.00872353371232748,
            0.0076510244980454445,
            0.022597847506403923,
            -0.013669375330209732,
            -0.00907192938029766,
            0.015903200954198837,
            -0.004320779349654913,
            0.03451158478856087,
            0.025303030386567116,
            0.01161999348551035,
            -0.03161512687802315,
            -0.023704513907432556,
            -0.014974148012697697,
            -4.317577258916572e-05,
            0.005721190012991428,
            0.02295307256281376,
            0.004822877701371908,
            0.015547974966466427,
            -0.017337767407298088,
            0.027147473767399788,
            0.0159988384693861,
            -0.03197035193443298,
            -0.01657266542315483,
            0.0087918471544981,
            0.012132339179515839,
            0.004013372119516134,
            0.012747153639793396,
            -0.006629749201238155,
            0.021299906075000763,
            0.010581640526652336,
            0.0010195673676207662,
            0.012883778661489487,
            -0.0031748334877192974,
            0.031669776886701584,
            0.024141713976860046,
            0.0024917065165936947,
            -0.003480532905086875,
            0.008798678405582905,
            -0.02451060339808464,
            -0.0019452046835795045,
            0.040304504334926605,
            0.016039825975894928,
            -0.03404705971479416,
            0.015370361506938934,
            0.005150779150426388,
            -0.030959324911236763,
            -0.010854890570044518,
            0.015588962472975254,
            0.025207392871379852,
            0.008634727448225021,
            -0.005082466173917055,
            -0.0008543359581381083,
            -0.026013482362031937,
            -0.018526408821344376,
            -0.03270813077688217,
            -0.013409786857664585,
            -0.02005661465227604,
            -0.01905924826860428,
            -0.0008052361663430929,
            -0.025316692888736725,
            0.008259007707238197,
            -0.03604178875684738,
            0.00815653894096613,
            0.0029203686863183975,
            -0.025781219825148582,
            -0.044375941157341,
            -0.003400265472009778,
            0.006110572721809149,
            -0.010199088603258133,
            0.028500067070126534,
            -0.006315510720014572,
            -0.003753783879801631,
            0.01269933395087719,
            -0.00019896079902537167,
            -0.0115448497235775,
            -0.016367727890610695,
            -0.009153904393315315,
            0.0038733312394469976,
            -0.0018529824446886778,
            -0.024920480325818062,
            -0.020821716636419296,
            0.0037571995053440332,
            0.01226896420121193,
            0.01254904642701149,
            -0.0077534937299788,
            0.00034455227432772517,
            0.009864356368780136,
            -0.007521230261772871,
            0.011654149740934372,
            0.025917844846844673,
            0.020589454099535942,
            -0.004597445949912071,
            -0.01890896074473858,
            -0.007295798510313034,
            -0.027516363188624382,
            -0.022761797532439232,
            -0.031642451882362366,
            0.005792918615043163,
            0.012562708929181099,
            0.028008215129375458,
            0.02306237444281578,
            0.002141603734344244,
            -0.010076126083731651,
            0.012091350741684437,
            -0.004249051213264465,
            0.022133320569992065,
            -0.006650242954492569,
            0.020944679155945778,
            0.010841228067874908,
            0.019291510805487633,
            -0.006274523213505745,
            0.01703719235956669,
            -0.02609545923769474,
            -0.01804821938276291,
            -0.047791577875614166,
            0.03464820981025696,
            0.006356498692184687,
            -0.0139221316203475,
            0.017692994326353073,
            0.014427646063268185,
            0.013211679644882679,
            -0.03328195586800575,
            0.012747153639793396,
            0.009987319819629192,
            0.024715540930628777,
            0.007582711987197399,
            -0.0014243201585486531,
            -0.011503862217068672,
            -0.02016591466963291,
            -0.01886797323822975,
            0.012938428670167923,
            -0.01527472399175167,
            -0.013799169100821018,
            0.011592668481171131,
            0.023035049438476562,
            2.3068885639077052e-05,
            -0.02352689951658249,
            0.016668302938342094,
            -0.025043442845344543,
            -0.008286332711577415,
            0.01886797323822975,
            0.010567978024482727,
            0.04686252400279045,
            -0.02468821592628956,
            0.026286734268069267,
            -0.03989462926983833,
            -0.026163771748542786,
            -7.952027772262227e-06,
            -0.028172165155410767,
            0.0026112536434084177,
            -0.0014098037499934435,
            0.030604097992181778,
            0.0159988384693861,
            0.04257248714566231,
            -0.014550609514117241,
            0.013355136848986149,
            0.00901727844029665,
            -0.005635799374431372,
            -0.008860159665346146,
            0.007172835525125265,
            -0.002059628488495946,
            -0.006042259745299816,
            -0.0038869937416166067,
            -0.0027120148297399282,
            -0.009413492865860462,
            0.025125417858362198,
            -0.004730655811727047,
            -0.00631209509447217,
            0.0005102106370031834,
            -0.015261061489582062,
            -0.012303120456635952,
            -0.02220163308084011,
            -0.02755735069513321,
            -0.02770763821899891,
            0.0022526117973029613,
            0.001700132736004889,
            0.015247398987412453,
            -0.019960977137088776,
            0.00017280982865486294,
            0.01601250097155571,
            0.012876947410404682,
            0.003238022793084383,
            -0.0032465618569403887,
            0.014263696037232876,
            0.006049090996384621,
            -0.00045129089266993105,
            0.023035049438476562,
            0.005946622230112553,
            -0.04199865832924843,
            -0.00526007916778326,
            -0.04279108718037605,
            0.0049629188142716885,
            0.02783060073852539,
            0.007466580253094435,
            0.017843281850218773,
            0.016777602955698967,
            -0.010171763598918915,
            0.00979604385793209,
            0.009058266878128052,
            0.003535183146595955,
            -0.008040406741201878,
            -0.011162297800183296,
            -0.03344590589404106,
            -0.0023123854771256447,
            -0.028172165155410767,
            -0.012583202682435513,
            -0.019127560779452324,
            -0.015124435536563396,
            -0.0017342891078442335,
            0.004621355328708887,
            0.030904673039913177,
            -0.029948296025395393,
            -0.02871866710484028,
            0.029237844049930573,
            0.018785998225212097,
            0.027161136269569397,
            -0.012863284908235073,
            0.024729203432798386,
            0.024578915908932686,
            0.013792337849736214,
            -0.01830780878663063,
            0.011845425702631474,
            0.027448050677776337,
            -0.007200160529464483,
            0.009024109691381454,
            0.003511273767799139,
            -0.012521721422672272,
            -0.00786279421299696,
            0.025029780343174934,
            0.01815752126276493,
            -0.0319976769387722,
            -0.04101495444774628,
            0.016053488478064537,
            0.008887484669685364,
            0.007125016767531633,
            -0.01631307788193226,
            -0.03270813077688217,
            -0.021231593564152718,
            0.012596865184605122,
            -0.007610036991536617,
            0.03218895196914673,
            -0.008696208707988262,
            -0.023581549525260925,
            0.01506978552788496,
            0.0005311314016580582,
            0.015343036502599716,
            -0.016504352912306786,
            -0.006489708088338375,
            0.014441308565437794,
            0.022420234978199005,
            -0.010984685271978378,
            -0.007172835525125265,
            0.007985756732523441,
            0.0003590687410905957,
            0.023253649473190308,
            0.013546411879360676,
            0.006076416466385126,
            -0.007384604774415493,
            0.019414475187659264,
            0.016668302938342094,
            -0.00907192938029766,
            0.0002085672749672085,
            0.030330847948789597,
            -0.02970236912369728,
            -0.01355324313044548,
            -0.008163370192050934,
            -0.015534312464296818,
            0.005953453481197357,
            -0.011292092502117157,
            -0.005482095759361982,
            -0.01616278849542141,
            0.01211867667734623,
            0.01240558922290802,
            0.0239231139421463,
            0.01601250097155571,
            -0.0061071570962667465,
            0.002361912280321121,
            -0.013881144113838673,
            -0.04790087789297104,
            0.001551552675664425,
            -0.001618157490156591,
            0.006438473705202341,
            -0.01933250017464161,
            0.011524355970323086,
            0.02594516985118389,
            -0.023704513907432556,
            0.005311314016580582,
            -0.0016770772635936737,
            -0.024742865934967995,
            -0.005164441652595997,
            0.013116042129695415,
            -0.02262517251074314,
            0.004631602205336094,
            0.019824350252747536,
            0.00814287643879652,
            -0.004973165690898895,
            0.010246907360851765,
            -0.00988485012203455,
            0.0012202359503135085,
            0.009037773124873638,
            -0.00577242486178875,
            -0.02449694089591503,
            -0.006493123713880777,
            -0.013928962871432304,
            0.009270035661756992,
            -0.0058714780025184155,
            -0.0016950092976912856,
            0.0070157162845134735,
            -0.010943697765469551,
            0.004594030324369669,
            -0.009850693866610527,
            -0.00801308173686266,
            0.022980399429798126,
            -0.009386167861521244,
            -0.0002309823757968843,
            0.013642050325870514,
            -0.0221879705786705,
            0.010000982321798801,
            -0.016395052894949913,
            0.001566922990605235,
            -0.019879000261425972,
            -0.03809117153286934,
            -0.012706165201961994,
            0.018813323229551315,
            0.002317508915439248,
            -0.005055141169577837,
            0.0019076326861977577,
            -0.015315711498260498,
            0.0025822208262979984,
            -0.009987319819629192,
            0.01859472133219242,
            0.0011211824603378773,
            0.014755547046661377,
            0.0015558222075924277,
            0.030658748000860214,
            0.019428137689828873,
            0.006547774188220501,
            -0.017665669322013855,
            -0.011742956005036831,
            -0.014304683543741703,
            -0.015206411480903625,
            -0.03273545578122139,
            -0.0071181850507855415,
            -0.03317265585064888,
            0.04918515682220459,
            -0.00222016335465014,
            -0.019660400226712227,
            -0.0016155957709997892,
            -0.01414073258638382,
            -0.030221546068787575,
            -0.011421886272728443,
            -0.008771353401243687,
            0.022857435047626495,
            0.012036700733006,
            0.02781693823635578,
            0.01045867707580328,
            0.02017957717180252,
            -0.008826003409922123,
            0.008163370192050934,
            -0.010731928050518036,
            -0.01132624875754118,
            0.006271107587963343,
            -0.00504489429295063,
            -0.006014934740960598,
            0.004358351230621338,
            -0.028664017096161842,
            -0.002032303484156728,
            0.0155069874599576,
            -0.0175836943089962,
            -0.006349666975438595,
            0.016217438504099846,
            -0.013088717125356197,
            -0.008259007707238197,
            0.017214804887771606,
            -0.01058847177773714,
            -0.0115448497235775,
            0.006233535706996918,
            0.007029378786683083,
            -0.0115448497235775,
            0.004423248581588268,
            0.002957940800115466,
            -0.025057105347514153,
            0.0007245417800731957,
            -0.013095548376441002,
            0.008607402443885803,
            0.005601643119007349,
            -9.211542783305049e-05,
            0.013218510895967484,
            -0.005243001040071249,
            -0.0011946186423301697,
            -0.01253538392484188,
            0.05079733580350876,
            0.002647117944434285,
            0.010246907360851765,
            0.020220564678311348,
            -0.004703330807387829,
            0.004696499556303024,
            -0.0071045225486159325,
            0.009966826066374779,
            0.007282136008143425,
            0.002141603734344244,
            0.028062865138053894,
            -0.025589944794774055,
            -0.01746073178946972,
            -0.012371432967483997,
            -0.0018615216249600053,
            -0.024401303380727768,
            -0.021860070526599884,
            -0.015834888443350792,
            -0.001762468134984374,
            -0.03153315186500549,
            0.01804821938276291,
            0.010861721821129322,
            -0.01491949800401926,
            -0.0008299995097331703,
            -0.007760324981063604,
            0.006435058079659939,
            -0.010363039560616016,
            -0.02959306910634041,
            0.012637852691113949,
            -0.018389783799648285,
            0.0009461311274208128,
            0.0019161717500537634,
            -0.007418761029839516,
            -0.011428717523813248,
            0.012487565167248249,
            0.009693575091660023,
            -0.022010358050465584,
            0.00306382542476058,
            0.2013312429189682,
            -0.026027144864201546,
            -0.005676786880940199,
            0.012385095469653606,
            -0.00025275704683735967,
            -0.020370852202177048,
            0.04128820821642876,
            0.0023516654036939144,
            -0.004604277200996876,
            0.013737687841057777,
            -0.007685180753469467,
            0.004529133439064026,
            -0.024469615891575813,
            -0.0006989245302975178,
            0.0065170335583388805,
            -0.024114388972520828,
            -0.04820145294070244,
            -0.03371915966272354,
            -0.01434567105025053,
            0.008914809674024582,
            -0.012733491137623787,
            0.012931597419083118,
            -0.016244765371084213,
            -0.026792248710989952,
            0.020261552184820175,
            -0.002981850178912282,
            0.004013372119516134,
            0.01978336274623871,
            0.0012586618540808558,
            0.011838594451546669,
            -0.0016173035837709904,
            0.005628968123346567,
            -0.0016036410816013813,
            0.0033439076505601406,
            0.0052498322911560535,
            -0.01818484626710415,
            0.00663658045232296,
            -0.0036888867616653442,
            0.010438183322548866,
            0.022816447541117668,
            0.01848542131483555,
            0.0011544849257916212,
            0.028090190142393112,
            -0.03158780187368393,
            0.011920569464564323,
            0.044184666126966476,
            -0.006349666975438595,
            -0.0016249887412413955,
            -0.008552752435207367,
            -0.0019503281218931079,
            -0.022297270596027374,
            0.01147653628140688,
            0.017351429909467697,
            0.03145117685198784,
            0.0017829620046541095,
            0.006920078303664923,
            -0.02612278424203396,
            0.01743340492248535,
            0.0008654367411509156,
            -0.005232754163444042,
            -0.03404705971479416,
            -0.024578915908932686,
            0.001741120358929038,
            0.010718265548348427,
            -0.012350939214229584,
            -0.006762959063053131,
            -0.009352011606097221,
            0.0062301200814545155,
            -0.0026932288892567158,
            -0.014577934518456459,
            -0.006834687665104866,
            -0.004682837054133415,
            -0.023117024451494217,
            0.012781309895217419,
            -0.026450684294104576,
            -0.05205428972840309,
            0.04033182933926582,
            0.033254630863666534,
            -0.008190695196390152,
            0.03131455183029175,
            -0.004798968322575092,
            -0.023253649473190308,
            0.0016975710168480873,
            -0.026764923706650734,
            -0.006383823696523905,
            -0.02885529212653637,
            0.018266821280121803,
            -0.020999329164624214,
            -0.020411839708685875,
            -0.007022547535598278,
            -0.007869625464081764,
            -0.018239496275782585,
            0.0033661092165857553,
            0.01132624875754118,
            0.002440471900627017,
            0.009597936645150185,
            0.020001964643597603,
            -0.005007322411984205,
            0.004337857477366924,
            -0.016900567337870598,
            -0.021422868594527245,
            0.05268276855349541,
            0.02841809019446373,
            -0.00823168270289898,
            0.01196155697107315,
            0.016559002920985222,
            -0.017679331824183464,
            -0.002886212430894375,
            0.0035693396348506212,
            -0.016039825975894928,
            -0.01543867401778698,
            -0.034675534814596176,
            0.01759735681116581,
            0.007329954765737057,
            -0.006862012669444084,
            0.01030838955193758,
            -0.0034070967230945826,
            -0.0061959633603692055,
            0.027898915112018585,
            0.0005285696825012565,
            -8.635154517833143e-05,
            -0.015903200954198837,
            0.0047750589437782764,
            0.0025736817624419928,
            0.005485511384904385,
            -0.02410072647035122,
            -0.014168057590723038,
            0.0013303902233019471,
            -0.036998167634010315,
            0.000961501500569284,
            0.00674588093534112,
            -0.036998167634010315,
            0.00573826814070344,
            0.012794972397387028,
            -0.004734071437269449,
            -0.004949256312102079,
            0.010547483339905739,
            -0.0025122002698481083,
            -0.009215385653078556,
            -0.006547774188220501,
            -0.0036888867616653442,
            0.011285261251032352,
            -0.008853328414261341,
            -0.005365964025259018,
            0.01800723187625408,
            -0.0040543596260249615,
            0.014113407582044601,
            0.01991998963057995,
            -0.01284279115498066,
            -0.00771933700889349,
            -0.019510112702846527,
            -0.0015686308033764362,
            0.004559874068945646,
            -0.0018137026345357299,
            0.019113898277282715,
            0.02032986469566822,
            -0.0248248428106308,
            0.013109210878610611,
            0.006226704455912113,
            0.024128051474690437,
            -0.04298236221075058,
            0.017050854861736298,
            0.01743340492248535,
            -0.012296289205551147,
            -0.011230611242353916,
            -0.01776130683720112,
            -0.17575496435165405,
            -0.005991025362163782,
            0.003934812732040882,
            -0.05298334360122681,
            0.02783060073852539,
            0.0221879705786705,
            0.019236860796809196,
            -0.013573736883699894,
            -0.032407552003860474,
            -0.0009435694082640111,
            0.008375138975679874,
            -0.008976290933787823,
            -0.017652006819844246,
            -0.009194891899824142,
            -0.015629950910806656,
            0.013034067116677761,
            -0.005348885897547007,
            0.01989266276359558,
            0.02091735415160656,
            0.0015250814612954855,
            0.037544671446084976,
            -0.026942536234855652,
            -0.01670929044485092,
            -0.017201142385601997,
            0.011777112260460854,
            0.007145510520786047,
            -0.004149997606873512,
            0.014413983561098576,
            -0.012282626703381538,
            -0.022160645574331284,
            -0.010294727049767971,
            -0.014427646063268185,
            0.03544063866138458,
            -0.007493905257433653,
            0.024742865934967995,
            0.015261061489582062,
            0.032653480768203735,
            0.0010340837761759758,
            -0.03401973471045494,
            0.002466089092195034,
            0.015793900936841965,
            0.036123763769865036,
            0.02105397917330265,
            0.002672735136002302,
            -0.011203286238014698,
            0.014195382595062256,
            0.016326740384101868,
            -0.03462088480591774,
            0.016258427873253822,
            -0.01573925092816353,
            0.027338750660419464,
            -0.02508443035185337,
            0.0233902744948864,
            -0.00411584135144949,
            0.010861721821129322,
            0.025125417858362198,
            0.0027444635052233934,
            0.004932178184390068,
            -0.010697771795094013,
            -0.019113898277282715,
            -0.012378264218568802,
            -0.011517524719238281,
            0.0038528372533619404,
            -0.009386167861521244,
            -0.015534312464296818,
            -0.028199490159749985,
            -0.013375630602240562,
            0.010950529016554356,
            -0.02681957371532917,
            0.007657855749130249,
            -0.020958341658115387,
            -0.0034634547773748636,
            -0.011162297800183296,
            -0.015889538452029228,
            0.031068624928593636,
            0.023117024451494217,
            0.0005482095875777304,
            0.015124435536563396,
            0.027297763153910637,
            1.6437747035524808e-05,
            -0.008545921184122562,
            0.01916854828596115,
            -0.0032619324047118425,
            0.024852167814970016,
            -0.016627315431833267,
            -0.008101888000965118,
            0.008443452417850494,
            0.014577934518456459,
            -0.028773317113518715,
            -0.010656784288585186,
            0.02522105537354946,
            -0.02090369164943695,
            -0.008040406741201878,
            -0.01508344803005457,
            -0.015302048996090889,
            0.004727240186184645,
            0.00987118761986494,
            0.00013353001850191504,
            -0.0030450394842773676,
            -0.0009785797446966171,
            0.013573736883699894,
            0.00886699091643095,
            -0.0013431988190859556,
            -0.011879581958055496,
            0.037544671446084976,
            0.0018137026345357299,
            -0.030604097992181778,
            0.011134972795844078,
            0.03874697536230087,
            -0.019564762711524963,
            -0.02855471707880497,
            -0.008306826464831829,
            -0.008033575490117073,
            -0.0007518668426200747,
            -0.008771353401243687,
            0.015684600919485092,
            -0.0137718440964818,
            -0.019414475187659264,
            0.016982542350888252,
            0.001952035934664309,
            0.03718944266438484,
            0.00609691021963954,
            -0.001953743863850832,
            0.005277157295495272,
            0.00015156884910538793,
            -0.011196454986929893,
            -0.11706067621707916,
            -0.021805420517921448,
            0.0030843191780149937,
            0.015179086476564407,
            -0.012214314192533493,
            0.0316971018910408,
            -0.00508929742500186,
            0.04546894505620003,
            -0.029046567156910896,
            0.04560557007789612,
            -0.01434567105025053,
            -0.008853328414261341,
            0.007145510520786047,
            -0.0026915210764855146,
            0.014332008548080921,
            -0.00014996776008047163,
            -0.0023875294718891382,
            -0.01815752126276493,
            -0.03587783873081207,
            0.023540562018752098,
            0.019441800191998482,
            -0.0012697626370936632,
            0.007657855749130249,
            -0.03044014796614647,
            -0.0058475686237216,
            -0.01299307867884636,
            -0.03937545046210289,
            0.02798089012503624,
            0.023567887023091316,
            0.0017385586397722363,
            0.009030940942466259,
            -0.028664017096161842,
            0.005666540004312992,
            -0.011339911259710789,
            0.010076126083731651,
            -0.01024007610976696,
            0.010889047756791115,
            -0.014195382595062256,
            0.01802089437842369,
            -0.012986247427761555,
            -0.002857179380953312,
            0.0079242754727602,
            0.013676206581294537,
            -0.010226413607597351,
            -0.007125016767531633,
            -0.02319899946451187,
            -0.008532258681952953,
            0.011674643494188786,
            0.007370942272245884,
            -0.0155069874599576,
            -0.03951207548379898,
            -0.02064410410821438,
            -0.04453989118337631,
            0.013525918126106262,
            0.01875867322087288,
            0.0036205740179866552,
            0.018130196258425713,
            0.016121800988912582,
            -0.013560074381530285,
            -0.00030975547269918025,
            -0.011380898766219616,
            -0.004508639220148325,
            -0.027174798771739006,
            -0.00344296102412045,
            0.01833513379096985,
            -0.0027308010030537844,
            6.217524787643924e-05,
            -0.02494780533015728,
            0.009816537611186504,
            -0.00959110539406538,
            0.006933740805834532,
            0.022283608093857765,
            -0.005854399874806404,
            0.007610036991536617,
            -0.01916854828596115,
            0.0009085591300390661,
            -0.013355136848986149,
            -0.03500343859195709,
            -0.0031270147301256657,
            0.0089421346783638,
            -0.010513327084481716,
            -0.01325949840247631,
            0.007097691297531128,
            -0.0041534132324159145,
            6.009384378558025e-05,
            -0.0029237843118608,
            -0.02020690217614174,
            0.0007377773872576654,
            0.0009128287201747298,
            -0.014413983561098576,
            0.012631021440029144,
            0.028636692091822624,
            0.004518886562436819,
            -0.01414073258638382,
            0.0012347523588687181,
            0.010725096799433231,
            -0.019414475187659264,
            -0.005748515482991934,
            0.028472740203142166,
            0.003120183479040861,
            -0.02724311128258705,
            0.009468142874538898,
            -0.055879805237054825,
            0.042927712202072144,
            0.0014055342180654407,
            -0.010759253054857254,
            -0.0011988881742581725,
            -0.02031620219349861,
            0.01613546349108219,
            -0.020083939656615257,
            0.005410367157310247,
            0.008340982720255852,
            -0.01053382083773613,
            0.016340402886271477,
            -0.01434567105025053,
            -0.0005213114200159907,
            -0.021586818620562553,
            -0.005140532273799181,
            0.018116533756256104,
            0.009679912589490414,
            0.007603205740451813,
            -0.0014644538750872016,
            0.007603205740451813,
            -0.0177339818328619,
            0.022966735064983368,
            0.017925256863236427,
            0.010889047756791115,
            -0.01508344803005457,
            -0.02583586983382702,
            0.015247398987412453,
            -0.009167566895484924,
            -0.010772915557026863,
            -0.01586221344769001,
            -0.01161999348551035,
            -0.006800530944019556,
            0.0078013124875724316,
            -0.019414475187659264,
            -0.010608965530991554,
            0.024633565917611122,
            0.032653480768203735,
            0.026505334302783012,
            0.04626137390732765,
            -0.021231593564152718,
            -0.04254516214132309,
            0.010062463581562042,
            -0.02046648971736431,
            -0.0089421346783638,
            -0.023103361949324608,
            -0.007876456715166569,
            -0.000488435965962708,
            0.02090369164943695,
            0.003811849746853113,
            0.037134792655706406,
            0.015479662455618382,
            -0.029155869036912918,
            -0.03016689606010914,
            -0.008238513953983784,
            0.013778675347566605,
            0.005266910418868065,
            -0.00022436458675656468,
            -0.0013218511594459414,
            0.025207392871379852,
            0.029620394110679626,
            0.005741683766245842,
            0.007077197544276714,
            -0.004891190677881241,
            -0.0018478590063750744,
            -0.006817609537392855,
            -0.007125016767531633,
            0.004573536571115255,
            0.0034258828964084387,
            0.008545921184122562,
            -0.006499954964965582,
            0.011859088204801083,
            0.00382209662348032,
            -0.001369670033454895,
            0.012385095469653606,
            -0.021231593564152718,
            0.016340402886271477,
            0.016477027907967567,
            -0.01804821938276291,
            0.006503370590507984,
            0.008026744239032269,
            -0.016968879848718643,
            0.01081390306353569,
            0.002855471568182111,
            0.023882126435637474,
            0.006042259745299816,
            -0.015957850962877274,
            0.0022594432812184095,
            -0.008921640925109386,
            -0.005417198408395052,
            0.0008667176007293165,
            -0.0036000802647322416,
            -0.011893244460225105,
            0.01670929044485092,
            0.021996695548295975,
            -0.009693575091660023,
            0.00428662309423089,
            0.011988881975412369,
            0.017091842368245125,
            -0.0033251214772462845,
            0.010199088603258133,
            -0.01978336274623871,
            0.007575880270451307,
            -0.010151269845664501,
            -0.033254630863666534,
            0.005994440987706184,
            -0.03153315186500549,
            -0.010410858318209648,
            0.03000294603407383,
            0.014837522059679031,
            0.0156982634216547,
            -0.0036957180127501488,
            0.006708309054374695,
            0.00843662116676569,
            -0.01399044506251812,
            0.020111264660954475,
            -0.010281064547598362,
            -0.0038050184957683086,
            -0.032407552003860474,
            0.039867304265499115,
            0.044922444969415665,
            0.017474394291639328,
            0.040741704404354095,
            -0.016791265457868576,
            0.005994440987706184,
            0.0014337131287902594,
            0.024319328367710114,
            -0.028008215129375458,
            0.014058757573366165,
            -0.01089587900787592,
            -0.013314148411154747,
            -0.008224851451814175,
            -0.012972584925591946,
            0.005082466173917055,
            -0.006113988347351551,
            1.228828386956593e-05,
            -0.012289457954466343,
            0.02205134555697441,
            -0.0017044023843482137,
            0.05563387647271156,
            0.016490690410137177,
            0.0022696901578456163,
            0.02665562368929386,
            -0.014618922024965286,
            0.015302048996090889,
            0.00849810242652893,
            0.025740232318639755,
            -0.00522933853790164,
            -0.017488056793808937,
            -0.0015566761139780283,
            -0.004143166355788708,
            -0.005755346734076738,
            -0.028336115181446075,
            -0.013006741181015968,
            -0.00310310535132885,
            -0.007562217768281698,
            0.013621555641293526,
            -0.0073777735233306885,
            -0.007917444221675396,
            0.02378648892045021,
            0.005051725544035435,
            0.023089699447155,
            0.007507567759603262,
            -0.0051473635248839855,
            0.0007343617035076022,
            0.028308790177106857,
            0.001484947744756937,
            0.0008744028164073825,
            -0.006810777820646763,
            0.03429298475384712,
            0.01873134821653366,
            -0.0335005559027195,
            -0.017952581867575645,
            -0.001010174280963838,
            -0.0016497521428391337,
            0.014045095071196556,
            -0.04008590430021286,
            -0.005161026027053595,
            -0.009638924151659012,
            -0.006711724679917097,
            0.008894315920770168,
            -0.0020562128629535437,
            -0.02263883501291275,
            0.0010853182757273316,
            0.00042780840885825455,
            -0.016326740384101868,
            -0.004112425725907087,
            -0.015821225941181183
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "fdc0edaa-a052-46f5-bc3f-ba8c9173c4f2",
      "type": "child",
      "source": {
        "id": "1d35c58e-1af7-4136-bfeb-f0894fc21db0",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere’s my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey’re actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don’t yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n\nThe ethics of this space remain diabolically complex\n\nMy blog in 2023\n\nHere’s the sequel to this post: Things we learned about LLMs in 2024.\n\nLarge Language Models\n\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\n\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\n\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\n\nSo far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\n\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\n\nThey’re actually quite easy to build\n\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\n\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\n\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\n\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\n\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\n\nThe training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing.\n\nSo training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\n\nYou can run LLMs on your own devices\n\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\n\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\n\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\n\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use.\n\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\n\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\n\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\n\nHobbyists can build their own fine-tuned models\n\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\n\nThere’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\n\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours.\n\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\n\nThis is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\n\nWe don’t yet know how to build GPT-4\n\nFrustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4.\n\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\n\nThis may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out.\n\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then.\n\nStill, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet.\n\nVibes Based Development\n\nAs a computer scientist and software engineer, LLMs are infuriating.\n\nEven the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\n\nI’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\n\nThe worst part is the challenge of evaluating them.\n\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task.\n\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself!\n\nThe most frustrating thing for me is at the level of individual prompting.\n\nSometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out.\n\nWe’re left with what’s effectively Vibes Based Development. It’s vibes all the way down.\n\nI’d love to see us move beyond vibes in 2024!\n\nLLMs are really smart, and also really, really dumb\n\nOn the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun!\n\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\n\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\n\nThe honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\n\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead.\n\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works!\n\nGullibility is the biggest unsolved problem\n\nI coined the term prompt injection in September last year.\n\n15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem.\n\nI’ve written a ton about this already.\n\nBeyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility.\n\nLanguage Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt.\n\nIn order to be useful tools for us, we need them to believe what we feed them!\n\nBut it turns out a lot of the things we want to build need them not to be gullible.\n\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\n\nA lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\n\nI think this is because of gullibility.\n\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\n\nCode may be the best application\n\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.\n\nIf you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\n\nIt’s still astonishing to me how effective they are though.\n\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\n\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\n\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\n\nHow should we feel about this as software engineers?\n\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\n\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\n\nThe ethics of this space remain diabolically complex\n\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\n\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\n\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\n\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\n\nLaw is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\n\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\n\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\n\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\n\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\n\nMy blog in 2023\n\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\n\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\n\nI’ve written a lot about this stuff!\n\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\n\nArticle Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k\n\nI also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\n\nPrompt injection explained, with video, slides, and a transcript\n\nCatching up on the weird world of LLMs\n\nMaking Large Language Models work for you\n\nOpen questions for AI engineering\n\nEmbeddings: What they are and why they matter\n\nFinancial sustainability for open source projects at GitHub Universe\n\nAnd in podcasts:\n\nWhat AI can do for you on the Theory of Change\n\nWorking in public on Path to Citus Con\n\nLLMs break the internet on the Changelog\n\nTalking Large Language Models on Rooftop Ruby\n\nThoughts on the OpenAI board situation on Newsroom Robots\n\nIndustry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1089\n\ngenerative-ai\n            934\n\nllms\n            922\n\nNext: Tom Scott, and the formidable power of escalating streaks\n\nPrevious: Last weeknotes of 2023\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2023_llms.html"
          },
          "headlines": [
            "Large Language Models",
            "Vibes Based Development",
            "LLMs are really smart, and also really, really dumb",
            "Gullibility is the biggest unsolved problem",
            "Code may be the best application"
          ],
          "summary": "In 2023, Large Language Models (LLMs) emerged as a significant development in AI, with advancements making them easier to build and run on personal devices. Despite their capabilities, LLMs face challenges like gullibility and ethical concerns over unlicensed training data. They excel in code generation, where errors can be iteratively corrected. The year saw a proliferation of LLMs from various organizations, though none surpassed GPT-4. The complexity of LLMs remains a barrier, with 'Vibes Based Development' highlighting the difficulty in evaluating their performance. The ethical and legal implications of AI continue to be complex and unresolved.",
          "summary_embedding": [
            -0.01449595857411623,
            -0.005809996742755175,
            0.0007078905473463237,
            -0.03407438471913338,
            -0.0034515000879764557,
            0.01598517596721649,
            -0.015042460523545742,
            0.02121793106198311,
            -0.023909451439976692,
            -0.04071437940001488,
            0.025603607296943665,
            0.020726079121232033,
            -0.023991426452994347,
            -0.0034241750836372375,
            0.0019861923065036535,
            0.0180345568805933,
            0.021983033046126366,
            0.0025992989540100098,
            0.017529044300317764,
            -0.007432423997670412,
            -0.0009734562481753528,
            -0.0001246707106474787,
            -0.014441308565437794,
            -0.017406079918146133,
            -0.005376210901886225,
            0.006428226828575134,
            0.022898422554135323,
            -0.03661561757326126,
            -0.014618922024965286,
            -0.0006575098959729075,
            0.02032986469566822,
            -0.0041534132324159145,
            -0.03789989650249481,
            0.007480242755264044,
            -0.029647719115018845,
            -0.00886699091643095,
            0.013819662854075432,
            -0.007883287966251373,
            0.020575791597366333,
            0.005519667640328407,
            0.028800642117857933,
            0.02869134210050106,
            0.015138098038733006,
            -0.02002928964793682,
            -0.029073892161250114,
            0.017815956845879555,
            0.004792137071490288,
            -0.01644970290362835,
            -0.03341858088970184,
            0.012070856988430023,
            0.019318837672472,
            0.037872571498155594,
            -0.011421886272728443,
            -0.014536946080625057,
            -0.00784913171082735,
            0.002877673367038369,
            -0.014113407582044601,
            0.01066361553966999,
            0.017706656828522682,
            0.00035437222686596215,
            -0.006291601341217756,
            -0.01573925092816353,
            -0.002004978246986866,
            0.024879492819309235,
            -0.006978144403547049,
            -0.010670446790754795,
            -0.027584675699472427,
            0.03314533084630966,
            0.033965084701776505,
            -0.013266329653561115,
            -0.0027427556924521923,
            -0.008095056749880314,
            0.019154885783791542,
            -0.011121310293674469,
            0.017925256863236427,
            0.007992587983608246,
            -0.006277938839048147,
            -0.015261061489582062,
            -0.0275436881929636,
            0.004747733939439058,
            -0.0019281264394521713,
            -0.0006651951116509736,
            -0.003910903353244066,
            0.013150198385119438,
            0.013744519092142582,
            0.004597445949912071,
            -0.005208844784647226,
            -0.017692994326353073,
            -0.0015805854927748442,
            -0.013437111862003803,
            -0.005540161393582821,
            -0.005652877502143383,
            0.016531677916646004,
            0.017228467389941216,
            -0.0062813544645905495,
            -0.0023106776643544436,
            -0.008962628431618214,
            0.002288476098328829,
            -0.005642630625516176,
            -0.026764923706650734,
            -0.00512003805488348,
            0.0024695047177374363,
            -0.040905654430389404,
            -0.008313657715916634,
            -0.023608876392245293,
            0.0037196276243776083,
            0.015684600919485092,
            0.021245256066322327,
            0.015124435536563396,
            0.018949948251247406,
            -0.00663658045232296,
            0.027652988210320473,
            0.016518015414476395,
            -0.02136821858584881,
            -0.0024438875261694193,
            -0.015807563439011574,
            0.0029476936906576157,
            -0.0006071293028071523,
            0.005079050548374653,
            0.008095056749880314,
            0.031095949932932854,
            0.010404027067124844,
            0.013778675347566605,
            -0.0019042170606553555,
            0.019701387733221054,
            -0.015151760540902615,
            0.005929544102400541,
            -0.020726079121232033,
            -0.01234410796314478,
            -0.0003955733263865113,
            0.0020237641874700785,
            -0.0027290931902825832,
            0.010294727049767971,
            0.0027188463136553764,
            -0.026901548728346825,
            0.031369201838970184,
            -0.023704513907432556,
            -0.0034737016540020704,
            0.009386167861521244,
            0.010397195816040039,
            0.0009649171261116862,
            0.02467455342411995,
            -0.022761797532439232,
            0.017515381798148155,
            -0.004358351230621338,
            0.018717685714364052,
            0.009413492865860462,
            0.013361968100070953,
            -0.0024353484623134136,
            -0.000552052166312933,
            0.004450573585927486,
            -0.03153315186500549,
            -0.01802089437842369,
            0.02321266196668148,
            0.004679421428591013,
            0.004518886562436819,
            0.00696106581017375,
            0.023267311975359917,
            -0.031369201838970184,
            -0.010349377058446407,
            -0.014741884544491768,
            0.009563780389726162,
            -0.0032328993547707796,
            0.019797025248408318,
            0.01363521907478571,
            0.021245256066322327,
            0.028800642117857933,
            0.0026932288892567158,
            -0.006749296560883522,
            -0.014687234535813332,
            -0.014427646063268185,
            0.030604097992181778,
            -0.03085002303123474,
            0.011968388222157955,
            -0.015138098038733006,
            0.020111264660954475,
            0.023321961984038353,
            -0.0037025492638349533,
            -0.024783853441476822,
            -0.018553733825683594,
            0.01804821938276291,
            0.009666250087320805,
            0.01613546349108219,
            -0.009270035661756992,
            -0.013505424372851849,
            0.0011843717657029629,
            0.010499664582312107,
            0.0041397507302463055,
            0.012214314192533493,
            -0.02293941006064415,
            0.027652988210320473,
            0.018581058830022812,
            -0.005468433257192373,
            -0.028800642117857933,
            -0.6571137309074402,
            -0.0369708426296711,
            0.015821225941181183,
            -0.010690940544009209,
            0.002980142366141081,
            -0.0036342365201562643,
            0.0009486929047852755,
            -0.01284279115498066,
            -0.015343036502599716,
            0.03306335583329201,
            -0.013525918126106262,
            0.006315510720014572,
            -0.003108228789642453,
            -0.019646737724542618,
            0.011216948740184307,
            -0.009782381355762482,
            0.0042900387197732925,
            -0.026478009298443794,
            0.007610036991536617,
            0.018376121297478676,
            -0.005273741669952869,
            0.019072910770773888,
            -0.0071181850507855415,
            -0.006609255447983742,
            -0.028445415198802948,
            0.006571683567017317,
            0.009010447189211845,
            0.015206411480903625,
            -0.006018350366503,
            0.004180738236755133,
            -0.01097102276980877,
            0.00577242486178875,
            0.0032414384186267853,
            0.016258427873253822,
            0.04273643717169762,
            -0.001592540298588574,
            -0.014004107564687729,
            0.027311425656080246,
            -0.001619865302927792,
            -0.00018519151490181684,
            -0.044758494943380356,
            -0.010943697765469551,
            -0.01646336540579796,
            0.03147850185632706,
            -0.005205429159104824,
            0.03186105191707611,
            0.024401303380727768,
            -0.001023836899548769,
            -0.013819662854075432,
            -0.007972094230353832,
            -0.0005725459777750075,
            0.010205919854342937,
            0.00799941923469305,
            0.00587830925360322,
            -0.008259007707238197,
            0.008183863945305347,
            0.014837522059679031,
            -0.010984685271978378,
            -0.000806943979114294,
            -0.019578425213694572,
            0.006585346069186926,
            -0.016408715397119522,
            -0.021381881088018417,
            -0.03128722682595253,
            -0.013484930619597435,
            0.011059829033911228,
            0.007773987483233213,
            0.012542215175926685,
            0.0005225922795943916,
            -0.024838505312800407,
            -0.00268468982540071,
            0.043337587267160416,
            0.004126088228076696,
            -0.01181126944720745,
            -0.0068859220482409,
            0.01528838649392128,
            0.00994633138179779,
            -0.010615796782076359,
            -0.006687815301120281,
            0.008989953435957432,
            0.006428226828575134,
            -0.018362458795309067,
            0.0042251418344676495,
            -0.0011211824603378773,
            0.03861035034060478,
            -0.006137897726148367,
            0.01658632792532444,
            -0.006120819598436356,
            0.0015626534586772323,
            0.009119748137891293,
            0.010848059318959713,
            0.02349957451224327,
            -0.017501719295978546,
            -0.05607108026742935,
            0.004952671937644482,
            0.009119748137891293,
            -0.013450774364173412,
            0.011449211277067661,
            0.017242129892110825,
            -0.02093101665377617,
            -0.006954234559088945,
            -0.012077688239514828,
            0.03505808860063553,
            0.023704513907432556,
            0.011633655987679958,
            0.0232399869710207,
            0.0023499575909227133,
            0.005625552497804165,
            0.015479662455618382,
            -0.02410072647035122,
            -0.006503370590507984,
            0.002655657008290291,
            0.003606911515817046,
            -0.00566312437877059,
            -0.0068859220482409,
            -0.029620394110679626,
            -0.01053382083773613,
            -0.0029306155629456043,
            0.012446577660739422,
            -0.02683323621749878,
            0.012637852691113949,
            -0.0044130017049610615,
            0.026887886226177216,
            0.00035330484388396144,
            -0.005079050548374653,
            0.004785305820405483,
            -0.002153558423742652,
            -0.006141313351690769,
            -0.005796334240585566,
            0.023745501413941383,
            -0.01643604040145874,
            -0.027775950729846954,
            0.012064025737345219,
            -0.016395052894949913,
            0.028773317113518715,
            0.01629941537976265,
            0.04265446215867996,
            -0.001146799768321216,
            0.006032012868672609,
            0.015343036502599716,
            -0.023690851405262947,
            0.0001412152050761506,
            0.0016659764805808663,
            0.0137718440964818,
            -0.04216260835528374,
            -0.02538500539958477,
            -0.005628968123346567,
            -0.004952671937644482,
            0.011920569464564323,
            -0.016477027907967567,
            -0.017911594361066818,
            -0.008621064946055412,
            -0.016340402886271477,
            0.03415635973215103,
            0.0009871188085526228,
            -0.009433986619114876,
            -0.015520649962127209,
            -0.02378648892045021,
            -0.003485656576231122,
            -0.026300396770238876,
            0.0016958632040768862,
            0.006954234559088945,
            0.0024438875261694193,
            0.006513617932796478,
            0.02468821592628956,
            -0.02064410410821438,
            -0.0076510244980454445,
            0.02710648626089096,
            -0.03358253091573715,
            -0.022966735064983368,
            0.009461311623454094,
            -0.03284475579857826,
            0.012446577660739422,
            0.030084921047091484,
            -0.010704603046178818,
            0.018130196258425713,
            -0.0034515000879764557,
            0.001164731802418828,
            0.008211188949644566,
            -0.011319417506456375,
            0.004358351230621338,
            0.03057677298784256,
            0.014632584527134895,
            -0.004300285596400499,
            0.022447559982538223,
            0.02337661199271679,
            0.03514006361365318,
            0.014058757573366165,
            -0.006414564326405525,
            0.01001464482396841,
            0.00311847566626966,
            0.032407552003860474,
            -0.011640487238764763,
            0.000776630244217813,
            -0.006387239322066307,
            0.022119658067822456,
            -0.0013158736983314157,
            -0.009953162632882595,
            0.009755056351423264,
            0.023841138929128647,
            0.014113407582044601,
            0.024278340861201286,
            0.018225833773612976,
            -0.010902710258960724,
            0.0012535384157672524,
            -0.01814385876059532,
            -0.002088661305606365,
            -0.014782872051000595,
            -0.00263003958389163,
            0.016545340418815613,
            0.009645755402743816,
            0.003593249013647437,
            -0.013034067116677761,
            -0.02250220999121666,
            -0.0016779311699792743,
            0.02855471707880497,
            0.005219091661274433,
            0.011790774762630463,
            -0.014605259522795677,
            -3.9333182940026745e-05,
            0.0129042724147439,
            0.011797606945037842,
            0.018840648233890533,
            -0.012337276712059975,
            -0.0029630642384290695,
            0.019578425213694572,
            -0.025999819859862328,
            0.014332008548080921,
            -0.0006276231142692268,
            -0.0360691137611866,
            0.009843862615525723,
            0.015028798021376133,
            0.024551590904593468,
            0.015588962472975254,
            0.007760324981063604,
            -0.010007813572883606,
            0.026751261204481125,
            -0.016108138486742973,
            0.024278340861201286,
            -0.0026095458306372166,
            -0.003579586511477828,
            0.008696208707988262,
            0.0004265275492798537,
            -0.010547483339905739,
            0.03287208080291748,
            -0.0005921858828514814,
            0.0325988307595253,
            -0.011729293502867222,
            -0.02636870928108692,
            0.005953453481197357,
            -0.037571996450424194,
            0.0070157162845134735,
            -0.02453792840242386,
            0.004792137071490288,
            0.002737632254138589,
            -0.017091842368245125,
            0.010110282339155674,
            0.0030723644886165857,
            0.016982542350888252,
            0.009563780389726162,
            -0.013512255623936653,
            0.009673081338405609,
            0.004829709418118,
            0.0012287750141695142,
            0.0158485509455204,
            -0.007951600477099419,
            -0.008259007707238197,
            0.0018222418148070574,
            -0.012924766167998314,
            -0.015411349013447762,
            -0.022092333063483238,
            -0.023144349455833435,
            0.00914707314223051,
            -0.03057677298784256,
            0.03473018482327461,
            -0.002833270002156496,
            0.0007988318684510887,
            0.006264276336878538,
            0.012692502699792385,
            0.046234048902988434,
            -0.022379247471690178,
            -0.037408046424388885,
            0.013812831602990627,
            0.0038972406182438135,
            -0.006595592945814133,
            -0.016408715397119522,
            -0.050688035786151886,
            0.01008978858590126,
            -0.0005913319764658809,
            -0.013621555641293526,
            -0.004966334439814091,
            0.0068790907971560955,
            -0.001078487024642527,
            0.006527280434966087,
            -0.013621555641293526,
            0.022310933098196983,
            0.025425994768738747,
            -0.005365964025259018,
            -0.007965262979269028,
            -0.020097602158784866,
            0.007582711987197399,
            0.009181229397654533,
            -0.017802294343709946,
            -0.025207392871379852,
            0.031642451882362366,
            -0.012849622406065464,
            -0.04139750823378563,
            -0.00045854912605136633,
            -0.003917734604328871,
            -0.01182493194937706,
            -0.009256373159587383,
            0.006824440788477659,
            0.013013572432100773,
            0.004498392343521118,
            0.016955217346549034,
            0.0161491259932518,
            0.014386658556759357,
            0.002249196171760559,
            0.012275795452296734,
            0.006069585215300322,
            0.013225342147052288,
            -0.043911416083574295,
            0.0020425503607839346,
            0.015028798021376133,
            0.07498004287481308,
            0.006001272238790989,
            -0.02192838303744793,
            0.003606911515817046,
            -0.021381881088018417,
            -0.009290529415011406,
            -0.02020690217614174,
            -0.00872353371232748,
            0.0076510244980454445,
            0.022597847506403923,
            -0.013669375330209732,
            -0.00907192938029766,
            0.015903200954198837,
            -0.004320779349654913,
            0.03451158478856087,
            0.025303030386567116,
            0.01161999348551035,
            -0.03161512687802315,
            -0.023704513907432556,
            -0.014974148012697697,
            -4.317577258916572e-05,
            0.005721190012991428,
            0.02295307256281376,
            0.004822877701371908,
            0.015547974966466427,
            -0.017337767407298088,
            0.027147473767399788,
            0.0159988384693861,
            -0.03197035193443298,
            -0.01657266542315483,
            0.0087918471544981,
            0.012132339179515839,
            0.004013372119516134,
            0.012747153639793396,
            -0.006629749201238155,
            0.021299906075000763,
            0.010581640526652336,
            0.0010195673676207662,
            0.012883778661489487,
            -0.0031748334877192974,
            0.031669776886701584,
            0.024141713976860046,
            0.0024917065165936947,
            -0.003480532905086875,
            0.008798678405582905,
            -0.02451060339808464,
            -0.0019452046835795045,
            0.040304504334926605,
            0.016039825975894928,
            -0.03404705971479416,
            0.015370361506938934,
            0.005150779150426388,
            -0.030959324911236763,
            -0.010854890570044518,
            0.015588962472975254,
            0.025207392871379852,
            0.008634727448225021,
            -0.005082466173917055,
            -0.0008543359581381083,
            -0.026013482362031937,
            -0.018526408821344376,
            -0.03270813077688217,
            -0.013409786857664585,
            -0.02005661465227604,
            -0.01905924826860428,
            -0.0008052361663430929,
            -0.025316692888736725,
            0.008259007707238197,
            -0.03604178875684738,
            0.00815653894096613,
            0.0029203686863183975,
            -0.025781219825148582,
            -0.044375941157341,
            -0.003400265472009778,
            0.006110572721809149,
            -0.010199088603258133,
            0.028500067070126534,
            -0.006315510720014572,
            -0.003753783879801631,
            0.01269933395087719,
            -0.00019896079902537167,
            -0.0115448497235775,
            -0.016367727890610695,
            -0.009153904393315315,
            0.0038733312394469976,
            -0.0018529824446886778,
            -0.024920480325818062,
            -0.020821716636419296,
            0.0037571995053440332,
            0.01226896420121193,
            0.01254904642701149,
            -0.0077534937299788,
            0.00034455227432772517,
            0.009864356368780136,
            -0.007521230261772871,
            0.011654149740934372,
            0.025917844846844673,
            0.020589454099535942,
            -0.004597445949912071,
            -0.01890896074473858,
            -0.007295798510313034,
            -0.027516363188624382,
            -0.022761797532439232,
            -0.031642451882362366,
            0.005792918615043163,
            0.012562708929181099,
            0.028008215129375458,
            0.02306237444281578,
            0.002141603734344244,
            -0.010076126083731651,
            0.012091350741684437,
            -0.004249051213264465,
            0.022133320569992065,
            -0.006650242954492569,
            0.020944679155945778,
            0.010841228067874908,
            0.019291510805487633,
            -0.006274523213505745,
            0.01703719235956669,
            -0.02609545923769474,
            -0.01804821938276291,
            -0.047791577875614166,
            0.03464820981025696,
            0.006356498692184687,
            -0.0139221316203475,
            0.017692994326353073,
            0.014427646063268185,
            0.013211679644882679,
            -0.03328195586800575,
            0.012747153639793396,
            0.009987319819629192,
            0.024715540930628777,
            0.007582711987197399,
            -0.0014243201585486531,
            -0.011503862217068672,
            -0.02016591466963291,
            -0.01886797323822975,
            0.012938428670167923,
            -0.01527472399175167,
            -0.013799169100821018,
            0.011592668481171131,
            0.023035049438476562,
            2.3068885639077052e-05,
            -0.02352689951658249,
            0.016668302938342094,
            -0.025043442845344543,
            -0.008286332711577415,
            0.01886797323822975,
            0.010567978024482727,
            0.04686252400279045,
            -0.02468821592628956,
            0.026286734268069267,
            -0.03989462926983833,
            -0.026163771748542786,
            -7.952027772262227e-06,
            -0.028172165155410767,
            0.0026112536434084177,
            -0.0014098037499934435,
            0.030604097992181778,
            0.0159988384693861,
            0.04257248714566231,
            -0.014550609514117241,
            0.013355136848986149,
            0.00901727844029665,
            -0.005635799374431372,
            -0.008860159665346146,
            0.007172835525125265,
            -0.002059628488495946,
            -0.006042259745299816,
            -0.0038869937416166067,
            -0.0027120148297399282,
            -0.009413492865860462,
            0.025125417858362198,
            -0.004730655811727047,
            -0.00631209509447217,
            0.0005102106370031834,
            -0.015261061489582062,
            -0.012303120456635952,
            -0.02220163308084011,
            -0.02755735069513321,
            -0.02770763821899891,
            0.0022526117973029613,
            0.001700132736004889,
            0.015247398987412453,
            -0.019960977137088776,
            0.00017280982865486294,
            0.01601250097155571,
            0.012876947410404682,
            0.003238022793084383,
            -0.0032465618569403887,
            0.014263696037232876,
            0.006049090996384621,
            -0.00045129089266993105,
            0.023035049438476562,
            0.005946622230112553,
            -0.04199865832924843,
            -0.00526007916778326,
            -0.04279108718037605,
            0.0049629188142716885,
            0.02783060073852539,
            0.007466580253094435,
            0.017843281850218773,
            0.016777602955698967,
            -0.010171763598918915,
            0.00979604385793209,
            0.009058266878128052,
            0.003535183146595955,
            -0.008040406741201878,
            -0.011162297800183296,
            -0.03344590589404106,
            -0.0023123854771256447,
            -0.028172165155410767,
            -0.012583202682435513,
            -0.019127560779452324,
            -0.015124435536563396,
            -0.0017342891078442335,
            0.004621355328708887,
            0.030904673039913177,
            -0.029948296025395393,
            -0.02871866710484028,
            0.029237844049930573,
            0.018785998225212097,
            0.027161136269569397,
            -0.012863284908235073,
            0.024729203432798386,
            0.024578915908932686,
            0.013792337849736214,
            -0.01830780878663063,
            0.011845425702631474,
            0.027448050677776337,
            -0.007200160529464483,
            0.009024109691381454,
            0.003511273767799139,
            -0.012521721422672272,
            -0.00786279421299696,
            0.025029780343174934,
            0.01815752126276493,
            -0.0319976769387722,
            -0.04101495444774628,
            0.016053488478064537,
            0.008887484669685364,
            0.007125016767531633,
            -0.01631307788193226,
            -0.03270813077688217,
            -0.021231593564152718,
            0.012596865184605122,
            -0.007610036991536617,
            0.03218895196914673,
            -0.008696208707988262,
            -0.023581549525260925,
            0.01506978552788496,
            0.0005311314016580582,
            0.015343036502599716,
            -0.016504352912306786,
            -0.006489708088338375,
            0.014441308565437794,
            0.022420234978199005,
            -0.010984685271978378,
            -0.007172835525125265,
            0.007985756732523441,
            0.0003590687410905957,
            0.023253649473190308,
            0.013546411879360676,
            0.006076416466385126,
            -0.007384604774415493,
            0.019414475187659264,
            0.016668302938342094,
            -0.00907192938029766,
            0.0002085672749672085,
            0.030330847948789597,
            -0.02970236912369728,
            -0.01355324313044548,
            -0.008163370192050934,
            -0.015534312464296818,
            0.005953453481197357,
            -0.011292092502117157,
            -0.005482095759361982,
            -0.01616278849542141,
            0.01211867667734623,
            0.01240558922290802,
            0.0239231139421463,
            0.01601250097155571,
            -0.0061071570962667465,
            0.002361912280321121,
            -0.013881144113838673,
            -0.04790087789297104,
            0.001551552675664425,
            -0.001618157490156591,
            0.006438473705202341,
            -0.01933250017464161,
            0.011524355970323086,
            0.02594516985118389,
            -0.023704513907432556,
            0.005311314016580582,
            -0.0016770772635936737,
            -0.024742865934967995,
            -0.005164441652595997,
            0.013116042129695415,
            -0.02262517251074314,
            0.004631602205336094,
            0.019824350252747536,
            0.00814287643879652,
            -0.004973165690898895,
            0.010246907360851765,
            -0.00988485012203455,
            0.0012202359503135085,
            0.009037773124873638,
            -0.00577242486178875,
            -0.02449694089591503,
            -0.006493123713880777,
            -0.013928962871432304,
            0.009270035661756992,
            -0.0058714780025184155,
            -0.0016950092976912856,
            0.0070157162845134735,
            -0.010943697765469551,
            0.004594030324369669,
            -0.009850693866610527,
            -0.00801308173686266,
            0.022980399429798126,
            -0.009386167861521244,
            -0.0002309823757968843,
            0.013642050325870514,
            -0.0221879705786705,
            0.010000982321798801,
            -0.016395052894949913,
            0.001566922990605235,
            -0.019879000261425972,
            -0.03809117153286934,
            -0.012706165201961994,
            0.018813323229551315,
            0.002317508915439248,
            -0.005055141169577837,
            0.0019076326861977577,
            -0.015315711498260498,
            0.0025822208262979984,
            -0.009987319819629192,
            0.01859472133219242,
            0.0011211824603378773,
            0.014755547046661377,
            0.0015558222075924277,
            0.030658748000860214,
            0.019428137689828873,
            0.006547774188220501,
            -0.017665669322013855,
            -0.011742956005036831,
            -0.014304683543741703,
            -0.015206411480903625,
            -0.03273545578122139,
            -0.0071181850507855415,
            -0.03317265585064888,
            0.04918515682220459,
            -0.00222016335465014,
            -0.019660400226712227,
            -0.0016155957709997892,
            -0.01414073258638382,
            -0.030221546068787575,
            -0.011421886272728443,
            -0.008771353401243687,
            0.022857435047626495,
            0.012036700733006,
            0.02781693823635578,
            0.01045867707580328,
            0.02017957717180252,
            -0.008826003409922123,
            0.008163370192050934,
            -0.010731928050518036,
            -0.01132624875754118,
            0.006271107587963343,
            -0.00504489429295063,
            -0.006014934740960598,
            0.004358351230621338,
            -0.028664017096161842,
            -0.002032303484156728,
            0.0155069874599576,
            -0.0175836943089962,
            -0.006349666975438595,
            0.016217438504099846,
            -0.013088717125356197,
            -0.008259007707238197,
            0.017214804887771606,
            -0.01058847177773714,
            -0.0115448497235775,
            0.006233535706996918,
            0.007029378786683083,
            -0.0115448497235775,
            0.004423248581588268,
            0.002957940800115466,
            -0.025057105347514153,
            0.0007245417800731957,
            -0.013095548376441002,
            0.008607402443885803,
            0.005601643119007349,
            -9.211542783305049e-05,
            0.013218510895967484,
            -0.005243001040071249,
            -0.0011946186423301697,
            -0.01253538392484188,
            0.05079733580350876,
            0.002647117944434285,
            0.010246907360851765,
            0.020220564678311348,
            -0.004703330807387829,
            0.004696499556303024,
            -0.0071045225486159325,
            0.009966826066374779,
            0.007282136008143425,
            0.002141603734344244,
            0.028062865138053894,
            -0.025589944794774055,
            -0.01746073178946972,
            -0.012371432967483997,
            -0.0018615216249600053,
            -0.024401303380727768,
            -0.021860070526599884,
            -0.015834888443350792,
            -0.001762468134984374,
            -0.03153315186500549,
            0.01804821938276291,
            0.010861721821129322,
            -0.01491949800401926,
            -0.0008299995097331703,
            -0.007760324981063604,
            0.006435058079659939,
            -0.010363039560616016,
            -0.02959306910634041,
            0.012637852691113949,
            -0.018389783799648285,
            0.0009461311274208128,
            0.0019161717500537634,
            -0.007418761029839516,
            -0.011428717523813248,
            0.012487565167248249,
            0.009693575091660023,
            -0.022010358050465584,
            0.00306382542476058,
            0.2013312429189682,
            -0.026027144864201546,
            -0.005676786880940199,
            0.012385095469653606,
            -0.00025275704683735967,
            -0.020370852202177048,
            0.04128820821642876,
            0.0023516654036939144,
            -0.004604277200996876,
            0.013737687841057777,
            -0.007685180753469467,
            0.004529133439064026,
            -0.024469615891575813,
            -0.0006989245302975178,
            0.0065170335583388805,
            -0.024114388972520828,
            -0.04820145294070244,
            -0.03371915966272354,
            -0.01434567105025053,
            0.008914809674024582,
            -0.012733491137623787,
            0.012931597419083118,
            -0.016244765371084213,
            -0.026792248710989952,
            0.020261552184820175,
            -0.002981850178912282,
            0.004013372119516134,
            0.01978336274623871,
            0.0012586618540808558,
            0.011838594451546669,
            -0.0016173035837709904,
            0.005628968123346567,
            -0.0016036410816013813,
            0.0033439076505601406,
            0.0052498322911560535,
            -0.01818484626710415,
            0.00663658045232296,
            -0.0036888867616653442,
            0.010438183322548866,
            0.022816447541117668,
            0.01848542131483555,
            0.0011544849257916212,
            0.028090190142393112,
            -0.03158780187368393,
            0.011920569464564323,
            0.044184666126966476,
            -0.006349666975438595,
            -0.0016249887412413955,
            -0.008552752435207367,
            -0.0019503281218931079,
            -0.022297270596027374,
            0.01147653628140688,
            0.017351429909467697,
            0.03145117685198784,
            0.0017829620046541095,
            0.006920078303664923,
            -0.02612278424203396,
            0.01743340492248535,
            0.0008654367411509156,
            -0.005232754163444042,
            -0.03404705971479416,
            -0.024578915908932686,
            0.001741120358929038,
            0.010718265548348427,
            -0.012350939214229584,
            -0.006762959063053131,
            -0.009352011606097221,
            0.0062301200814545155,
            -0.0026932288892567158,
            -0.014577934518456459,
            -0.006834687665104866,
            -0.004682837054133415,
            -0.023117024451494217,
            0.012781309895217419,
            -0.026450684294104576,
            -0.05205428972840309,
            0.04033182933926582,
            0.033254630863666534,
            -0.008190695196390152,
            0.03131455183029175,
            -0.004798968322575092,
            -0.023253649473190308,
            0.0016975710168480873,
            -0.026764923706650734,
            -0.006383823696523905,
            -0.02885529212653637,
            0.018266821280121803,
            -0.020999329164624214,
            -0.020411839708685875,
            -0.007022547535598278,
            -0.007869625464081764,
            -0.018239496275782585,
            0.0033661092165857553,
            0.01132624875754118,
            0.002440471900627017,
            0.009597936645150185,
            0.020001964643597603,
            -0.005007322411984205,
            0.004337857477366924,
            -0.016900567337870598,
            -0.021422868594527245,
            0.05268276855349541,
            0.02841809019446373,
            -0.00823168270289898,
            0.01196155697107315,
            0.016559002920985222,
            -0.017679331824183464,
            -0.002886212430894375,
            0.0035693396348506212,
            -0.016039825975894928,
            -0.01543867401778698,
            -0.034675534814596176,
            0.01759735681116581,
            0.007329954765737057,
            -0.006862012669444084,
            0.01030838955193758,
            -0.0034070967230945826,
            -0.0061959633603692055,
            0.027898915112018585,
            0.0005285696825012565,
            -8.635154517833143e-05,
            -0.015903200954198837,
            0.0047750589437782764,
            0.0025736817624419928,
            0.005485511384904385,
            -0.02410072647035122,
            -0.014168057590723038,
            0.0013303902233019471,
            -0.036998167634010315,
            0.000961501500569284,
            0.00674588093534112,
            -0.036998167634010315,
            0.00573826814070344,
            0.012794972397387028,
            -0.004734071437269449,
            -0.004949256312102079,
            0.010547483339905739,
            -0.0025122002698481083,
            -0.009215385653078556,
            -0.006547774188220501,
            -0.0036888867616653442,
            0.011285261251032352,
            -0.008853328414261341,
            -0.005365964025259018,
            0.01800723187625408,
            -0.0040543596260249615,
            0.014113407582044601,
            0.01991998963057995,
            -0.01284279115498066,
            -0.00771933700889349,
            -0.019510112702846527,
            -0.0015686308033764362,
            0.004559874068945646,
            -0.0018137026345357299,
            0.019113898277282715,
            0.02032986469566822,
            -0.0248248428106308,
            0.013109210878610611,
            0.006226704455912113,
            0.024128051474690437,
            -0.04298236221075058,
            0.017050854861736298,
            0.01743340492248535,
            -0.012296289205551147,
            -0.011230611242353916,
            -0.01776130683720112,
            -0.17575496435165405,
            -0.005991025362163782,
            0.003934812732040882,
            -0.05298334360122681,
            0.02783060073852539,
            0.0221879705786705,
            0.019236860796809196,
            -0.013573736883699894,
            -0.032407552003860474,
            -0.0009435694082640111,
            0.008375138975679874,
            -0.008976290933787823,
            -0.017652006819844246,
            -0.009194891899824142,
            -0.015629950910806656,
            0.013034067116677761,
            -0.005348885897547007,
            0.01989266276359558,
            0.02091735415160656,
            0.0015250814612954855,
            0.037544671446084976,
            -0.026942536234855652,
            -0.01670929044485092,
            -0.017201142385601997,
            0.011777112260460854,
            0.007145510520786047,
            -0.004149997606873512,
            0.014413983561098576,
            -0.012282626703381538,
            -0.022160645574331284,
            -0.010294727049767971,
            -0.014427646063268185,
            0.03544063866138458,
            -0.007493905257433653,
            0.024742865934967995,
            0.015261061489582062,
            0.032653480768203735,
            0.0010340837761759758,
            -0.03401973471045494,
            0.002466089092195034,
            0.015793900936841965,
            0.036123763769865036,
            0.02105397917330265,
            0.002672735136002302,
            -0.011203286238014698,
            0.014195382595062256,
            0.016326740384101868,
            -0.03462088480591774,
            0.016258427873253822,
            -0.01573925092816353,
            0.027338750660419464,
            -0.02508443035185337,
            0.0233902744948864,
            -0.00411584135144949,
            0.010861721821129322,
            0.025125417858362198,
            0.0027444635052233934,
            0.004932178184390068,
            -0.010697771795094013,
            -0.019113898277282715,
            -0.012378264218568802,
            -0.011517524719238281,
            0.0038528372533619404,
            -0.009386167861521244,
            -0.015534312464296818,
            -0.028199490159749985,
            -0.013375630602240562,
            0.010950529016554356,
            -0.02681957371532917,
            0.007657855749130249,
            -0.020958341658115387,
            -0.0034634547773748636,
            -0.011162297800183296,
            -0.015889538452029228,
            0.031068624928593636,
            0.023117024451494217,
            0.0005482095875777304,
            0.015124435536563396,
            0.027297763153910637,
            1.6437747035524808e-05,
            -0.008545921184122562,
            0.01916854828596115,
            -0.0032619324047118425,
            0.024852167814970016,
            -0.016627315431833267,
            -0.008101888000965118,
            0.008443452417850494,
            0.014577934518456459,
            -0.028773317113518715,
            -0.010656784288585186,
            0.02522105537354946,
            -0.02090369164943695,
            -0.008040406741201878,
            -0.01508344803005457,
            -0.015302048996090889,
            0.004727240186184645,
            0.00987118761986494,
            0.00013353001850191504,
            -0.0030450394842773676,
            -0.0009785797446966171,
            0.013573736883699894,
            0.00886699091643095,
            -0.0013431988190859556,
            -0.011879581958055496,
            0.037544671446084976,
            0.0018137026345357299,
            -0.030604097992181778,
            0.011134972795844078,
            0.03874697536230087,
            -0.019564762711524963,
            -0.02855471707880497,
            -0.008306826464831829,
            -0.008033575490117073,
            -0.0007518668426200747,
            -0.008771353401243687,
            0.015684600919485092,
            -0.0137718440964818,
            -0.019414475187659264,
            0.016982542350888252,
            0.001952035934664309,
            0.03718944266438484,
            0.00609691021963954,
            -0.001953743863850832,
            0.005277157295495272,
            0.00015156884910538793,
            -0.011196454986929893,
            -0.11706067621707916,
            -0.021805420517921448,
            0.0030843191780149937,
            0.015179086476564407,
            -0.012214314192533493,
            0.0316971018910408,
            -0.00508929742500186,
            0.04546894505620003,
            -0.029046567156910896,
            0.04560557007789612,
            -0.01434567105025053,
            -0.008853328414261341,
            0.007145510520786047,
            -0.0026915210764855146,
            0.014332008548080921,
            -0.00014996776008047163,
            -0.0023875294718891382,
            -0.01815752126276493,
            -0.03587783873081207,
            0.023540562018752098,
            0.019441800191998482,
            -0.0012697626370936632,
            0.007657855749130249,
            -0.03044014796614647,
            -0.0058475686237216,
            -0.01299307867884636,
            -0.03937545046210289,
            0.02798089012503624,
            0.023567887023091316,
            0.0017385586397722363,
            0.009030940942466259,
            -0.028664017096161842,
            0.005666540004312992,
            -0.011339911259710789,
            0.010076126083731651,
            -0.01024007610976696,
            0.010889047756791115,
            -0.014195382595062256,
            0.01802089437842369,
            -0.012986247427761555,
            -0.002857179380953312,
            0.0079242754727602,
            0.013676206581294537,
            -0.010226413607597351,
            -0.007125016767531633,
            -0.02319899946451187,
            -0.008532258681952953,
            0.011674643494188786,
            0.007370942272245884,
            -0.0155069874599576,
            -0.03951207548379898,
            -0.02064410410821438,
            -0.04453989118337631,
            0.013525918126106262,
            0.01875867322087288,
            0.0036205740179866552,
            0.018130196258425713,
            0.016121800988912582,
            -0.013560074381530285,
            -0.00030975547269918025,
            -0.011380898766219616,
            -0.004508639220148325,
            -0.027174798771739006,
            -0.00344296102412045,
            0.01833513379096985,
            -0.0027308010030537844,
            6.217524787643924e-05,
            -0.02494780533015728,
            0.009816537611186504,
            -0.00959110539406538,
            0.006933740805834532,
            0.022283608093857765,
            -0.005854399874806404,
            0.007610036991536617,
            -0.01916854828596115,
            0.0009085591300390661,
            -0.013355136848986149,
            -0.03500343859195709,
            -0.0031270147301256657,
            0.0089421346783638,
            -0.010513327084481716,
            -0.01325949840247631,
            0.007097691297531128,
            -0.0041534132324159145,
            6.009384378558025e-05,
            -0.0029237843118608,
            -0.02020690217614174,
            0.0007377773872576654,
            0.0009128287201747298,
            -0.014413983561098576,
            0.012631021440029144,
            0.028636692091822624,
            0.004518886562436819,
            -0.01414073258638382,
            0.0012347523588687181,
            0.010725096799433231,
            -0.019414475187659264,
            -0.005748515482991934,
            0.028472740203142166,
            0.003120183479040861,
            -0.02724311128258705,
            0.009468142874538898,
            -0.055879805237054825,
            0.042927712202072144,
            0.0014055342180654407,
            -0.010759253054857254,
            -0.0011988881742581725,
            -0.02031620219349861,
            0.01613546349108219,
            -0.020083939656615257,
            0.005410367157310247,
            0.008340982720255852,
            -0.01053382083773613,
            0.016340402886271477,
            -0.01434567105025053,
            -0.0005213114200159907,
            -0.021586818620562553,
            -0.005140532273799181,
            0.018116533756256104,
            0.009679912589490414,
            0.007603205740451813,
            -0.0014644538750872016,
            0.007603205740451813,
            -0.0177339818328619,
            0.022966735064983368,
            0.017925256863236427,
            0.010889047756791115,
            -0.01508344803005457,
            -0.02583586983382702,
            0.015247398987412453,
            -0.009167566895484924,
            -0.010772915557026863,
            -0.01586221344769001,
            -0.01161999348551035,
            -0.006800530944019556,
            0.0078013124875724316,
            -0.019414475187659264,
            -0.010608965530991554,
            0.024633565917611122,
            0.032653480768203735,
            0.026505334302783012,
            0.04626137390732765,
            -0.021231593564152718,
            -0.04254516214132309,
            0.010062463581562042,
            -0.02046648971736431,
            -0.0089421346783638,
            -0.023103361949324608,
            -0.007876456715166569,
            -0.000488435965962708,
            0.02090369164943695,
            0.003811849746853113,
            0.037134792655706406,
            0.015479662455618382,
            -0.029155869036912918,
            -0.03016689606010914,
            -0.008238513953983784,
            0.013778675347566605,
            0.005266910418868065,
            -0.00022436458675656468,
            -0.0013218511594459414,
            0.025207392871379852,
            0.029620394110679626,
            0.005741683766245842,
            0.007077197544276714,
            -0.004891190677881241,
            -0.0018478590063750744,
            -0.006817609537392855,
            -0.007125016767531633,
            0.004573536571115255,
            0.0034258828964084387,
            0.008545921184122562,
            -0.006499954964965582,
            0.011859088204801083,
            0.00382209662348032,
            -0.001369670033454895,
            0.012385095469653606,
            -0.021231593564152718,
            0.016340402886271477,
            0.016477027907967567,
            -0.01804821938276291,
            0.006503370590507984,
            0.008026744239032269,
            -0.016968879848718643,
            0.01081390306353569,
            0.002855471568182111,
            0.023882126435637474,
            0.006042259745299816,
            -0.015957850962877274,
            0.0022594432812184095,
            -0.008921640925109386,
            -0.005417198408395052,
            0.0008667176007293165,
            -0.0036000802647322416,
            -0.011893244460225105,
            0.01670929044485092,
            0.021996695548295975,
            -0.009693575091660023,
            0.00428662309423089,
            0.011988881975412369,
            0.017091842368245125,
            -0.0033251214772462845,
            0.010199088603258133,
            -0.01978336274623871,
            0.007575880270451307,
            -0.010151269845664501,
            -0.033254630863666534,
            0.005994440987706184,
            -0.03153315186500549,
            -0.010410858318209648,
            0.03000294603407383,
            0.014837522059679031,
            0.0156982634216547,
            -0.0036957180127501488,
            0.006708309054374695,
            0.00843662116676569,
            -0.01399044506251812,
            0.020111264660954475,
            -0.010281064547598362,
            -0.0038050184957683086,
            -0.032407552003860474,
            0.039867304265499115,
            0.044922444969415665,
            0.017474394291639328,
            0.040741704404354095,
            -0.016791265457868576,
            0.005994440987706184,
            0.0014337131287902594,
            0.024319328367710114,
            -0.028008215129375458,
            0.014058757573366165,
            -0.01089587900787592,
            -0.013314148411154747,
            -0.008224851451814175,
            -0.012972584925591946,
            0.005082466173917055,
            -0.006113988347351551,
            1.228828386956593e-05,
            -0.012289457954466343,
            0.02205134555697441,
            -0.0017044023843482137,
            0.05563387647271156,
            0.016490690410137177,
            0.0022696901578456163,
            0.02665562368929386,
            -0.014618922024965286,
            0.015302048996090889,
            0.00849810242652893,
            0.025740232318639755,
            -0.00522933853790164,
            -0.017488056793808937,
            -0.0015566761139780283,
            -0.004143166355788708,
            -0.005755346734076738,
            -0.028336115181446075,
            -0.013006741181015968,
            -0.00310310535132885,
            -0.007562217768281698,
            0.013621555641293526,
            -0.0073777735233306885,
            -0.007917444221675396,
            0.02378648892045021,
            0.005051725544035435,
            0.023089699447155,
            0.007507567759603262,
            -0.0051473635248839855,
            0.0007343617035076022,
            0.028308790177106857,
            0.001484947744756937,
            0.0008744028164073825,
            -0.006810777820646763,
            0.03429298475384712,
            0.01873134821653366,
            -0.0335005559027195,
            -0.017952581867575645,
            -0.001010174280963838,
            -0.0016497521428391337,
            0.014045095071196556,
            -0.04008590430021286,
            -0.005161026027053595,
            -0.009638924151659012,
            -0.006711724679917097,
            0.008894315920770168,
            -0.0020562128629535437,
            -0.02263883501291275,
            0.0010853182757273316,
            0.00042780840885825455,
            -0.016326740384101868,
            -0.004112425725907087,
            -0.015821225941181183
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "1e94db5b-0dda-42ed-83ba-f4e7e8a1c06d",
      "type": "child",
      "source": {
        "id": "1d35c58e-1af7-4136-bfeb-f0894fc21db0",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere’s my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey’re actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don’t yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n\nThe ethics of this space remain diabolically complex\n\nMy blog in 2023\n\nHere’s the sequel to this post: Things we learned about LLMs in 2024.\n\nLarge Language Models\n\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\n\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\n\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\n\nSo far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\n\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\n\nThey’re actually quite easy to build\n\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\n\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\n\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\n\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\n\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\n\nThe training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing.\n\nSo training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\n\nYou can run LLMs on your own devices\n\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\n\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\n\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\n\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use.\n\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\n\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\n\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\n\nHobbyists can build their own fine-tuned models\n\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\n\nThere’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\n\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours.\n\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\n\nThis is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\n\nWe don’t yet know how to build GPT-4\n\nFrustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4.\n\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\n\nThis may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out.\n\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then.\n\nStill, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet.\n\nVibes Based Development\n\nAs a computer scientist and software engineer, LLMs are infuriating.\n\nEven the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\n\nI’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\n\nThe worst part is the challenge of evaluating them.\n\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task.\n\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself!\n\nThe most frustrating thing for me is at the level of individual prompting.\n\nSometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out.\n\nWe’re left with what’s effectively Vibes Based Development. It’s vibes all the way down.\n\nI’d love to see us move beyond vibes in 2024!\n\nLLMs are really smart, and also really, really dumb\n\nOn the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun!\n\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\n\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\n\nThe honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\n\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead.\n\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works!\n\nGullibility is the biggest unsolved problem\n\nI coined the term prompt injection in September last year.\n\n15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem.\n\nI’ve written a ton about this already.\n\nBeyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility.\n\nLanguage Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt.\n\nIn order to be useful tools for us, we need them to believe what we feed them!\n\nBut it turns out a lot of the things we want to build need them not to be gullible.\n\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\n\nA lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\n\nI think this is because of gullibility.\n\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\n\nCode may be the best application\n\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.\n\nIf you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\n\nIt’s still astonishing to me how effective they are though.\n\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\n\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\n\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\n\nHow should we feel about this as software engineers?\n\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\n\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\n\nThe ethics of this space remain diabolically complex\n\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\n\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\n\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\n\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\n\nLaw is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\n\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\n\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\n\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\n\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\n\nMy blog in 2023\n\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\n\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\n\nI’ve written a lot about this stuff!\n\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\n\nArticle Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k\n\nI also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\n\nPrompt injection explained, with video, slides, and a transcript\n\nCatching up on the weird world of LLMs\n\nMaking Large Language Models work for you\n\nOpen questions for AI engineering\n\nEmbeddings: What they are and why they matter\n\nFinancial sustainability for open source projects at GitHub Universe\n\nAnd in podcasts:\n\nWhat AI can do for you on the Theory of Change\n\nWorking in public on Path to Citus Con\n\nLLMs break the internet on the Changelog\n\nTalking Large Language Models on Rooftop Ruby\n\nThoughts on the OpenAI board situation on Newsroom Robots\n\nIndustry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1089\n\ngenerative-ai\n            934\n\nllms\n            922\n\nNext: Tom Scott, and the formidable power of escalating streaks\n\nPrevious: Last weeknotes of 2023\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2023_llms.html"
          },
          "headlines": [
            "Large Language Models",
            "Vibes Based Development",
            "LLMs are really smart, and also really, really dumb",
            "Gullibility is the biggest unsolved problem",
            "Code may be the best application"
          ],
          "summary": "In 2023, Large Language Models (LLMs) emerged as a significant development in AI, with advancements making them easier to build and run on personal devices. Despite their capabilities, LLMs face challenges like gullibility and ethical concerns over unlicensed training data. They excel in code generation, where errors can be iteratively corrected. The year saw a proliferation of LLMs from various organizations, though none surpassed GPT-4. The complexity of LLMs remains a barrier, with 'Vibes Based Development' highlighting the difficulty in evaluating their performance. The ethical and legal implications of AI continue to be complex and unresolved.",
          "summary_embedding": [
            -0.01449595857411623,
            -0.005809996742755175,
            0.0007078905473463237,
            -0.03407438471913338,
            -0.0034515000879764557,
            0.01598517596721649,
            -0.015042460523545742,
            0.02121793106198311,
            -0.023909451439976692,
            -0.04071437940001488,
            0.025603607296943665,
            0.020726079121232033,
            -0.023991426452994347,
            -0.0034241750836372375,
            0.0019861923065036535,
            0.0180345568805933,
            0.021983033046126366,
            0.0025992989540100098,
            0.017529044300317764,
            -0.007432423997670412,
            -0.0009734562481753528,
            -0.0001246707106474787,
            -0.014441308565437794,
            -0.017406079918146133,
            -0.005376210901886225,
            0.006428226828575134,
            0.022898422554135323,
            -0.03661561757326126,
            -0.014618922024965286,
            -0.0006575098959729075,
            0.02032986469566822,
            -0.0041534132324159145,
            -0.03789989650249481,
            0.007480242755264044,
            -0.029647719115018845,
            -0.00886699091643095,
            0.013819662854075432,
            -0.007883287966251373,
            0.020575791597366333,
            0.005519667640328407,
            0.028800642117857933,
            0.02869134210050106,
            0.015138098038733006,
            -0.02002928964793682,
            -0.029073892161250114,
            0.017815956845879555,
            0.004792137071490288,
            -0.01644970290362835,
            -0.03341858088970184,
            0.012070856988430023,
            0.019318837672472,
            0.037872571498155594,
            -0.011421886272728443,
            -0.014536946080625057,
            -0.00784913171082735,
            0.002877673367038369,
            -0.014113407582044601,
            0.01066361553966999,
            0.017706656828522682,
            0.00035437222686596215,
            -0.006291601341217756,
            -0.01573925092816353,
            -0.002004978246986866,
            0.024879492819309235,
            -0.006978144403547049,
            -0.010670446790754795,
            -0.027584675699472427,
            0.03314533084630966,
            0.033965084701776505,
            -0.013266329653561115,
            -0.0027427556924521923,
            -0.008095056749880314,
            0.019154885783791542,
            -0.011121310293674469,
            0.017925256863236427,
            0.007992587983608246,
            -0.006277938839048147,
            -0.015261061489582062,
            -0.0275436881929636,
            0.004747733939439058,
            -0.0019281264394521713,
            -0.0006651951116509736,
            -0.003910903353244066,
            0.013150198385119438,
            0.013744519092142582,
            0.004597445949912071,
            -0.005208844784647226,
            -0.017692994326353073,
            -0.0015805854927748442,
            -0.013437111862003803,
            -0.005540161393582821,
            -0.005652877502143383,
            0.016531677916646004,
            0.017228467389941216,
            -0.0062813544645905495,
            -0.0023106776643544436,
            -0.008962628431618214,
            0.002288476098328829,
            -0.005642630625516176,
            -0.026764923706650734,
            -0.00512003805488348,
            0.0024695047177374363,
            -0.040905654430389404,
            -0.008313657715916634,
            -0.023608876392245293,
            0.0037196276243776083,
            0.015684600919485092,
            0.021245256066322327,
            0.015124435536563396,
            0.018949948251247406,
            -0.00663658045232296,
            0.027652988210320473,
            0.016518015414476395,
            -0.02136821858584881,
            -0.0024438875261694193,
            -0.015807563439011574,
            0.0029476936906576157,
            -0.0006071293028071523,
            0.005079050548374653,
            0.008095056749880314,
            0.031095949932932854,
            0.010404027067124844,
            0.013778675347566605,
            -0.0019042170606553555,
            0.019701387733221054,
            -0.015151760540902615,
            0.005929544102400541,
            -0.020726079121232033,
            -0.01234410796314478,
            -0.0003955733263865113,
            0.0020237641874700785,
            -0.0027290931902825832,
            0.010294727049767971,
            0.0027188463136553764,
            -0.026901548728346825,
            0.031369201838970184,
            -0.023704513907432556,
            -0.0034737016540020704,
            0.009386167861521244,
            0.010397195816040039,
            0.0009649171261116862,
            0.02467455342411995,
            -0.022761797532439232,
            0.017515381798148155,
            -0.004358351230621338,
            0.018717685714364052,
            0.009413492865860462,
            0.013361968100070953,
            -0.0024353484623134136,
            -0.000552052166312933,
            0.004450573585927486,
            -0.03153315186500549,
            -0.01802089437842369,
            0.02321266196668148,
            0.004679421428591013,
            0.004518886562436819,
            0.00696106581017375,
            0.023267311975359917,
            -0.031369201838970184,
            -0.010349377058446407,
            -0.014741884544491768,
            0.009563780389726162,
            -0.0032328993547707796,
            0.019797025248408318,
            0.01363521907478571,
            0.021245256066322327,
            0.028800642117857933,
            0.0026932288892567158,
            -0.006749296560883522,
            -0.014687234535813332,
            -0.014427646063268185,
            0.030604097992181778,
            -0.03085002303123474,
            0.011968388222157955,
            -0.015138098038733006,
            0.020111264660954475,
            0.023321961984038353,
            -0.0037025492638349533,
            -0.024783853441476822,
            -0.018553733825683594,
            0.01804821938276291,
            0.009666250087320805,
            0.01613546349108219,
            -0.009270035661756992,
            -0.013505424372851849,
            0.0011843717657029629,
            0.010499664582312107,
            0.0041397507302463055,
            0.012214314192533493,
            -0.02293941006064415,
            0.027652988210320473,
            0.018581058830022812,
            -0.005468433257192373,
            -0.028800642117857933,
            -0.6571137309074402,
            -0.0369708426296711,
            0.015821225941181183,
            -0.010690940544009209,
            0.002980142366141081,
            -0.0036342365201562643,
            0.0009486929047852755,
            -0.01284279115498066,
            -0.015343036502599716,
            0.03306335583329201,
            -0.013525918126106262,
            0.006315510720014572,
            -0.003108228789642453,
            -0.019646737724542618,
            0.011216948740184307,
            -0.009782381355762482,
            0.0042900387197732925,
            -0.026478009298443794,
            0.007610036991536617,
            0.018376121297478676,
            -0.005273741669952869,
            0.019072910770773888,
            -0.0071181850507855415,
            -0.006609255447983742,
            -0.028445415198802948,
            0.006571683567017317,
            0.009010447189211845,
            0.015206411480903625,
            -0.006018350366503,
            0.004180738236755133,
            -0.01097102276980877,
            0.00577242486178875,
            0.0032414384186267853,
            0.016258427873253822,
            0.04273643717169762,
            -0.001592540298588574,
            -0.014004107564687729,
            0.027311425656080246,
            -0.001619865302927792,
            -0.00018519151490181684,
            -0.044758494943380356,
            -0.010943697765469551,
            -0.01646336540579796,
            0.03147850185632706,
            -0.005205429159104824,
            0.03186105191707611,
            0.024401303380727768,
            -0.001023836899548769,
            -0.013819662854075432,
            -0.007972094230353832,
            -0.0005725459777750075,
            0.010205919854342937,
            0.00799941923469305,
            0.00587830925360322,
            -0.008259007707238197,
            0.008183863945305347,
            0.014837522059679031,
            -0.010984685271978378,
            -0.000806943979114294,
            -0.019578425213694572,
            0.006585346069186926,
            -0.016408715397119522,
            -0.021381881088018417,
            -0.03128722682595253,
            -0.013484930619597435,
            0.011059829033911228,
            0.007773987483233213,
            0.012542215175926685,
            0.0005225922795943916,
            -0.024838505312800407,
            -0.00268468982540071,
            0.043337587267160416,
            0.004126088228076696,
            -0.01181126944720745,
            -0.0068859220482409,
            0.01528838649392128,
            0.00994633138179779,
            -0.010615796782076359,
            -0.006687815301120281,
            0.008989953435957432,
            0.006428226828575134,
            -0.018362458795309067,
            0.0042251418344676495,
            -0.0011211824603378773,
            0.03861035034060478,
            -0.006137897726148367,
            0.01658632792532444,
            -0.006120819598436356,
            0.0015626534586772323,
            0.009119748137891293,
            0.010848059318959713,
            0.02349957451224327,
            -0.017501719295978546,
            -0.05607108026742935,
            0.004952671937644482,
            0.009119748137891293,
            -0.013450774364173412,
            0.011449211277067661,
            0.017242129892110825,
            -0.02093101665377617,
            -0.006954234559088945,
            -0.012077688239514828,
            0.03505808860063553,
            0.023704513907432556,
            0.011633655987679958,
            0.0232399869710207,
            0.0023499575909227133,
            0.005625552497804165,
            0.015479662455618382,
            -0.02410072647035122,
            -0.006503370590507984,
            0.002655657008290291,
            0.003606911515817046,
            -0.00566312437877059,
            -0.0068859220482409,
            -0.029620394110679626,
            -0.01053382083773613,
            -0.0029306155629456043,
            0.012446577660739422,
            -0.02683323621749878,
            0.012637852691113949,
            -0.0044130017049610615,
            0.026887886226177216,
            0.00035330484388396144,
            -0.005079050548374653,
            0.004785305820405483,
            -0.002153558423742652,
            -0.006141313351690769,
            -0.005796334240585566,
            0.023745501413941383,
            -0.01643604040145874,
            -0.027775950729846954,
            0.012064025737345219,
            -0.016395052894949913,
            0.028773317113518715,
            0.01629941537976265,
            0.04265446215867996,
            -0.001146799768321216,
            0.006032012868672609,
            0.015343036502599716,
            -0.023690851405262947,
            0.0001412152050761506,
            0.0016659764805808663,
            0.0137718440964818,
            -0.04216260835528374,
            -0.02538500539958477,
            -0.005628968123346567,
            -0.004952671937644482,
            0.011920569464564323,
            -0.016477027907967567,
            -0.017911594361066818,
            -0.008621064946055412,
            -0.016340402886271477,
            0.03415635973215103,
            0.0009871188085526228,
            -0.009433986619114876,
            -0.015520649962127209,
            -0.02378648892045021,
            -0.003485656576231122,
            -0.026300396770238876,
            0.0016958632040768862,
            0.006954234559088945,
            0.0024438875261694193,
            0.006513617932796478,
            0.02468821592628956,
            -0.02064410410821438,
            -0.0076510244980454445,
            0.02710648626089096,
            -0.03358253091573715,
            -0.022966735064983368,
            0.009461311623454094,
            -0.03284475579857826,
            0.012446577660739422,
            0.030084921047091484,
            -0.010704603046178818,
            0.018130196258425713,
            -0.0034515000879764557,
            0.001164731802418828,
            0.008211188949644566,
            -0.011319417506456375,
            0.004358351230621338,
            0.03057677298784256,
            0.014632584527134895,
            -0.004300285596400499,
            0.022447559982538223,
            0.02337661199271679,
            0.03514006361365318,
            0.014058757573366165,
            -0.006414564326405525,
            0.01001464482396841,
            0.00311847566626966,
            0.032407552003860474,
            -0.011640487238764763,
            0.000776630244217813,
            -0.006387239322066307,
            0.022119658067822456,
            -0.0013158736983314157,
            -0.009953162632882595,
            0.009755056351423264,
            0.023841138929128647,
            0.014113407582044601,
            0.024278340861201286,
            0.018225833773612976,
            -0.010902710258960724,
            0.0012535384157672524,
            -0.01814385876059532,
            -0.002088661305606365,
            -0.014782872051000595,
            -0.00263003958389163,
            0.016545340418815613,
            0.009645755402743816,
            0.003593249013647437,
            -0.013034067116677761,
            -0.02250220999121666,
            -0.0016779311699792743,
            0.02855471707880497,
            0.005219091661274433,
            0.011790774762630463,
            -0.014605259522795677,
            -3.9333182940026745e-05,
            0.0129042724147439,
            0.011797606945037842,
            0.018840648233890533,
            -0.012337276712059975,
            -0.0029630642384290695,
            0.019578425213694572,
            -0.025999819859862328,
            0.014332008548080921,
            -0.0006276231142692268,
            -0.0360691137611866,
            0.009843862615525723,
            0.015028798021376133,
            0.024551590904593468,
            0.015588962472975254,
            0.007760324981063604,
            -0.010007813572883606,
            0.026751261204481125,
            -0.016108138486742973,
            0.024278340861201286,
            -0.0026095458306372166,
            -0.003579586511477828,
            0.008696208707988262,
            0.0004265275492798537,
            -0.010547483339905739,
            0.03287208080291748,
            -0.0005921858828514814,
            0.0325988307595253,
            -0.011729293502867222,
            -0.02636870928108692,
            0.005953453481197357,
            -0.037571996450424194,
            0.0070157162845134735,
            -0.02453792840242386,
            0.004792137071490288,
            0.002737632254138589,
            -0.017091842368245125,
            0.010110282339155674,
            0.0030723644886165857,
            0.016982542350888252,
            0.009563780389726162,
            -0.013512255623936653,
            0.009673081338405609,
            0.004829709418118,
            0.0012287750141695142,
            0.0158485509455204,
            -0.007951600477099419,
            -0.008259007707238197,
            0.0018222418148070574,
            -0.012924766167998314,
            -0.015411349013447762,
            -0.022092333063483238,
            -0.023144349455833435,
            0.00914707314223051,
            -0.03057677298784256,
            0.03473018482327461,
            -0.002833270002156496,
            0.0007988318684510887,
            0.006264276336878538,
            0.012692502699792385,
            0.046234048902988434,
            -0.022379247471690178,
            -0.037408046424388885,
            0.013812831602990627,
            0.0038972406182438135,
            -0.006595592945814133,
            -0.016408715397119522,
            -0.050688035786151886,
            0.01008978858590126,
            -0.0005913319764658809,
            -0.013621555641293526,
            -0.004966334439814091,
            0.0068790907971560955,
            -0.001078487024642527,
            0.006527280434966087,
            -0.013621555641293526,
            0.022310933098196983,
            0.025425994768738747,
            -0.005365964025259018,
            -0.007965262979269028,
            -0.020097602158784866,
            0.007582711987197399,
            0.009181229397654533,
            -0.017802294343709946,
            -0.025207392871379852,
            0.031642451882362366,
            -0.012849622406065464,
            -0.04139750823378563,
            -0.00045854912605136633,
            -0.003917734604328871,
            -0.01182493194937706,
            -0.009256373159587383,
            0.006824440788477659,
            0.013013572432100773,
            0.004498392343521118,
            0.016955217346549034,
            0.0161491259932518,
            0.014386658556759357,
            0.002249196171760559,
            0.012275795452296734,
            0.006069585215300322,
            0.013225342147052288,
            -0.043911416083574295,
            0.0020425503607839346,
            0.015028798021376133,
            0.07498004287481308,
            0.006001272238790989,
            -0.02192838303744793,
            0.003606911515817046,
            -0.021381881088018417,
            -0.009290529415011406,
            -0.02020690217614174,
            -0.00872353371232748,
            0.0076510244980454445,
            0.022597847506403923,
            -0.013669375330209732,
            -0.00907192938029766,
            0.015903200954198837,
            -0.004320779349654913,
            0.03451158478856087,
            0.025303030386567116,
            0.01161999348551035,
            -0.03161512687802315,
            -0.023704513907432556,
            -0.014974148012697697,
            -4.317577258916572e-05,
            0.005721190012991428,
            0.02295307256281376,
            0.004822877701371908,
            0.015547974966466427,
            -0.017337767407298088,
            0.027147473767399788,
            0.0159988384693861,
            -0.03197035193443298,
            -0.01657266542315483,
            0.0087918471544981,
            0.012132339179515839,
            0.004013372119516134,
            0.012747153639793396,
            -0.006629749201238155,
            0.021299906075000763,
            0.010581640526652336,
            0.0010195673676207662,
            0.012883778661489487,
            -0.0031748334877192974,
            0.031669776886701584,
            0.024141713976860046,
            0.0024917065165936947,
            -0.003480532905086875,
            0.008798678405582905,
            -0.02451060339808464,
            -0.0019452046835795045,
            0.040304504334926605,
            0.016039825975894928,
            -0.03404705971479416,
            0.015370361506938934,
            0.005150779150426388,
            -0.030959324911236763,
            -0.010854890570044518,
            0.015588962472975254,
            0.025207392871379852,
            0.008634727448225021,
            -0.005082466173917055,
            -0.0008543359581381083,
            -0.026013482362031937,
            -0.018526408821344376,
            -0.03270813077688217,
            -0.013409786857664585,
            -0.02005661465227604,
            -0.01905924826860428,
            -0.0008052361663430929,
            -0.025316692888736725,
            0.008259007707238197,
            -0.03604178875684738,
            0.00815653894096613,
            0.0029203686863183975,
            -0.025781219825148582,
            -0.044375941157341,
            -0.003400265472009778,
            0.006110572721809149,
            -0.010199088603258133,
            0.028500067070126534,
            -0.006315510720014572,
            -0.003753783879801631,
            0.01269933395087719,
            -0.00019896079902537167,
            -0.0115448497235775,
            -0.016367727890610695,
            -0.009153904393315315,
            0.0038733312394469976,
            -0.0018529824446886778,
            -0.024920480325818062,
            -0.020821716636419296,
            0.0037571995053440332,
            0.01226896420121193,
            0.01254904642701149,
            -0.0077534937299788,
            0.00034455227432772517,
            0.009864356368780136,
            -0.007521230261772871,
            0.011654149740934372,
            0.025917844846844673,
            0.020589454099535942,
            -0.004597445949912071,
            -0.01890896074473858,
            -0.007295798510313034,
            -0.027516363188624382,
            -0.022761797532439232,
            -0.031642451882362366,
            0.005792918615043163,
            0.012562708929181099,
            0.028008215129375458,
            0.02306237444281578,
            0.002141603734344244,
            -0.010076126083731651,
            0.012091350741684437,
            -0.004249051213264465,
            0.022133320569992065,
            -0.006650242954492569,
            0.020944679155945778,
            0.010841228067874908,
            0.019291510805487633,
            -0.006274523213505745,
            0.01703719235956669,
            -0.02609545923769474,
            -0.01804821938276291,
            -0.047791577875614166,
            0.03464820981025696,
            0.006356498692184687,
            -0.0139221316203475,
            0.017692994326353073,
            0.014427646063268185,
            0.013211679644882679,
            -0.03328195586800575,
            0.012747153639793396,
            0.009987319819629192,
            0.024715540930628777,
            0.007582711987197399,
            -0.0014243201585486531,
            -0.011503862217068672,
            -0.02016591466963291,
            -0.01886797323822975,
            0.012938428670167923,
            -0.01527472399175167,
            -0.013799169100821018,
            0.011592668481171131,
            0.023035049438476562,
            2.3068885639077052e-05,
            -0.02352689951658249,
            0.016668302938342094,
            -0.025043442845344543,
            -0.008286332711577415,
            0.01886797323822975,
            0.010567978024482727,
            0.04686252400279045,
            -0.02468821592628956,
            0.026286734268069267,
            -0.03989462926983833,
            -0.026163771748542786,
            -7.952027772262227e-06,
            -0.028172165155410767,
            0.0026112536434084177,
            -0.0014098037499934435,
            0.030604097992181778,
            0.0159988384693861,
            0.04257248714566231,
            -0.014550609514117241,
            0.013355136848986149,
            0.00901727844029665,
            -0.005635799374431372,
            -0.008860159665346146,
            0.007172835525125265,
            -0.002059628488495946,
            -0.006042259745299816,
            -0.0038869937416166067,
            -0.0027120148297399282,
            -0.009413492865860462,
            0.025125417858362198,
            -0.004730655811727047,
            -0.00631209509447217,
            0.0005102106370031834,
            -0.015261061489582062,
            -0.012303120456635952,
            -0.02220163308084011,
            -0.02755735069513321,
            -0.02770763821899891,
            0.0022526117973029613,
            0.001700132736004889,
            0.015247398987412453,
            -0.019960977137088776,
            0.00017280982865486294,
            0.01601250097155571,
            0.012876947410404682,
            0.003238022793084383,
            -0.0032465618569403887,
            0.014263696037232876,
            0.006049090996384621,
            -0.00045129089266993105,
            0.023035049438476562,
            0.005946622230112553,
            -0.04199865832924843,
            -0.00526007916778326,
            -0.04279108718037605,
            0.0049629188142716885,
            0.02783060073852539,
            0.007466580253094435,
            0.017843281850218773,
            0.016777602955698967,
            -0.010171763598918915,
            0.00979604385793209,
            0.009058266878128052,
            0.003535183146595955,
            -0.008040406741201878,
            -0.011162297800183296,
            -0.03344590589404106,
            -0.0023123854771256447,
            -0.028172165155410767,
            -0.012583202682435513,
            -0.019127560779452324,
            -0.015124435536563396,
            -0.0017342891078442335,
            0.004621355328708887,
            0.030904673039913177,
            -0.029948296025395393,
            -0.02871866710484028,
            0.029237844049930573,
            0.018785998225212097,
            0.027161136269569397,
            -0.012863284908235073,
            0.024729203432798386,
            0.024578915908932686,
            0.013792337849736214,
            -0.01830780878663063,
            0.011845425702631474,
            0.027448050677776337,
            -0.007200160529464483,
            0.009024109691381454,
            0.003511273767799139,
            -0.012521721422672272,
            -0.00786279421299696,
            0.025029780343174934,
            0.01815752126276493,
            -0.0319976769387722,
            -0.04101495444774628,
            0.016053488478064537,
            0.008887484669685364,
            0.007125016767531633,
            -0.01631307788193226,
            -0.03270813077688217,
            -0.021231593564152718,
            0.012596865184605122,
            -0.007610036991536617,
            0.03218895196914673,
            -0.008696208707988262,
            -0.023581549525260925,
            0.01506978552788496,
            0.0005311314016580582,
            0.015343036502599716,
            -0.016504352912306786,
            -0.006489708088338375,
            0.014441308565437794,
            0.022420234978199005,
            -0.010984685271978378,
            -0.007172835525125265,
            0.007985756732523441,
            0.0003590687410905957,
            0.023253649473190308,
            0.013546411879360676,
            0.006076416466385126,
            -0.007384604774415493,
            0.019414475187659264,
            0.016668302938342094,
            -0.00907192938029766,
            0.0002085672749672085,
            0.030330847948789597,
            -0.02970236912369728,
            -0.01355324313044548,
            -0.008163370192050934,
            -0.015534312464296818,
            0.005953453481197357,
            -0.011292092502117157,
            -0.005482095759361982,
            -0.01616278849542141,
            0.01211867667734623,
            0.01240558922290802,
            0.0239231139421463,
            0.01601250097155571,
            -0.0061071570962667465,
            0.002361912280321121,
            -0.013881144113838673,
            -0.04790087789297104,
            0.001551552675664425,
            -0.001618157490156591,
            0.006438473705202341,
            -0.01933250017464161,
            0.011524355970323086,
            0.02594516985118389,
            -0.023704513907432556,
            0.005311314016580582,
            -0.0016770772635936737,
            -0.024742865934967995,
            -0.005164441652595997,
            0.013116042129695415,
            -0.02262517251074314,
            0.004631602205336094,
            0.019824350252747536,
            0.00814287643879652,
            -0.004973165690898895,
            0.010246907360851765,
            -0.00988485012203455,
            0.0012202359503135085,
            0.009037773124873638,
            -0.00577242486178875,
            -0.02449694089591503,
            -0.006493123713880777,
            -0.013928962871432304,
            0.009270035661756992,
            -0.0058714780025184155,
            -0.0016950092976912856,
            0.0070157162845134735,
            -0.010943697765469551,
            0.004594030324369669,
            -0.009850693866610527,
            -0.00801308173686266,
            0.022980399429798126,
            -0.009386167861521244,
            -0.0002309823757968843,
            0.013642050325870514,
            -0.0221879705786705,
            0.010000982321798801,
            -0.016395052894949913,
            0.001566922990605235,
            -0.019879000261425972,
            -0.03809117153286934,
            -0.012706165201961994,
            0.018813323229551315,
            0.002317508915439248,
            -0.005055141169577837,
            0.0019076326861977577,
            -0.015315711498260498,
            0.0025822208262979984,
            -0.009987319819629192,
            0.01859472133219242,
            0.0011211824603378773,
            0.014755547046661377,
            0.0015558222075924277,
            0.030658748000860214,
            0.019428137689828873,
            0.006547774188220501,
            -0.017665669322013855,
            -0.011742956005036831,
            -0.014304683543741703,
            -0.015206411480903625,
            -0.03273545578122139,
            -0.0071181850507855415,
            -0.03317265585064888,
            0.04918515682220459,
            -0.00222016335465014,
            -0.019660400226712227,
            -0.0016155957709997892,
            -0.01414073258638382,
            -0.030221546068787575,
            -0.011421886272728443,
            -0.008771353401243687,
            0.022857435047626495,
            0.012036700733006,
            0.02781693823635578,
            0.01045867707580328,
            0.02017957717180252,
            -0.008826003409922123,
            0.008163370192050934,
            -0.010731928050518036,
            -0.01132624875754118,
            0.006271107587963343,
            -0.00504489429295063,
            -0.006014934740960598,
            0.004358351230621338,
            -0.028664017096161842,
            -0.002032303484156728,
            0.0155069874599576,
            -0.0175836943089962,
            -0.006349666975438595,
            0.016217438504099846,
            -0.013088717125356197,
            -0.008259007707238197,
            0.017214804887771606,
            -0.01058847177773714,
            -0.0115448497235775,
            0.006233535706996918,
            0.007029378786683083,
            -0.0115448497235775,
            0.004423248581588268,
            0.002957940800115466,
            -0.025057105347514153,
            0.0007245417800731957,
            -0.013095548376441002,
            0.008607402443885803,
            0.005601643119007349,
            -9.211542783305049e-05,
            0.013218510895967484,
            -0.005243001040071249,
            -0.0011946186423301697,
            -0.01253538392484188,
            0.05079733580350876,
            0.002647117944434285,
            0.010246907360851765,
            0.020220564678311348,
            -0.004703330807387829,
            0.004696499556303024,
            -0.0071045225486159325,
            0.009966826066374779,
            0.007282136008143425,
            0.002141603734344244,
            0.028062865138053894,
            -0.025589944794774055,
            -0.01746073178946972,
            -0.012371432967483997,
            -0.0018615216249600053,
            -0.024401303380727768,
            -0.021860070526599884,
            -0.015834888443350792,
            -0.001762468134984374,
            -0.03153315186500549,
            0.01804821938276291,
            0.010861721821129322,
            -0.01491949800401926,
            -0.0008299995097331703,
            -0.007760324981063604,
            0.006435058079659939,
            -0.010363039560616016,
            -0.02959306910634041,
            0.012637852691113949,
            -0.018389783799648285,
            0.0009461311274208128,
            0.0019161717500537634,
            -0.007418761029839516,
            -0.011428717523813248,
            0.012487565167248249,
            0.009693575091660023,
            -0.022010358050465584,
            0.00306382542476058,
            0.2013312429189682,
            -0.026027144864201546,
            -0.005676786880940199,
            0.012385095469653606,
            -0.00025275704683735967,
            -0.020370852202177048,
            0.04128820821642876,
            0.0023516654036939144,
            -0.004604277200996876,
            0.013737687841057777,
            -0.007685180753469467,
            0.004529133439064026,
            -0.024469615891575813,
            -0.0006989245302975178,
            0.0065170335583388805,
            -0.024114388972520828,
            -0.04820145294070244,
            -0.03371915966272354,
            -0.01434567105025053,
            0.008914809674024582,
            -0.012733491137623787,
            0.012931597419083118,
            -0.016244765371084213,
            -0.026792248710989952,
            0.020261552184820175,
            -0.002981850178912282,
            0.004013372119516134,
            0.01978336274623871,
            0.0012586618540808558,
            0.011838594451546669,
            -0.0016173035837709904,
            0.005628968123346567,
            -0.0016036410816013813,
            0.0033439076505601406,
            0.0052498322911560535,
            -0.01818484626710415,
            0.00663658045232296,
            -0.0036888867616653442,
            0.010438183322548866,
            0.022816447541117668,
            0.01848542131483555,
            0.0011544849257916212,
            0.028090190142393112,
            -0.03158780187368393,
            0.011920569464564323,
            0.044184666126966476,
            -0.006349666975438595,
            -0.0016249887412413955,
            -0.008552752435207367,
            -0.0019503281218931079,
            -0.022297270596027374,
            0.01147653628140688,
            0.017351429909467697,
            0.03145117685198784,
            0.0017829620046541095,
            0.006920078303664923,
            -0.02612278424203396,
            0.01743340492248535,
            0.0008654367411509156,
            -0.005232754163444042,
            -0.03404705971479416,
            -0.024578915908932686,
            0.001741120358929038,
            0.010718265548348427,
            -0.012350939214229584,
            -0.006762959063053131,
            -0.009352011606097221,
            0.0062301200814545155,
            -0.0026932288892567158,
            -0.014577934518456459,
            -0.006834687665104866,
            -0.004682837054133415,
            -0.023117024451494217,
            0.012781309895217419,
            -0.026450684294104576,
            -0.05205428972840309,
            0.04033182933926582,
            0.033254630863666534,
            -0.008190695196390152,
            0.03131455183029175,
            -0.004798968322575092,
            -0.023253649473190308,
            0.0016975710168480873,
            -0.026764923706650734,
            -0.006383823696523905,
            -0.02885529212653637,
            0.018266821280121803,
            -0.020999329164624214,
            -0.020411839708685875,
            -0.007022547535598278,
            -0.007869625464081764,
            -0.018239496275782585,
            0.0033661092165857553,
            0.01132624875754118,
            0.002440471900627017,
            0.009597936645150185,
            0.020001964643597603,
            -0.005007322411984205,
            0.004337857477366924,
            -0.016900567337870598,
            -0.021422868594527245,
            0.05268276855349541,
            0.02841809019446373,
            -0.00823168270289898,
            0.01196155697107315,
            0.016559002920985222,
            -0.017679331824183464,
            -0.002886212430894375,
            0.0035693396348506212,
            -0.016039825975894928,
            -0.01543867401778698,
            -0.034675534814596176,
            0.01759735681116581,
            0.007329954765737057,
            -0.006862012669444084,
            0.01030838955193758,
            -0.0034070967230945826,
            -0.0061959633603692055,
            0.027898915112018585,
            0.0005285696825012565,
            -8.635154517833143e-05,
            -0.015903200954198837,
            0.0047750589437782764,
            0.0025736817624419928,
            0.005485511384904385,
            -0.02410072647035122,
            -0.014168057590723038,
            0.0013303902233019471,
            -0.036998167634010315,
            0.000961501500569284,
            0.00674588093534112,
            -0.036998167634010315,
            0.00573826814070344,
            0.012794972397387028,
            -0.004734071437269449,
            -0.004949256312102079,
            0.010547483339905739,
            -0.0025122002698481083,
            -0.009215385653078556,
            -0.006547774188220501,
            -0.0036888867616653442,
            0.011285261251032352,
            -0.008853328414261341,
            -0.005365964025259018,
            0.01800723187625408,
            -0.0040543596260249615,
            0.014113407582044601,
            0.01991998963057995,
            -0.01284279115498066,
            -0.00771933700889349,
            -0.019510112702846527,
            -0.0015686308033764362,
            0.004559874068945646,
            -0.0018137026345357299,
            0.019113898277282715,
            0.02032986469566822,
            -0.0248248428106308,
            0.013109210878610611,
            0.006226704455912113,
            0.024128051474690437,
            -0.04298236221075058,
            0.017050854861736298,
            0.01743340492248535,
            -0.012296289205551147,
            -0.011230611242353916,
            -0.01776130683720112,
            -0.17575496435165405,
            -0.005991025362163782,
            0.003934812732040882,
            -0.05298334360122681,
            0.02783060073852539,
            0.0221879705786705,
            0.019236860796809196,
            -0.013573736883699894,
            -0.032407552003860474,
            -0.0009435694082640111,
            0.008375138975679874,
            -0.008976290933787823,
            -0.017652006819844246,
            -0.009194891899824142,
            -0.015629950910806656,
            0.013034067116677761,
            -0.005348885897547007,
            0.01989266276359558,
            0.02091735415160656,
            0.0015250814612954855,
            0.037544671446084976,
            -0.026942536234855652,
            -0.01670929044485092,
            -0.017201142385601997,
            0.011777112260460854,
            0.007145510520786047,
            -0.004149997606873512,
            0.014413983561098576,
            -0.012282626703381538,
            -0.022160645574331284,
            -0.010294727049767971,
            -0.014427646063268185,
            0.03544063866138458,
            -0.007493905257433653,
            0.024742865934967995,
            0.015261061489582062,
            0.032653480768203735,
            0.0010340837761759758,
            -0.03401973471045494,
            0.002466089092195034,
            0.015793900936841965,
            0.036123763769865036,
            0.02105397917330265,
            0.002672735136002302,
            -0.011203286238014698,
            0.014195382595062256,
            0.016326740384101868,
            -0.03462088480591774,
            0.016258427873253822,
            -0.01573925092816353,
            0.027338750660419464,
            -0.02508443035185337,
            0.0233902744948864,
            -0.00411584135144949,
            0.010861721821129322,
            0.025125417858362198,
            0.0027444635052233934,
            0.004932178184390068,
            -0.010697771795094013,
            -0.019113898277282715,
            -0.012378264218568802,
            -0.011517524719238281,
            0.0038528372533619404,
            -0.009386167861521244,
            -0.015534312464296818,
            -0.028199490159749985,
            -0.013375630602240562,
            0.010950529016554356,
            -0.02681957371532917,
            0.007657855749130249,
            -0.020958341658115387,
            -0.0034634547773748636,
            -0.011162297800183296,
            -0.015889538452029228,
            0.031068624928593636,
            0.023117024451494217,
            0.0005482095875777304,
            0.015124435536563396,
            0.027297763153910637,
            1.6437747035524808e-05,
            -0.008545921184122562,
            0.01916854828596115,
            -0.0032619324047118425,
            0.024852167814970016,
            -0.016627315431833267,
            -0.008101888000965118,
            0.008443452417850494,
            0.014577934518456459,
            -0.028773317113518715,
            -0.010656784288585186,
            0.02522105537354946,
            -0.02090369164943695,
            -0.008040406741201878,
            -0.01508344803005457,
            -0.015302048996090889,
            0.004727240186184645,
            0.00987118761986494,
            0.00013353001850191504,
            -0.0030450394842773676,
            -0.0009785797446966171,
            0.013573736883699894,
            0.00886699091643095,
            -0.0013431988190859556,
            -0.011879581958055496,
            0.037544671446084976,
            0.0018137026345357299,
            -0.030604097992181778,
            0.011134972795844078,
            0.03874697536230087,
            -0.019564762711524963,
            -0.02855471707880497,
            -0.008306826464831829,
            -0.008033575490117073,
            -0.0007518668426200747,
            -0.008771353401243687,
            0.015684600919485092,
            -0.0137718440964818,
            -0.019414475187659264,
            0.016982542350888252,
            0.001952035934664309,
            0.03718944266438484,
            0.00609691021963954,
            -0.001953743863850832,
            0.005277157295495272,
            0.00015156884910538793,
            -0.011196454986929893,
            -0.11706067621707916,
            -0.021805420517921448,
            0.0030843191780149937,
            0.015179086476564407,
            -0.012214314192533493,
            0.0316971018910408,
            -0.00508929742500186,
            0.04546894505620003,
            -0.029046567156910896,
            0.04560557007789612,
            -0.01434567105025053,
            -0.008853328414261341,
            0.007145510520786047,
            -0.0026915210764855146,
            0.014332008548080921,
            -0.00014996776008047163,
            -0.0023875294718891382,
            -0.01815752126276493,
            -0.03587783873081207,
            0.023540562018752098,
            0.019441800191998482,
            -0.0012697626370936632,
            0.007657855749130249,
            -0.03044014796614647,
            -0.0058475686237216,
            -0.01299307867884636,
            -0.03937545046210289,
            0.02798089012503624,
            0.023567887023091316,
            0.0017385586397722363,
            0.009030940942466259,
            -0.028664017096161842,
            0.005666540004312992,
            -0.011339911259710789,
            0.010076126083731651,
            -0.01024007610976696,
            0.010889047756791115,
            -0.014195382595062256,
            0.01802089437842369,
            -0.012986247427761555,
            -0.002857179380953312,
            0.0079242754727602,
            0.013676206581294537,
            -0.010226413607597351,
            -0.007125016767531633,
            -0.02319899946451187,
            -0.008532258681952953,
            0.011674643494188786,
            0.007370942272245884,
            -0.0155069874599576,
            -0.03951207548379898,
            -0.02064410410821438,
            -0.04453989118337631,
            0.013525918126106262,
            0.01875867322087288,
            0.0036205740179866552,
            0.018130196258425713,
            0.016121800988912582,
            -0.013560074381530285,
            -0.00030975547269918025,
            -0.011380898766219616,
            -0.004508639220148325,
            -0.027174798771739006,
            -0.00344296102412045,
            0.01833513379096985,
            -0.0027308010030537844,
            6.217524787643924e-05,
            -0.02494780533015728,
            0.009816537611186504,
            -0.00959110539406538,
            0.006933740805834532,
            0.022283608093857765,
            -0.005854399874806404,
            0.007610036991536617,
            -0.01916854828596115,
            0.0009085591300390661,
            -0.013355136848986149,
            -0.03500343859195709,
            -0.0031270147301256657,
            0.0089421346783638,
            -0.010513327084481716,
            -0.01325949840247631,
            0.007097691297531128,
            -0.0041534132324159145,
            6.009384378558025e-05,
            -0.0029237843118608,
            -0.02020690217614174,
            0.0007377773872576654,
            0.0009128287201747298,
            -0.014413983561098576,
            0.012631021440029144,
            0.028636692091822624,
            0.004518886562436819,
            -0.01414073258638382,
            0.0012347523588687181,
            0.010725096799433231,
            -0.019414475187659264,
            -0.005748515482991934,
            0.028472740203142166,
            0.003120183479040861,
            -0.02724311128258705,
            0.009468142874538898,
            -0.055879805237054825,
            0.042927712202072144,
            0.0014055342180654407,
            -0.010759253054857254,
            -0.0011988881742581725,
            -0.02031620219349861,
            0.01613546349108219,
            -0.020083939656615257,
            0.005410367157310247,
            0.008340982720255852,
            -0.01053382083773613,
            0.016340402886271477,
            -0.01434567105025053,
            -0.0005213114200159907,
            -0.021586818620562553,
            -0.005140532273799181,
            0.018116533756256104,
            0.009679912589490414,
            0.007603205740451813,
            -0.0014644538750872016,
            0.007603205740451813,
            -0.0177339818328619,
            0.022966735064983368,
            0.017925256863236427,
            0.010889047756791115,
            -0.01508344803005457,
            -0.02583586983382702,
            0.015247398987412453,
            -0.009167566895484924,
            -0.010772915557026863,
            -0.01586221344769001,
            -0.01161999348551035,
            -0.006800530944019556,
            0.0078013124875724316,
            -0.019414475187659264,
            -0.010608965530991554,
            0.024633565917611122,
            0.032653480768203735,
            0.026505334302783012,
            0.04626137390732765,
            -0.021231593564152718,
            -0.04254516214132309,
            0.010062463581562042,
            -0.02046648971736431,
            -0.0089421346783638,
            -0.023103361949324608,
            -0.007876456715166569,
            -0.000488435965962708,
            0.02090369164943695,
            0.003811849746853113,
            0.037134792655706406,
            0.015479662455618382,
            -0.029155869036912918,
            -0.03016689606010914,
            -0.008238513953983784,
            0.013778675347566605,
            0.005266910418868065,
            -0.00022436458675656468,
            -0.0013218511594459414,
            0.025207392871379852,
            0.029620394110679626,
            0.005741683766245842,
            0.007077197544276714,
            -0.004891190677881241,
            -0.0018478590063750744,
            -0.006817609537392855,
            -0.007125016767531633,
            0.004573536571115255,
            0.0034258828964084387,
            0.008545921184122562,
            -0.006499954964965582,
            0.011859088204801083,
            0.00382209662348032,
            -0.001369670033454895,
            0.012385095469653606,
            -0.021231593564152718,
            0.016340402886271477,
            0.016477027907967567,
            -0.01804821938276291,
            0.006503370590507984,
            0.008026744239032269,
            -0.016968879848718643,
            0.01081390306353569,
            0.002855471568182111,
            0.023882126435637474,
            0.006042259745299816,
            -0.015957850962877274,
            0.0022594432812184095,
            -0.008921640925109386,
            -0.005417198408395052,
            0.0008667176007293165,
            -0.0036000802647322416,
            -0.011893244460225105,
            0.01670929044485092,
            0.021996695548295975,
            -0.009693575091660023,
            0.00428662309423089,
            0.011988881975412369,
            0.017091842368245125,
            -0.0033251214772462845,
            0.010199088603258133,
            -0.01978336274623871,
            0.007575880270451307,
            -0.010151269845664501,
            -0.033254630863666534,
            0.005994440987706184,
            -0.03153315186500549,
            -0.010410858318209648,
            0.03000294603407383,
            0.014837522059679031,
            0.0156982634216547,
            -0.0036957180127501488,
            0.006708309054374695,
            0.00843662116676569,
            -0.01399044506251812,
            0.020111264660954475,
            -0.010281064547598362,
            -0.0038050184957683086,
            -0.032407552003860474,
            0.039867304265499115,
            0.044922444969415665,
            0.017474394291639328,
            0.040741704404354095,
            -0.016791265457868576,
            0.005994440987706184,
            0.0014337131287902594,
            0.024319328367710114,
            -0.028008215129375458,
            0.014058757573366165,
            -0.01089587900787592,
            -0.013314148411154747,
            -0.008224851451814175,
            -0.012972584925591946,
            0.005082466173917055,
            -0.006113988347351551,
            1.228828386956593e-05,
            -0.012289457954466343,
            0.02205134555697441,
            -0.0017044023843482137,
            0.05563387647271156,
            0.016490690410137177,
            0.0022696901578456163,
            0.02665562368929386,
            -0.014618922024965286,
            0.015302048996090889,
            0.00849810242652893,
            0.025740232318639755,
            -0.00522933853790164,
            -0.017488056793808937,
            -0.0015566761139780283,
            -0.004143166355788708,
            -0.005755346734076738,
            -0.028336115181446075,
            -0.013006741181015968,
            -0.00310310535132885,
            -0.007562217768281698,
            0.013621555641293526,
            -0.0073777735233306885,
            -0.007917444221675396,
            0.02378648892045021,
            0.005051725544035435,
            0.023089699447155,
            0.007507567759603262,
            -0.0051473635248839855,
            0.0007343617035076022,
            0.028308790177106857,
            0.001484947744756937,
            0.0008744028164073825,
            -0.006810777820646763,
            0.03429298475384712,
            0.01873134821653366,
            -0.0335005559027195,
            -0.017952581867575645,
            -0.001010174280963838,
            -0.0016497521428391337,
            0.014045095071196556,
            -0.04008590430021286,
            -0.005161026027053595,
            -0.009638924151659012,
            -0.006711724679917097,
            0.008894315920770168,
            -0.0020562128629535437,
            -0.02263883501291275,
            0.0010853182757273316,
            0.00042780840885825455,
            -0.016326740384101868,
            -0.004112425725907087,
            -0.015821225941181183
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "47b22e69-aa7a-458d-a919-c02f71283edd",
      "type": "next",
      "source": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "1656ac77-b784-47ff-99aa-6766d4e06e55",
      "type": "next",
      "source": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "d5ac6e88-bbcf-4232-8532-8c671fc74c19",
      "type": "next",
      "source": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {}
    },
    {
      "id": "4734226f-f679-4627-9805-d65c7506d398",
      "type": "cosine_similarity",
      "source": {
        "id": "b5e0eb8c-c851-4727-89fa-b01f23f25615",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nThings we learned about LLMs in 2024\n\n31st December 2024\n\nA lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments.\n\nThis is a sequel to my review of 2023.\n\nIn this article:\n\nThe GPT-4 barrier was comprehensively broken\n\nSome of those GPT-4 models run on my laptop\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nMultimodal vision is common, audio and video are starting to emerge\n\nVoice and live camera mode are science fiction come to life\n\nPrompt driven app generation is a commodity already\n\nUniversal access to the best models lasted for just a few short months\n\n“Agents” still haven’t really happened yet\n\nEvals really matter\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nThe rise of inference-scaling “reasoning” models\n\nWas the best currently available LLM trained in China for less than $6m?\n\nThe environmental impact got better\n\nThe environmental impact got much, much worse\n\nThe year of slop\n\nSynthetic training data works great\n\nLLMs somehow got even harder to use\n\nKnowledge is incredibly unevenly distributed\n\nLLMs need better criticism\n\nEverything tagged “llms” on my blog in 2024\n\nThe GPT-4 barrier was comprehensively broken\n\nIn my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t?\n\nI’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total.\n\nThe earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video.\n\nI wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May.\n\nGemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million.\n\nLonger inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern.\n\nGetting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6).\n\nThen there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent.\n\nTraining a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list.\n\nSome of those GPT-4 models run on my laptop\n\nMy personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment).\n\nThat same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that:\n\nQwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model!\n\nI can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December)\n\nThis remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs.\n\nThese models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else.\n\nThe fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come.\n\nMeta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second:\n\nHere’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now!\n\nLLM prices crashed, thanks to competition and increased efficiency\n\nThe past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs.\n\nIn December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo.\n\nToday $30/mTok gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable.\n\nOther model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year.\n\nI’ve been tracking these pricing changes under my llm-pricing tag.\n\nThese price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts.\n\nThere’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible.\n\nHere’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model?\n\nEach photo would need 260 input tokens and around 100 output tokens.\n\n260 * 68,000 = 17,680,000 input tokens\n\n17,680,000 * $0.0375/million = $0.66\n\n100 * 68,000 = 6,800,000 output tokens\n\n6,800,000 * $0.15/million = $1.02\n\nThat’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right.\n\nHow good are those descriptions? Here’s what I got from this command:\n\nAgainst this photo of butterflies at the California Academy of Sciences:\n\nA shallow dish, likely a hummingbird or butterfly feeder, is red.  Pieces of orange slices of fruit are visible inside the dish.\n\nTwo butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings.  The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit.\n\n260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent).\n\nThis increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting.\n\nMultimodal vision is common, audio and video are starting to emerge\n\nMy butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs.\n\nA year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window.\n\nIn 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova.\n\nIn October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models.\n\nI think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models.\n\nVoice and live camera mode are science fiction come to life\n\nThe audio and live video modes that have started to emerge deserve a special mention.\n\nThe ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text.\n\nThe May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models.\n\nThe demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product.\n\nThe delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet.\n\nWhen ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs.\n\nEven more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish.\n\nYour browser does not support the audio element.\n\nOpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025.\n\nGoogle’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans:\n\nThe most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did.\n\nThese abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should.\n\nBoth Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now!\n\nPrompt driven app generation is a commodity already\n\nThis was possible with GPT-4 in 2023, but the value it provides became evident in 2024.\n\nWe already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt.\n\nAnthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet.\n\nWith Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface.\n\nHere’s my Extract URLs app, entirely generated by Claude:\n\nI’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period.\n\nSince then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November.\n\nSteve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second.\n\nThen in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models.\n\nI’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv.\n\nThis prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025.\n\nUniversal access to the best models lasted for just a few short months\n\nFor a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world.\n\nOpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do.\n\nThat era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro.\n\nSince the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return.\n\n“Agents” still haven’t really happened yet\n\nI find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that.\n\nIf you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about.\n\nThe two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition.\n\n(I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.)\n\nWhatever the term may mean, agents still have that feeling of perpetually “coming soon”.\n\nTerminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction?\n\nJust the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki.\n\nPrompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022.\n\nI’m beginning to see the most popular idea of “agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed.\n\nEvals really matter\n\nAnthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character):\n\nThe boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them.\n\nIt’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition.\n\nVercel’s Malte Ubl:\n\nWhen @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity.\n\nWe completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual\n\nI’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like.\n\nApple Intelligence is bad, Apple’s MLX library is excellent\n\nAs a Mac user I’ve been feeling a lot better about my choice of platform this year.\n\nLast year it felt like my lack of a Linux/Windows  machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models.\n\nOn paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms.\n\nThe llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic.\n\nApple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format.\n\nPrince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ.\n\nWhile MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features.\n\nNow that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though.\n\nThe rise of inference-scaling “reasoning” models\n\nThe most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th.\n\nOne way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners.\n\nThis is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise.\n\no1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result.\n\nThe biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference.\n\nThe sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense!\n\no3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems.\n\nOpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th.\n\nAlibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally.\n\nDeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th.\n\nTo understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor.\n\nNothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December.\n\nWas the best currently available LLM trained in China for less than $6m?\n\nNot quite, but almost! It does make for a great attention-grabbing headline.\n\nThe big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the day after that.\n\nDeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B.\n\nBenchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model.\n\nThe really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse.\n\nThose US export regulations on GPUs to China seem to have inspired some very effective training optimizations!\n\nThe environmental impact got better\n\nA welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years.\n\nOpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss.\n\nI think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube.\n\nLikewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop.\n\nFor less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost.\n\nThe environmental impact got much, much worse\n\nThe much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future.\n\nCompanies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades.\n\nIs this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time?\n\nAn interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes!\n\nThe resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage.\n\nThe year of slop\n\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:\n\nWatching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons.\n\nI love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI!\n\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\n\nSynthetic training data works great\n\nAn idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.\n\nThe idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\n\nThat’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way.\n\nOne of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this:\n\nSynthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.\n\nStructured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.\n\nAnother common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\n\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\n\nLLMs somehow got even harder to use\n\nA drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\n\nIf anything, this problem got worse in 2024.\n\nWe’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set.\n\nThe number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\n\nDid you know ChatGPT has two entirely different ways of running Python now?\n\nWant to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first.\n\nThe models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\n\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.\n\nMeanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.\n\nThere’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\n\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\n\nKnowledge is incredibly unevenly distributed\n\nMost people have heard of ChatGPT by now. How many have heard of Claude?\n\nThe knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast.\n\nThe pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet.\n\nGiven the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this.\n\nLLMs need better criticism\n\nA lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight.\n\nI get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs.\n\nLLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative.\n\nI like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue.\n\nIf we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps.\n\n(If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!)\n\nI think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and needs guidance.\n\nThose of us who understand this stuff have a duty to help everyone else figure it out.\n\nEverything tagged “llms” on my blog in 2024\n\nBecause I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms:\n\nJanuary\n\n7th: It’s OK to call it Artificial Intelligence\n\n9th: What I should have said about the term Artificial Intelligence\n\n17th: Talking about Open Source LLMs on Oxide and Friends\n\n26th: LLM 0.13: The annotated release notes\n\nFebruary\n\n21st: The killer app of Gemini Pro 1.5 is video\n\nMarch\n\n5th: Prompt injection and jailbreaking are not the same thing\n\n8th: The GPT-4 barrier has finally been broken\n\n22nd: Claude and ChatGPT for ad-hoc sidequests\n\n23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter\n\n26th: llm cmd undo last git commit—a new plugin for LLM\n\nApril\n\n8th: Building files-to-prompt entirely using Claude 3 Opus\n\n10th: Three major LLM releases in 24 hours (plus weeknotes)\n\n17th: AI for Data Journalism: demonstrating what we can do with this stuff right now\n\n22nd: Options for accessing Llama 3 from the terminal using LLM\n\nMay\n\n8th: Slop is the new name for unwanted AI-generated content\n\n15th: ChatGPT in “4o” mode is not running the new features yet\n\n29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say\n\nJune\n\n6th: Accidental prompt injection against RAG applications\n\n10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence\n\n17th: Language models on the command-line\n\n21st: Building search-based RAG using Claude, Datasette and Val Town\n\n27th: Open challenges for AI engineering\n\nJuly\n\n14th: Imitation Intelligence, my keynote for PyCon US 2024\n\n19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment\n\nAugust\n\n6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs\n\n8th: django-http-debug, a new Django app mostly written by Claude\n\n23rd: Claude’s API now supports CORS requests, enabling client-side applications\n\n26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images\n\nSeptember\n\n6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes\n\n10th: Notes from my appearance on the Software Misadventures Podcast\n\n12th: Notes on OpenAI’s new o1 chain-of-thought models\n\n20th: Notes on using LLMs for code\n\n29th: NotebookLM’s automatically generated podcasts are surprisingly effective\n\n30th: Weeknotes: Three podcasts, two trips and a new plugin system\n\nOctober\n\n1st: OpenAI DevDay 2024 live blog\n\n2nd: OpenAI DevDay: Let’s build developer tools, not digital God\n\n15th: ChatGPT will happily write you a thinly disguised horoscope\n\n17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent\n\n18th: Experimenting with audio input and output for the OpenAI Chat Completion API\n\n19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs\n\n21st: Everything I built with Claude Artifacts this week\n\n22nd: Initial explorations of Anthropic’s new Computer Use capability\n\n24th: Notes on the new Claude analysis JavaScript code execution tool\n\n27th: Run a prompt to generate and execute jq programs using llm-jq\n\n29th: You can now run prompts against images, audio and video in your terminal using LLM\n\n30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶  Monthnotes for October\n\nNovember\n\n4th: Claude 3.5 Haiku\n\n7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5\n\n12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac\n\n19th: Notes from Bing Chat—Our First Encounter With Manipulative AI\n\n25th: Ask questions of SQLite databases and CSV/JSON files in your terminal\n\nDecember\n\n4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin)\n\n7th: Prompts.js\n\n9th: I can now run a GPT-4 class model on my laptop\n\n10th: ChatGPT Canvas can make API requests now, but it’s complicated\n\n11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode\n\n19th: Building Python tools with a one-shot prompt using uv run and Claude Projects\n\n19th: Gemini 2.0 Flash “Thinking mode”\n\n20th: December in LLMs has been a lot\n\n20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3”\n\n24th: Trying out QvQ—Qwen’s new visual reasoning model\n\n31st: Things we learned about LLMs in 2024\n\n(This list generated using Django SQL Dashboard with a SQL query written for me by Claude.)\n\nPosted \n\n31st December 2024 at 6:07 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\ngoogle\n            346\n\nai\n            1089\n\nopenai\n            253\n\ngenerative-ai\n            934\n\nllms\n            922\n\nanthropic\n            114\n\ngemini\n            55\n\nmeta\n            26\n\ninference-scaling\n            28\n\nlong-context\n            10\n\nNext: Ending a year long posting streak\n\nPrevious: Trying out QvQ - Qwen's new visual reasoning model\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2024_llms.html"
          },
          "headlines": [
            "The GPT-4 barrier was comprehensively broken",
            "Some of those GPT-4 models run on my laptop",
            "LLM prices crashed, thanks to competition and increased efficiency",
            "Multimodal vision is common, audio and video are starting to emerge",
            "Prompt driven app generation is a commodity already"
          ],
          "summary": "In 2024, significant advancements were made in the field of Large Language Models (LLMs), with the GPT-4 barrier being broken by multiple organizations. LLM prices dropped due to increased competition and efficiency, and multimodal capabilities became more common, allowing for audio and video inputs. Prompt-driven app generation became a commodity, and the environmental impact of LLMs improved due to efficiency gains. However, the infrastructure buildout for AI models raised environmental concerns. The term 'slop' emerged to describe unwanted AI-generated content, and synthetic training data proved effective. Despite advancements, LLMs became harder to use, and knowledge about them remained unevenly distributed. Better criticism and understanding of LLMs are needed to harness their potential responsibly.",
          "summary_embedding": [
            -0.014540364034473896,
            0.0007438954198732972,
            0.005369548685848713,
            -0.02401237189769745,
            -0.003787418594583869,
            0.017005302011966705,
            -0.005504566244781017,
            0.010136710479855537,
            -0.03810960054397583,
            -0.04378726705908775,
            0.029606949537992477,
            0.027322035282850266,
            -0.01417339313775301,
            -0.007976428605616093,
            0.005265688989311457,
            0.017213020473718643,
            0.021533586084842682,
            0.01747613213956356,
            0.013965672813355923,
            -0.004649454262107611,
            -0.014969650655984879,
            -0.005141057074069977,
            -0.014415732584893703,
            -0.02142280340194702,
            0.004763700067996979,
            0.0012904573231935501,
            0.02474631369113922,
            -0.04162698611617088,
            -0.01675603911280632,
            0.019317911937832832,
            0.011410723440349102,
            0.00557380635291338,
            -0.04154389724135399,
            0.00294788577593863,
            -0.017019148916006088,
            -0.009001177735626698,
            0.013342514634132385,
            -0.00898732990026474,
            0.015260457992553711,
            -0.022101353853940964,
            0.00837801955640316,
            0.018376249819993973,
            0.017628459259867668,
            -0.02135356329381466,
            -0.02052268572151661,
            0.017545372247695923,
            0.01775309257209301,
            -0.004822554066777229,
            -0.027502059936523438,
            0.009264289401471615,
            0.014609603211283684,
            0.035035353153944016,
            -0.009409692138433456,
            -0.01184693444520235,
            -0.007747936528176069,
            0.016077488660812378,
            -0.005400706548243761,
            0.011902326717972755,
            0.01841779425740242,
            -0.0061796545051038265,
            -0.0005028542364016175,
            0.004237477667629719,
            -0.01563435234129429,
            0.035589270293712616,
            -0.018736297264695168,
            -0.009063493460416794,
            -0.02701738104224205,
            0.02253063954412937,
            0.02163052186369896,
            -0.016936061903834343,
            0.002709008287638426,
            0.011071448214352131,
            0.014415732584893703,
            -0.015828223899006844,
            0.010967588983476162,
            0.004538670647889376,
            -0.005040659569203854,
            -0.011293016374111176,
            -0.03273659199476242,
            0.015163522213697433,
            0.01042751781642437,
            -0.008177223615348339,
            0.005459560547024012,
            0.02186593785881996,
            0.01730995625257492,
            0.01172922644764185,
            0.01231776550412178,
            -0.012047730386257172,
            -0.015038890764117241,
            -0.013716409914195538,
            -0.005975397303700447,
            0.0022537563927471638,
            0.017171477898955345,
            0.008544194512069225,
            -0.0064912340603768826,
            -0.0017413817113265395,
            0.003489687340334058,
            0.012303917668759823,
            -0.007664849050343037,
            -0.029136119410395622,
            -0.004846788011491299,
            -0.0055807302705943584,
            -0.037915728986263275,
            -0.007810252718627453,
            -0.02085503563284874,
            0.000979743548668921,
            0.01120300404727459,
            0.013557158410549164,
            0.009312757290899754,
            0.011293016374111176,
            -0.01658986322581768,
            0.017573067918419838,
            0.011556127108633518,
            -0.020993515849113464,
            -0.008551118895411491,
            -0.009319680742919445,
            -0.0009304101695306599,
            -0.020051853731274605,
            0.007526369299739599,
            0.004961033817380667,
            0.034813784062862396,
            0.004954109899699688,
            0.02185208909213543,
            0.0022537563927471638,
            0.013321742415428162,
            0.0008992522489279509,
            -0.0032663887832313776,
            -0.002058153972029686,
            -0.013564081862568855,
            0.004736004397273064,
            -0.005411092657595873,
            0.0060654086992144585,
            0.01201311033219099,
            0.004677150398492813,
            -0.030382435768842697,
            0.027322035282850266,
            -0.014471123926341534,
            -0.005947701167315245,
            -0.013709485530853271,
            -0.007858720608055592,
            0.001367486547678709,
            0.02258603274822235,
            -0.006678181234747171,
            0.014554211869835854,
            -0.004725618287920952,
            0.028776071965694427,
            0.004292869474738836,
            0.004767162259668112,
            0.005258765071630478,
            0.006896286737173796,
            0.0030188565142452717,
            -0.029219206422567368,
            -0.01686682179570198,
            0.020951971411705017,
            0.016506774351000786,
            0.004448658786714077,
            0.015329698100686073,
            0.03154566511511803,
            -0.031711842864751816,
            -0.014235708862543106,
            -0.013764877803623676,
            0.004497126676142216,
            -0.0020927737932652235,
            0.011860782280564308,
            0.014941954985260963,
            0.024261634796857834,
            0.025258690118789673,
            0.00151462119538337,
            -0.0046598403714597225,
            -0.010946816764771938,
            -0.010939892381429672,
            0.024372419342398643,
            -0.04076841101050377,
            0.010926044546067715,
            -0.016008248552680016,
            0.01132071204483509,
            0.0229045357555151,
            -0.007678696885704994,
            -0.008564966730773449,
            -0.03317972645163536,
            0.007512521464377642,
            0.013204035349190235,
            -0.003808190580457449,
            0.0019664110150188208,
            -0.0067958892323076725,
            -0.005321080796420574,
            0.017656156793236732,
            0.00022589493892155588,
            0.0013345977058634162,
            -0.013993369415402412,
            0.014789626933634281,
            0.006671257317066193,
            0.0022866453509777784,
            -0.025757215917110443,
            -0.6518514156341553,
            -0.019816439598798752,
            0.015218914486467838,
            -0.010385974310338497,
            0.0029773125424981117,
            0.003157336264848709,
            -0.005514952354133129,
            0.006473924033343792,
            -0.007131702266633511,
            0.03772185742855072,
            -0.017462285235524178,
            0.005511490162461996,
            -0.0173653494566679,
            -0.004805244039744139,
            0.010829108767211437,
            -0.007187094073742628,
            4.5249307731864974e-05,
            -0.0248017068952322,
            0.00014269896200858057,
            0.020771948620676994,
            -0.010171330533921719,
            0.017794635146856308,
            -0.014131848700344563,
            -0.0017431126907467842,
            -0.031767234206199646,
            9.926178609021008e-05,
            0.006269666366279125,
            0.013744105584919453,
            -0.0068374332040548325,
            0.00859958678483963,
            -0.00831570290029049,
            0.008281083777546883,
            -0.013162490911781788,
            -0.0008624686161056161,
            0.04891101270914078,
            -0.0033442836720496416,
            -0.019497934728860855,
            0.020314965397119522,
            0.0056534321047365665,
            0.012206981889903545,
            -0.038054209202528,
            -0.006089642643928528,
            -0.016769886016845703,
            0.020896580070257187,
            -0.00209450488910079,
            0.03154566511511803,
            0.022835295647382736,
            0.006023865193128586,
            -0.017046846449375153,
            -0.009742043912410736,
            0.00453520892187953,
            0.02351384609937668,
            0.0015076972777023911,
            0.004677150398492813,
            -0.00037908804370090365,
            0.019110191613435745,
            0.02390158921480179,
            -0.011923098005354404,
            0.01042751781642437,
            -0.007086696568876505,
            0.0005305501981638372,
            -0.011653062887489796,
            -0.025037121027708054,
            -0.01520506665110588,
            -0.033041246235370636,
            0.013986445032060146,
            -0.00019116683688480407,
            0.0032819679472595453,
            0.01179154310375452,
            -0.030908659100532532,
            -0.008862697519361973,
            0.038912784308195114,
            0.006657409481704235,
            -0.0033512075897306204,
            -0.00990129541605711,
            0.002290107309818268,
            -0.0004160880926065147,
            -0.014803474768996239,
            0.0009485856280662119,
            0.00638044998049736,
            0.018625514581799507,
            -0.01780848391354084,
            0.011784618720412254,
            -0.009465084411203861,
            0.036863286048173904,
            -0.011853858828544617,
            0.009340452961623669,
            -0.001243720413185656,
            -0.013273275457322598,
            -0.005643045995384455,
            0.013294046744704247,
            0.027446668595075607,
            -0.0162575114518404,
            -0.04375956952571869,
            -0.007844872772693634,
            0.014443428255617619,
            -0.005577268078923225,
            0.020979667082428932,
            0.0110160568729043,
            -0.013003239408135414,
            -0.004081687889993191,
            -0.00756098935380578,
            0.031185617670416832,
            0.028886856511235237,
            0.027003532275557518,
            0.025535648688673973,
            -0.009298908524215221,
            0.012186209671199322,
            0.01248394139111042,
            -0.02613111026585102,
            0.000608877744525671,
            0.01197849027812481,
            0.00284402584657073,
            -0.02434472367167473,
            -0.013681789860129356,
            -0.027100468054413795,
            -0.0005387724377214909,
            -0.006512005813419819,
            -0.0012982467887923121,
            -0.02424778789281845,
            0.015246610157191753,
            0.00241993204690516,
            0.022876838222146034,
            -0.0040886118076741695,
            -0.007464053574949503,
            0.014900410547852516,
            0.008994253352284431,
            -0.002598224440589547,
            -0.009894371032714844,
            0.024677075445652008,
            -0.009478932246565819,
            -0.029302295297384262,
            0.025480257347226143,
            -0.024774011224508286,
            0.029080728068947792,
            0.012906303629279137,
            0.040076013654470444,
            -0.0054941801354289055,
            0.018334707245230675,
            0.0025497565511614084,
            -0.029413077980279922,
            -0.004559442866593599,
            -0.004310179501771927,
            0.006286976393312216,
            -0.04245786368846893,
            -0.0262280460447073,
            -0.011479963548481464,
            0.004296331200748682,
            0.001073217368684709,
            -0.033428989350795746,
            -0.01897171325981617,
            -0.014014140702784061,
            -0.013674866408109665,
            0.02496788278222084,
            -0.002310879295691848,
            -0.006820123177021742,
            -0.015371241606771946,
            -0.027031227946281433,
            -0.014568059705197811,
            -0.03248732537031174,
            0.008156451396644115,
            0.0008096732199192047,
            0.0007932287990115583,
            0.012601648457348347,
            0.015080434270203114,
            -0.02629728615283966,
            0.000766398326959461,
            0.023153798654675484,
            -0.037306420505046844,
            -0.023056862875819206,
            0.003399675479158759,
            -0.041959334164857864,
            0.0031971491407603025,
            0.019456392154097557,
            -0.013681789860129356,
            0.012006185948848724,
            0.004704846069216728,
            0.0017898496007546782,
            0.015066586434841156,
            -0.010434442199766636,
            0.013529462739825249,
            0.026172654703259468,
            0.0126778120175004,
            -0.008343399502336979,
            0.023098407313227654,
            0.02275220677256584,
            0.04785856977105141,
            0.014159544371068478,
            -0.007221714127808809,
            0.01534354593604803,
            0.02196287363767624,
            0.03063170053064823,
            -0.013107099570333958,
            -0.000241690271650441,
            -0.012802444398403168,
            0.021325867623090744,
            -0.002593031618744135,
            -0.0012601648923009634,
            0.016742190346121788,
            0.013529462739825249,
            0.01075986959040165,
            0.028609896078705788,
            0.010912196710705757,
            -0.013771802186965942,
            0.00873806606978178,
            -0.0157451368868351,
            0.002091042697429657,
            -0.022392161190509796,
            -0.001155439647845924,
            0.018293162807822227,
            0.008710370399057865,
            0.0012558373855426908,
            -0.004178623668849468,
            -0.029413077980279922,
            -0.0009269482106901705,
            0.03334590047597885,
            0.0025687976740300655,
            0.013806421309709549,
            -0.02268296852707863,
            0.004098997917026281,
            -0.004767162259668112,
            0.0070590004324913025,
            0.018445489928126335,
            -0.008398790843784809,
            -0.014180316589772701,
            0.008571891114115715,
            -0.02395698055624962,
            0.02279375120997429,
            0.014291100203990936,
            -0.029357686638832092,
            0.0034065996296703815,
            0.017102237790822983,
            0.025424864143133163,
            0.0204119011759758,
            0.00998438335955143,
            0.004732542205601931,
            0.02019033394753933,
            -0.0009529131348244846,
            0.036863286048173904,
            -0.003777032718062401,
            -0.011563051491975784,
            0.0060723330825567245,
            0.004756776150316,
            -0.0062246606685221195,
            0.024926338344812393,
            -0.017393045127391815,
            0.035977013409137726,
            -0.00837801955640316,
            -0.0234169103205204,
            -0.004237477667629719,
            -0.024760162457823753,
            0.0067093390971422195,
            -0.01819622702896595,
            -0.0009269482106901705,
            0.008717294782400131,
            -0.019262520596385002,
            0.015038890764117241,
            0.0066816434264183044,
            0.020868884399533272,
            0.013494842685759068,
            -0.009257365018129349,
            0.0061831166967749596,
            0.01109914481639862,
            -0.010448290035128593,
            0.006235046312212944,
            -0.007581761106848717,
            -0.003593547036871314,
            -0.0045906007289886475,
            -0.0037527987733483315,
            -0.011957718059420586,
            -0.011881554499268532,
            -0.021602826192975044,
            0.010856805369257927,
            -0.038248080760240555,
            0.015094282105565071,
            0.006003092974424362,
            0.005639583803713322,
            0.008170299232006073,
            0.013501766137778759,
            0.02496788278222084,
            -0.004687536507844925,
            -0.04331643506884575,
            0.003326973645016551,
            -0.008481878787279129,
            -0.011313787661492825,
            -0.035256918519735336,
            -0.04425809904932976,
            0.014471123926341534,
            -0.007796404417604208,
            -0.013508690521121025,
            -0.0009598371107131243,
            0.008364170789718628,
            -0.011230699717998505,
            0.013661017641425133,
            -0.00041911733569577336,
            0.014664995484054089,
            0.026186503469944,
            -0.013820270076394081,
            -0.013100175186991692,
            -0.0215197391808033,
            0.0028942248318344355,
            0.011999262496829033,
            -0.004486741032451391,
            -0.027529755607247353,
            0.03960518166422844,
            -0.016797581687569618,
            -0.03185031935572624,
            0.011327635496854782,
            -0.0071732462383806705,
            -0.019096344709396362,
            -0.0036281670909374952,
            0.004684074316173792,
            0.0029236518312245607,
            0.007090158294886351,
            0.01581437699496746,
            0.021325867623090744,
            0.0047706239856779575,
            0.0012393929064273834,
            0.014706538990139961,
            0.019317911937832832,
            0.0014124924782663584,
            -0.03101944364607334,
            0.004974881652742624,
            0.020979667082428932,
            0.06973835825920105,
            0.02063346840441227,
            -0.012269297614693642,
            0.017213020473718643,
            -0.018099291250109673,
            -0.00990129541605711,
            -0.028097521513700485,
            -0.007858720608055592,
            0.011389951221644878,
            0.019220976158976555,
            -0.007651000749319792,
            -0.002856142818927765,
            0.019373303279280663,
            -0.009866675361990929,
            0.036530934274196625,
            0.008114907890558243,
            0.004871021956205368,
            -0.03218267112970352,
            -0.020010311156511307,
            -0.025369472801685333,
            0.007117854431271553,
            -0.011244548484683037,
            0.01636829599738121,
            0.019207127392292023,
            0.002006224123761058,
            -0.018680905923247337,
            0.028332937508821487,
            0.018763992935419083,
            -0.01570359244942665,
            -0.018320858478546143,
            0.0061831166967749596,
            0.0004245266900397837,
            0.0035589272156357765,
            0.012296993285417557,
            -0.0017777326283976436,
            0.02074425294995308,
            0.024164699018001556,
            -0.010074394755065441,
            0.015731288120150566,
            0.004057453945279121,
            0.02557719312608242,
            0.020841188728809357,
            0.0029738505836576223,
            -0.010462137870490551,
            0.005286460742354393,
            -0.02063346840441227,
            0.010115939192473888,
            0.039355918765068054,
            0.0024857097305357456,
            -0.030050085857510567,
            0.016215967014431953,
            0.007574837189167738,
            -0.030714787542819977,
            0.009423540905117989,
            0.015011195093393326,
            0.021879784762859344,
            0.009423540905117989,
            0.0012480479199439287,
            -0.010891424492001534,
            -0.011535354889929295,
            -0.015108130872249603,
            -0.028776071965694427,
            -0.006695491261780262,
            -0.020979667082428932,
            -0.020314965397119522,
            -0.010600618086755276,
            -0.00976281613111496,
            0.019373303279280663,
            -0.03185031935572624,
            0.019539479166269302,
            0.013730257749557495,
            -0.02585415169596672,
            -0.05409015342593193,
            -0.0064185322262346745,
            0.011293016374111176,
            -0.011743075214326382,
            0.016395991668105125,
            -0.013224807567894459,
            -0.010642161592841148,
            0.006501619704067707,
            -0.004524822812527418,
            -0.012387005612254143,
            -0.010295961983501911,
            -0.026172654703259468,
            0.00934737641364336,
            -0.010185178369283676,
            -0.037140242755413055,
            -0.014720387756824493,
            -0.020605772733688354,
            0.023001471534371376,
            0.009278137236833572,
            -0.010475985705852509,
            -0.0004245266900397837,
            0.011258396320044994,
            -0.011341484263539314,
            0.010185178369283676,
            0.021395105868577957,
            0.016492927446961403,
            0.003100213361904025,
            -0.029025336727499962,
            -0.014360340312123299,
            -0.022613728418946266,
            -0.027100468054413795,
            -0.020785795524716377,
            0.015066586434841156,
            0.009243517182767391,
            0.023943131789565086,
            0.016562167555093765,
            0.007533293217420578,
            -0.011466115713119507,
            0.006238508503884077,
            -0.005639583803713322,
            0.01581437699496746,
            -0.008675750344991684,
            0.014221861027181149,
            0.011355332098901272,
            0.02363847754895687,
            -0.0021291247103363276,
            0.014332644641399384,
            -0.022336767986416817,
            -0.0253556240350008,
            -0.03982675075531006,
            0.029662342742085457,
            0.00026635697577148676,
            -0.014706538990139961,
            0.02007954940199852,
            0.017240718007087708,
            0.006743959151208401,
            -0.049354150891304016,
            0.002184516517445445,
            -0.020882731303572655,
            0.035146135836839676,
            0.0035796992015093565,
            -0.015537417493760586,
            -0.01996876671910286,
            -0.002068539848551154,
            -0.019941071048378944,
            -0.0007542813546024263,
            -0.029136119410395622,
            -0.0049679577350616455,
            0.007754860911518335,
            0.015177370049059391,
            -0.01276089996099472,
            -0.018999408930540085,
            0.017116084694862366,
            -0.02485709823668003,
            -0.007657925132662058,
            0.006920520681887865,
            0.025203296914696693,
            0.040962282568216324,
            -0.0248432494699955,
            0.011701530776917934,
            -0.037527985870838165,
            -0.013287123292684555,
            0.002075463766232133,
            -0.02563258446753025,
            0.008931937627494335,
            -0.01375795342028141,
            0.031878016889095306,
            0.012026958167552948,
            0.04043605923652649,
            -0.01963641494512558,
            0.013688714243471622,
            0.013584854081273079,
            -0.0014592293882742524,
            -0.022766055539250374,
            0.0021862476132810116,
            0.007955656386911869,
            -0.015274305827915668,
            -0.00835032295435667,
            0.003552003065124154,
            -0.0017110892804339528,
            0.029551558196544647,
            0.001432398916222155,
            0.008101060055196285,
            -0.0020702709443867207,
            -0.01409030519425869,
            -0.0006365736480802298,
            -0.014360340312123299,
            -0.03193340823054314,
            -0.019027104601264,
            -0.005625735968351364,
            0.0018867852631956339,
            -0.0035693130921572447,
            -0.019331760704517365,
            -0.009797435253858566,
            0.011209928430616856,
            0.014014140702784061,
            0.010856805369257927,
            -0.0030153945554047823,
            0.02568797580897808,
            0.008980405516922474,
            -0.0017119547119364142,
            0.023167645558714867,
            0.009631260298192501,
            -0.03395521268248558,
            -0.012525484897196293,
            -0.0429563894867897,
            0.004060915671288967,
            0.01564820110797882,
            -0.015177370049059391,
            0.010032851248979568,
            0.019345607608556747,
            -0.021145842969417572,
            0.018376249819993973,
            0.00284402584657073,
            0.0032438859343528748,
            -0.008620359003543854,
            -0.023818500339984894,
            -0.034536827355623245,
            0.005355700850486755,
            -0.028042130172252655,
            -0.006885901093482971,
            -0.012310841120779514,
            -0.015274305827915668,
            0.004137079697102308,
            0.0003397079126443714,
            0.02462168224155903,
            -0.026975836604833603,
            -0.025701824575662613,
            0.03827577829360962,
            0.011611519381403923,
            0.02679581381380558,
            -0.010088242590427399,
            0.010240570642054081,
            0.007768708746880293,
            0.000310930103296414,
            -0.020591923967003822,
            0.01952563226222992,
            0.01703299768269062,
            -0.006252356339246035,
            0.01725456491112709,
            0.016908366233110428,
            -0.011112992651760578,
            -0.005418016575276852,
            0.01819622702896595,
            0.019054800271987915,
            -0.021062755957245827,
            -0.049021799117326736,
            0.013273275457322598,
            0.018113138154149055,
            -0.005874999333173037,
            -0.020550381392240524,
            -0.03356746956706047,
            -0.0245662909001112,
            0.014581907540559769,
            -0.009215821512043476,
            0.016991453245282173,
            -0.004223629366606474,
            -0.007457129657268524,
            0.016880670562386513,
            0.005293384660035372,
            0.023555388674139977,
            -0.012463169172406197,
            -0.0013094982132315636,
            -0.005864613223820925,
            0.019345607608556747,
            -0.016880670562386513,
            0.0075194453820586205,
            0.010171330533921719,
            -0.004112845752388239,
            0.018514730036258698,
            0.007401737384498119,
            0.012989391572773457,
            -0.009395844303071499,
            0.013404830358922482,
            0.009153504855930805,
            -0.008184147998690605,
            -0.0030361665412783623,
            0.027765171602368355,
            -0.020938124507665634,
            -0.0034221785608679056,
            0.0022866453509777784,
            -0.011604594998061657,
            0.0051895249634981155,
            -0.0234169103205204,
            -0.010884501039981842,
            0.00040764949517324567,
            0.0011805390240624547,
            0.007761784829199314,
            0.01969180628657341,
            0.010462137870490551,
            -0.005781525745987892,
            0.011078372597694397,
            -0.0038912782911211252,
            -0.03185031935572624,
            0.007076310459524393,
            -0.0031919560860842466,
            0.01858397014439106,
            -0.032985854893922806,
            0.0027142013423144817,
            0.01542663387954235,
            -0.008731142617762089,
            0.00689282501116395,
            0.002828446915373206,
            -0.021090451627969742,
            0.0028249849565327168,
            0.02541101723909378,
            -0.028277546167373657,
            0.012165437452495098,
            0.01663140580058098,
            -0.0063769882544875145,
            0.00260168663226068,
            0.019650263711810112,
            0.0023905050475150347,
            -0.0023506921716034412,
            0.014484971761703491,
            -0.02275220677256584,
            -0.02430317923426628,
            -0.003756260732188821,
            -0.017393045127391815,
            -0.008751913905143738,
            -0.0019473701249808073,
            0.012518560513854027,
            0.01231776550412178,
            -0.017129933461546898,
            0.0008983867592178285,
            -0.018376249819993973,
            0.00796257983893156,
            0.019664110615849495,
            -0.0066435616463422775,
            -0.006352754309773445,
            0.02423393912613392,
            -0.020924275740981102,
            0.003221383085474372,
            -0.015108130872249603,
            0.009395844303071499,
            -0.024261634796857834,
            -0.05035120248794556,
            0.0017110892804339528,
            0.023112254217267036,
            -0.0006486906204372644,
            0.006307748146355152,
            -0.012470092624425888,
            -0.026948140934109688,
            0.0031261781696230173,
            -0.004756776150316,
            0.018016202375292778,
            -0.004995653405785561,
            0.013321742415428162,
            0.009956687688827515,
            0.01797465980052948,
            0.028526809066534042,
            0.009825131855905056,
            -0.017517676576972008,
            -0.02125662751495838,
            -0.011992338113486767,
            -0.011050676926970482,
            -0.035256918519735336,
            -0.0165344700217247,
            -0.02307070977985859,
            0.041793160140514374,
            -0.002992891473695636,
            -0.01747613213956356,
            -0.008301855064928532,
            -0.003043090458959341,
            -0.043177954852581024,
            -0.022336767986416817,
            -0.003728564828634262,
            0.012428549118340015,
            0.004341337364166975,
            0.03594931960105896,
            0.0004967957502231002,
            0.019110191613435745,
            -0.0030240495689213276,
            0.0020494989585131407,
            -0.008731142617762089,
            -0.01642368733882904,
            -0.0024130078963935375,
            -0.0038912782911211252,
            0.01067678164690733,
            0.00859958678483963,
            -0.02435857057571411,
            -0.00021042415755800903,
            0.020093398168683052,
            -0.008281083777546883,
            -0.003510459326207638,
            0.029551558196544647,
            -0.0017638845602050424,
            -0.009562020190060139,
            0.006716263480484486,
            0.0075471410527825356,
            0.002277990337461233,
            0.005899233277887106,
            0.01042751781642437,
            -0.012255449779331684,
            0.01137610338628292,
            0.012947848066687584,
            -0.020730404183268547,
            -0.014955802820622921,
            -0.006927444599568844,
            0.014235708862543106,
            0.013439450412988663,
            -0.0009053107351064682,
            0.00028539792401716113,
            -0.026214199140667915,
            0.0033771726302802563,
            -0.017711548134684563,
            0.04796935245394707,
            0.003918974194675684,
            0.0023905050475150347,
            0.02561873570084572,
            -0.0023178032133728266,
            -0.009354300796985626,
            0.00445212097838521,
            0.004420963115990162,
            0.008066440001130104,
            0.003950132057070732,
            0.022004418075084686,
            -0.02269681543111801,
            -0.016963757574558258,
            -0.008004124276340008,
            0.0016435803845524788,
            -0.029800821095705032,
            -0.029246903955936432,
            -0.020619621500372887,
            -0.0017162822186946869,
            -0.026034174486994743,
            0.024040067568421364,
            0.012657040730118752,
            -0.014692691154778004,
            0.007166322320699692,
            -0.005663817748427391,
            0.0024476279504597187,
            -0.006712801288813353,
            -0.018542425706982613,
            0.007747936528176069,
            -0.018348554149270058,
            0.010012079030275345,
            0.013120947405695915,
            -0.013688714243471622,
            -0.00672664912417531,
            0.010275190696120262,
            0.016797581687569618,
            -0.028208306059241295,
            0.02301531843841076,
            0.21248318254947662,
            -0.03622627630829811,
            0.010912196710705757,
            0.025715671479701996,
            0.017019148916006088,
            -0.014166468754410744,
            0.029745429754257202,
            0.007339421659708023,
            -0.012490864843130112,
            0.01003977470099926,
            0.000831310695502907,
            0.008087212219834328,
            -0.02190748229622841,
            -0.0009027142659761012,
            0.008281083777546883,
            -0.0314071848988533,
            -0.04741543531417847,
            -0.026158807799220085,
            -0.0034585294779390097,
            0.017116084694862366,
            0.010725249536335468,
            0.004459044896066189,
            -0.004133617505431175,
            -0.026546550914645195,
            0.020439596846699715,
            -0.01006747130304575,
            -0.0026916982606053352,
            0.006342368200421333,
            0.006581245455890894,
            0.010316734202206135,
            0.0015994400018826127,
            0.0038324245251715183,
            0.006051560863852501,
            -0.0014999078121036291,
            0.0072563341818749905,
            -0.01697760634124279,
            0.007110930513590574,
            -0.010275190696120262,
            0.004690998233854771,
            0.017725395038723946,
            0.007387889549136162,
            0.0014471124159172177,
            0.029579253867268562,
            -0.03506304696202278,
            0.006878976710140705,
            0.025784911587834358,
            -0.005289922934025526,
            -0.010690629482269287,
            0.008620359003543854,
            0.0028821078594774008,
            -0.010808337479829788,
            0.03279198333621025,
            0.030991746112704277,
            0.038054209202528,
            -0.0020131480414420366,
            0.009236592799425125,
            -0.03921743854880333,
            0.01714378222823143,
            0.012262373231351376,
            0.008551118895411491,
            -0.0278482586145401,
            -0.009236592799425125,
            0.0053833965212106705,
            0.024220092222094536,
            -0.018126986920833588,
            0.001204772968776524,
            -0.009105036966502666,
            0.003498342353850603,
            0.009922067634761333,
            -0.02790364995598793,
            -0.007207866292446852,
            -0.012982468120753765,
            -0.032598111778497696,
            0.015523569658398628,
            -0.021547434851527214,
            -0.04724925756454468,
            0.03257041424512863,
            0.038192689418792725,
            0.011687682941555977,
            0.04054684191942215,
            -0.01245624478906393,
            -0.01051060575991869,
            -0.006255818530917168,
            -0.0220736563205719,
            -0.00790718849748373,
            -0.036641716957092285,
            0.017517676576972008,
            -0.023375365883111954,
            -0.016285207122564316,
            -0.013349439017474651,
            -0.012248525395989418,
            -0.014817323535680771,
            -0.009478932246565819,
            0.015468177385628223,
            0.00650508189573884,
            0.005604964215308428,
            0.006608941592276096,
            -0.015620505437254906,
            -0.0023662711028009653,
            -0.022669119760394096,
            -0.014415732584893703,
            0.053785499185323715,
            0.031130226328969002,
            0.0030188565142452717,
            0.011078372597694397,
            0.012975543737411499,
            -0.003593547036871314,
            0.011417647823691368,
            0.004988729488104582,
            -0.0008049129974097013,
            0.008488803170621395,
            -0.041793160140514374,
            0.015149674378335476,
            -0.005660356022417545,
            -0.013993369415402412,
            0.005057969596236944,
            -4.646641536965035e-05,
            -0.0039051263593137264,
            0.021118147298693657,
            0.00028496517916209996,
            0.0008905972936190665,
            -0.022544488310813904,
            0.006982836406677961,
            -0.0030205873772501945,
            -0.0033564006444066763,
            -0.023887740448117256,
            -0.017850028350949287,
            -0.005092589184641838,
            -0.040685322135686874,
            0.005089127458631992,
            0.007893340662121773,
            -0.020273420959711075,
            -0.004628682509064674,
            0.01858397014439106,
            -0.010171330533921719,
            -0.004694460425525904,
            0.008641130290925503,
            -0.011078372597694397,
            -0.0075471410527825356,
            -0.004195933695882559,
            0.0028665289282798767,
            0.018071595579385757,
            -0.002819791901856661,
            0.0009918605210259557,
            0.027972890064120293,
            0.0018487033667042851,
            0.010946816764771938,
            0.028969943523406982,
            -0.008177223615348339,
            -0.002456282963976264,
            -0.030077781528234482,
            0.0048571741208434105,
            0.004296331200748682,
            -0.005199911072850227,
            0.018708601593971252,
            0.009499704465270042,
            -0.035810839384794235,
            0.005968473386019468,
            0.010219798423349857,
            0.02264142408967018,
            -0.04644607752561569,
            0.001801966573111713,
            0.002335113240405917,
            -0.016215967014431953,
            -0.009714348241686821,
            -0.01603594422340393,
            -0.17869414389133453,
            -0.016229815781116486,
            -0.00013815509737469256,
            -0.04207012057304382,
            0.010282114148139954,
            0.021228931844234467,
            0.012525484897196293,
            -0.008855774067342281,
            -0.034370649605989456,
            -0.004684074316173792,
            0.0063389060087502,
            -0.0017846565460786223,
            0.0004937665071338415,
            -0.01203388161957264,
            -0.010808337479829788,
            0.004950647708028555,
            -0.007242485880851746,
            0.00973511952906847,
            -0.0013043052749708295,
            0.0028249849565327168,
            0.0319611057639122,
            -0.033041246235370636,
            -0.011479963548481464,
            -0.02552179992198944,
            0.010836033150553703,
            0.014831171371042728,
            -0.0072840298525989056,
            0.02368002198636532,
            -0.005376472603529692,
            -0.029579253867268562,
            -0.017448436468839645,
            -0.009305832907557487,
            0.030964050441980362,
            -0.014118000864982605,
            0.01168075855821371,
            0.011971565894782543,
            0.02129817195236683,
            -0.00027133358526043594,
            -0.024593986570835114,
            0.018265467137098312,
            0.021145842969417572,
            0.024275483563542366,
            0.020231878384947777,
            0.011078372597694397,
            -0.030659396201372147,
            0.006965526845306158,
            0.021755153313279152,
            -0.030437828972935677,
            0.002503019757568836,
            -0.013287123292684555,
            0.024164699018001556,
            -0.02445550635457039,
            0.005978859029710293,
            0.004053991753607988,
            -0.003846272360533476,
            0.02434472367167473,
            -0.0018088904907926917,
            0.008814229629933834,
            -0.00760253332555294,
            -0.001434995443560183,
            -0.019761046394705772,
            -0.013944901525974274,
            0.0011026442516595125,
            9.374423825647682e-05,
            -0.01808544248342514,
            -0.03522922471165657,
            -0.012573952786624432,
            0.003262926824390888,
            -0.024663226678967476,
            0.004597524646669626,
            -0.013079403899610043,
            0.002425124868750572,
            -0.008059515617787838,
            -0.005622274242341518,
            0.02723894827067852,
            0.018348554149270058,
            0.00865497812628746,
            0.02163052186369896,
            0.016132880002260208,
            -0.0012601648923009634,
            -0.018570121377706528,
            0.01814083568751812,
            -0.0004275559331290424,
            0.017725395038723946,
            -0.01941484771668911,
            -0.010822185315191746,
            -0.008101060055196285,
            0.014734235592186451,
            -0.02163052186369896,
            -0.010891424492001534,
            0.039245136082172394,
            -0.010102090425789356,
            -0.0038220384158194065,
            -0.013439450412988663,
            -0.004344799090176821,
            0.015578960999846458,
            0.00404706783592701,
            -0.017102237790822983,
            -0.003304470796138048,
            0.00030768447322770953,
            -0.005826531443744898,
            0.0026726573705673218,
            0.00034771376522257924,
            -0.004524822812527418,
            0.029136119410395622,
            0.008592662401497364,
            -0.04043605923652649,
            0.012532409280538559,
            0.036475542932748795,
            -0.026089567691087723,
            -0.025715671479701996,
            -0.01375795342028141,
            -0.011230699717998505,
            -0.003283698810264468,
            -0.01167383510619402,
            0.006162344478070736,
            0.0001670410856604576,
            -0.0319611057639122,
            0.01952563226222992,
            0.0018677443731576204,
            0.039688270539045334,
            -0.020314965397119522,
            -0.0027488211635500193,
            -0.006830508820712566,
            0.003352938685566187,
            -0.024261634796857834,
            -0.12164053320884705,
            -0.02613111026585102,
            0.01586976833641529,
            0.015163522213697433,
            -0.010600618086755276,
            0.034370649605989456,
            0.0038947402499616146,
            0.04619681462645531,
            -0.020148789510130882,
            0.04403652995824814,
            -0.02485709823668003,
            -0.0018677443731576204,
            0.013349439017474651,
            -0.002939230762422085,
            0.029329990968108177,
            -0.009873599745333195,
            0.002016610000282526,
            -0.015883617103099823,
            -0.040131404995918274,
            0.027086621150374413,
            0.019165584817528725,
            -0.012442396953701973,
            0.007394813466817141,
            -0.028914552181959152,
            -0.014581907540559769,
            -0.006629713345319033,
            -0.03556157648563385,
            0.02319534309208393,
            0.02096582017838955,
            0.005629198160022497,
            0.017545372247695923,
            -0.034647610038518906,
            -0.008564966730773449,
            -0.026089567691087723,
            0.024317028000950813,
            -0.005767677444964647,
            0.0016617558430880308,
            -0.012248525395989418,
            0.016119031235575676,
            -0.012206981889903545,
            -0.008004124276340008,
            0.007803328800946474,
            0.021658217534422874,
            -0.004542132839560509,
            0.004040143918246031,
            -0.030050085857510567,
            -0.007346345577389002,
            0.02030111663043499,
            0.009555095806717873,
            -0.016839126124978065,
            -0.03489687293767929,
            -0.013377134688198566,
            -0.05032350867986679,
            0.01825161837041378,
            0.016880670562386513,
            0.01034442987293005,
            0.009091189131140709,
            0.017946964129805565,
            -0.0074779014103114605,
            0.00890424195677042,
            -0.01071140170097351,
            0.0018002354772761464,
            -0.029606949537992477,
            0.006290438584983349,
            0.00895963329821825,
            0.0005703630740754306,
            0.0006188309635035694,
            -0.02602032758295536,
            0.003044821321964264,
            -0.0016738728154450655,
            -0.0020720018073916435,
            0.019151736050844193,
            -0.011196080595254898,
            0.006799350958317518,
            -0.0018002354772761464,
            -0.0038531965110450983,
            -0.020176485180854797,
            -0.029634647071361542,
            -0.005992707330733538,
            0.0009961880277842283,
            -0.007644076831638813,
            -0.016686799004673958,
            0.007024380378425121,
            -0.009866675361990929,
            -6.009800563333556e-05,
            -0.0037389507051557302,
            -0.015662049874663353,
            -0.005199911072850227,
            -0.008461106568574905,
            -0.01913788914680481,
            0.0049229515716433525,
            0.029579253867268562,
            0.004725618287920952,
            -0.019788742065429688,
            0.006082718726247549,
            0.02474631369113922,
            -0.019096344709396362,
            -0.004247863311320543,
            0.037361811846494675,
            0.006581245455890894,
            -0.00837801955640316,
            0.009285060688853264,
            -0.05325927585363388,
            0.02696198970079422,
            0.016354447230696678,
            -0.006986298598349094,
            0.005355700850486755,
            -0.012110046111047268,
            0.019677959382534027,
            -0.023652324452996254,
            0.01245624478906393,
            0.009638183750212193,
            -0.023043014109134674,
            0.022544488310813904,
            -0.006238508503884077,
            -0.010462137870490551,
            -0.02219828963279724,
            0.0026155344676226377,
            0.03107483498752117,
            -0.0011251472169533372,
            0.011909250169992447,
            -0.0016617558430880308,
            0.003901664400473237,
            -0.02280759997665882,
            0.0027886340394616127,
            0.011078372597694397,
            0.007734088692814112,
            -0.0036627869121730328,
            -0.02762669138610363,
            0.012774747796356678,
            -0.0016756037948653102,
            -0.014568059705197811,
            -0.009243517182767391,
            -0.01098143681883812,
            -0.011639215052127838,
            0.003991676028817892,
            -0.014568059705197811,
            -0.009928991086781025,
            0.027723627164959908,
            0.030742483213543892,
            0.01852857880294323,
            0.04564289376139641,
            -0.02229522541165352,
            -0.03301354870200157,
            0.0015371241606771946,
            -0.020841188728809357,
            -0.004902179818600416,
            -0.017268413677811623,
            0.0025999555364251137,
            -0.007228638045489788,
            0.028969943523406982,
            -0.0016046330565586686,
            0.04597524553537369,
            0.014941954985260963,
            -0.020868884399533272,
            -0.02434472367167473,
            -0.009928991086781025,
            0.01175692304968834,
            0.01969180628657341,
            0.012040806002914906,
            0.011867706663906574,
            0.02057807706296444,
            0.02046729251742363,
            0.005521876271814108,
            0.008869621902704239,
            -0.012795520015060902,
            0.0030967514030635357,
            -0.009478932246565819,
            -0.014249556697905064,
            -0.007117854431271553,
            0.0037493365816771984,
            -0.002913265721872449,
            -0.0005448309239000082,
            0.005965011194348335,
            0.018445489928126335,
            0.006944754626601934,
            0.007110930513590574,
            -0.014900410547852516,
            0.004292869474738836,
            0.005445712246000767,
            -0.01062831375747919,
            0.010233646258711815,
            0.00032412895234301686,
            -0.007616381160914898,
            0.0019525631796568632,
            0.00658816983923316,
            0.018390098586678505,
            0.0019871830008924007,
            -0.004420963115990162,
            0.002729780273512006,
            -0.013259426690638065,
            -0.007311725988984108,
            0.0008633341058157384,
            -0.002729780273512006,
            -0.006273128557950258,
            0.026061872020363808,
            0.013107099570333958,
            0.0037943425122648478,
            -0.007000146433711052,
            0.005739981774240732,
            0.019456392154097557,
            0.0021498966962099075,
            0.010489833541214466,
            -0.022267527878284454,
            -0.009423540905117989,
            -0.010621389374136925,
            -0.028526809066534042,
            0.01179154310375452,
            -0.04373187571763992,
            -0.022544488310813904,
            0.03489687293767929,
            0.023666173219680786,
            0.010974512435495853,
            -0.010607541538774967,
            0.007484825327992439,
            0.013210958801209927,
            -0.027225099503993988,
            0.03428756445646286,
            -0.009555095806717873,
            -0.014886562712490559,
            -0.030520915985107422,
            0.027308188378810883,
            0.040962282568216324,
            0.0035866231191903353,
            0.050572771579027176,
            0.00012722818064503372,
            0.020675012841820717,
            -0.0014445158885791898,
            0.017268413677811623,
            -0.029551558196544647,
            0.017850028350949287,
            -0.0024701307993382215,
            -0.012643192894756794,
            -0.0038116525392979383,
            -0.014568059705197811,
            -0.0065985554829239845,
            -0.008232615888118744,
            -0.002944423584267497,
            -0.017116084694862366,
            0.029773125424981117,
            0.011936946772038937,
            0.058493807911872864,
            0.014817323535680771,
            -0.007948732003569603,
            0.028055978938937187,
            -0.007221714127808809,
            0.02269681543111801,
            0.014831171371042728,
            0.026200350373983383,
            -0.003037897404283285,
            -0.025369472801685333,
            -0.0016782003222033381,
            0.003309663850814104,
            0.005951163358986378,
            -0.029496166855096817,
            -0.0033200497273355722,
            -0.015662049874663353,
            0.003621242940425873,
            0.02478785812854767,
            -0.00934737641364336,
            -0.01109914481639862,
            0.03755568340420723,
            -0.005881923250854015,
            0.015468177385628223,
            0.013612549751996994,
            -0.014595755375921726,
            0.00790718849748373,
            0.018487034365534782,
            0.009319680742919445,
            -0.004746390040963888,
            0.001062831375747919,
            0.025881847366690636,
            0.021270474418997765,
            -0.031324099749326706,
            -0.015094282105565071,
            0.009548172354698181,
            0.007817176170647144,
            0.005646508187055588,
            -0.025757215917110443,
            -0.001264492399059236,
            -0.007692544721066952,
            -0.0018573583802208304,
            0.007810252718627453,
            -0.0038220384158194065,
            -0.02264142408967018,
            -0.0032975468784570694,
            0.0016150189330801368,
            -0.0009840710554271936,
            -0.008357247337698936,
            -0.01563435234129429
          ]
        },
        "type": "document"
      },
      "target": {
        "id": "1d35c58e-1af7-4136-bfeb-f0894fc21db0",
        "properties": {
          "page_content": "Simon Willison’s Weblog\n\nSubscribe\n\nStuff we figured out about AI in 2023\n\n31st December 2023\n\n2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s.\n\nHere’s my attempt to round up the highlights in one place!\n\nLarge Language Models\n\nThey’re actually quite easy to build\n\nYou can run LLMs on your own devices\n\nHobbyists can build their own fine-tuned models\n\nWe don’t yet know how to build GPT-4\n\nVibes Based Development\n\nLLMs are really smart, and also really, really dumb\n\nGullibility is the biggest unsolved problem\n\nCode may be the best application\n\nThe ethics of this space remain diabolically complex\n\nMy blog in 2023\n\nHere’s the sequel to this post: Things we learned about LLMs in 2024.\n\nLarge Language Models\n\nIn the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software.\n\nLLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code.\n\nThey can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes.\n\nSo far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life.\n\nA lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity.\n\nThey’re actually quite easy to build\n\nThe most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build.\n\nIntuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version!\n\nWhat matters most is the training  data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is.\n\nIf you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM.\n\nA year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\n\nThe training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing.\n\nSo training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries).\n\nYou can run LLMs on your own devices\n\nIn January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them.\n\nThen in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook.\n\nI wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call!\n\nThis unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use.\n\nToday there are literally thousands of LLMs that can be run locally, on all manner of different devices.\n\nI run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins.\n\nYou can even run them entirely in your browser using WebAssembly and the latest Chrome!\n\nHobbyists can build their own fine-tuned models\n\nI said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely.\n\nThere’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too.\n\nThe Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours.\n\nThe best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data.\n\nThis is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them.\n\nWe don’t yet know how to build GPT-4\n\nFrustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4.\n\nOpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing.\n\nThis may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out.\n\nThe team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then.\n\nStill, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet.\n\nVibes Based Development\n\nAs a computer scientist and software engineer, LLMs are infuriating.\n\nEven the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them.\n\nI’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that!\n\nThe worst part is the challenge of evaluating them.\n\nThere are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task.\n\nI find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself!\n\nThe most frustrating thing for me is at the level of individual prompting.\n\nSometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out.\n\nWe’re left with what’s effectively Vibes Based Development. It’s vibes all the way down.\n\nI’d love to see us move beyond vibes in 2024!\n\nLLMs are really smart, and also really, really dumb\n\nOn the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun!\n\nBut on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb.\n\nDoes ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays?\n\nThe honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer.\n\nSometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead.\n\nThere are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works!\n\nGullibility is the biggest unsolved problem\n\nI coined the term prompt injection in September last year.\n\n15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem.\n\nI’ve written a ton about this already.\n\nBeyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility.\n\nLanguage Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt.\n\nIn order to be useful tools for us, we need them to believe what we feed them!\n\nBut it turns out a lot of the things we want to build need them not to be gullible.\n\nEveryone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited.\n\nA lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes.\n\nI think this is because of gullibility.\n\nCan we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true!\n\nCode may be the best application\n\nOver the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of.\n\nIf you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English.\n\nIt’s still astonishing to me how effective they are though.\n\nOne of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless.\n\nExcept... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works!\n\nSo hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language!\n\nHow should we feel about this as software engineers?\n\nOn the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you?\n\nOn the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can.\n\nThe ethics of this space remain diabolically complex\n\nIn September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion.\n\nSince then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data.\n\nJust this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere.\n\nThe legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future.\n\nLaw is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people?\n\nAs the quality of results produced by AI models has increased over the year, these questions have become even more pressing.\n\nThe impact on human society in terms of these models is already huge, if difficult to objectively measure.\n\nPeople have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators.\n\nThere are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic.\n\nMy blog in 2023\n\nHere’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard):\n\nThe top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78).\n\nI’ve written a lot about this stuff!\n\nI grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic:\n\nArticle Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k\n\nI also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023:\n\nPrompt injection explained, with video, slides, and a transcript\n\nCatching up on the weird world of LLMs\n\nMaking Large Language Models work for you\n\nOpen questions for AI engineering\n\nEmbeddings: What they are and why they matter\n\nFinancial sustainability for open source projects at GitHub Universe\n\nAnd in podcasts:\n\nWhat AI can do for you on the Theory of Change\n\nWorking in public on Path to Citus Con\n\nLLMs break the internet on the Changelog\n\nTalking Large Language Models on Rooftop Ruby\n\nThoughts on the OpenAI board situation on Newsroom Robots\n\nIndustry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations\n\nPosted \n\n31st December 2023 at 11:59 pm · Follow me on\n\nMastodon or\n\nTwitter or\n\nsubscribe to my newsletter\n\nMore recent articles\n\nUsing pip to install a Large Language Model that's under 100MB - 7th February 2025\n\nOpenAI o3-mini, now available in LLM - 31st January 2025\n\nA selfish personal argument for releasing code as Open Source - 24th January 2025\n\nThis is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023.\n\nPart of series LLMs annual review\n\nStuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m.\n\nThings we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m.\n\nblogging\n            68\n\nai\n            1089\n\ngenerative-ai\n            934\n\nllms\n            922\n\nNext: Tom Scott, and the formidable power of escalating streaks\n\nPrevious: Last weeknotes of 2023\n\nColophon\n\n©\n\n2002\n\n2003\n\n2004\n\n2005\n\n2006\n\n2007\n\n2008\n\n2009\n\n2010\n\n2011\n\n2012\n\n2013\n\n2014\n\n2015\n\n2016\n\n2017\n\n2018\n\n2019\n\n2020\n\n2021\n\n2022\n\n2023\n\n2024\n\n2025",
          "document_metadata": {
            "source": "data/2023_llms.html"
          },
          "headlines": [
            "Large Language Models",
            "Vibes Based Development",
            "LLMs are really smart, and also really, really dumb",
            "Gullibility is the biggest unsolved problem",
            "Code may be the best application"
          ],
          "summary": "In 2023, Large Language Models (LLMs) emerged as a significant development in AI, with advancements making them easier to build and run on personal devices. Despite their capabilities, LLMs face challenges like gullibility and ethical concerns over unlicensed training data. They excel in code generation, where errors can be iteratively corrected. The year saw a proliferation of LLMs from various organizations, though none surpassed GPT-4. The complexity of LLMs remains a barrier, with 'Vibes Based Development' highlighting the difficulty in evaluating their performance. The ethical and legal implications of AI continue to be complex and unresolved.",
          "summary_embedding": [
            -0.01449595857411623,
            -0.005809996742755175,
            0.0007078905473463237,
            -0.03407438471913338,
            -0.0034515000879764557,
            0.01598517596721649,
            -0.015042460523545742,
            0.02121793106198311,
            -0.023909451439976692,
            -0.04071437940001488,
            0.025603607296943665,
            0.020726079121232033,
            -0.023991426452994347,
            -0.0034241750836372375,
            0.0019861923065036535,
            0.0180345568805933,
            0.021983033046126366,
            0.0025992989540100098,
            0.017529044300317764,
            -0.007432423997670412,
            -0.0009734562481753528,
            -0.0001246707106474787,
            -0.014441308565437794,
            -0.017406079918146133,
            -0.005376210901886225,
            0.006428226828575134,
            0.022898422554135323,
            -0.03661561757326126,
            -0.014618922024965286,
            -0.0006575098959729075,
            0.02032986469566822,
            -0.0041534132324159145,
            -0.03789989650249481,
            0.007480242755264044,
            -0.029647719115018845,
            -0.00886699091643095,
            0.013819662854075432,
            -0.007883287966251373,
            0.020575791597366333,
            0.005519667640328407,
            0.028800642117857933,
            0.02869134210050106,
            0.015138098038733006,
            -0.02002928964793682,
            -0.029073892161250114,
            0.017815956845879555,
            0.004792137071490288,
            -0.01644970290362835,
            -0.03341858088970184,
            0.012070856988430023,
            0.019318837672472,
            0.037872571498155594,
            -0.011421886272728443,
            -0.014536946080625057,
            -0.00784913171082735,
            0.002877673367038369,
            -0.014113407582044601,
            0.01066361553966999,
            0.017706656828522682,
            0.00035437222686596215,
            -0.006291601341217756,
            -0.01573925092816353,
            -0.002004978246986866,
            0.024879492819309235,
            -0.006978144403547049,
            -0.010670446790754795,
            -0.027584675699472427,
            0.03314533084630966,
            0.033965084701776505,
            -0.013266329653561115,
            -0.0027427556924521923,
            -0.008095056749880314,
            0.019154885783791542,
            -0.011121310293674469,
            0.017925256863236427,
            0.007992587983608246,
            -0.006277938839048147,
            -0.015261061489582062,
            -0.0275436881929636,
            0.004747733939439058,
            -0.0019281264394521713,
            -0.0006651951116509736,
            -0.003910903353244066,
            0.013150198385119438,
            0.013744519092142582,
            0.004597445949912071,
            -0.005208844784647226,
            -0.017692994326353073,
            -0.0015805854927748442,
            -0.013437111862003803,
            -0.005540161393582821,
            -0.005652877502143383,
            0.016531677916646004,
            0.017228467389941216,
            -0.0062813544645905495,
            -0.0023106776643544436,
            -0.008962628431618214,
            0.002288476098328829,
            -0.005642630625516176,
            -0.026764923706650734,
            -0.00512003805488348,
            0.0024695047177374363,
            -0.040905654430389404,
            -0.008313657715916634,
            -0.023608876392245293,
            0.0037196276243776083,
            0.015684600919485092,
            0.021245256066322327,
            0.015124435536563396,
            0.018949948251247406,
            -0.00663658045232296,
            0.027652988210320473,
            0.016518015414476395,
            -0.02136821858584881,
            -0.0024438875261694193,
            -0.015807563439011574,
            0.0029476936906576157,
            -0.0006071293028071523,
            0.005079050548374653,
            0.008095056749880314,
            0.031095949932932854,
            0.010404027067124844,
            0.013778675347566605,
            -0.0019042170606553555,
            0.019701387733221054,
            -0.015151760540902615,
            0.005929544102400541,
            -0.020726079121232033,
            -0.01234410796314478,
            -0.0003955733263865113,
            0.0020237641874700785,
            -0.0027290931902825832,
            0.010294727049767971,
            0.0027188463136553764,
            -0.026901548728346825,
            0.031369201838970184,
            -0.023704513907432556,
            -0.0034737016540020704,
            0.009386167861521244,
            0.010397195816040039,
            0.0009649171261116862,
            0.02467455342411995,
            -0.022761797532439232,
            0.017515381798148155,
            -0.004358351230621338,
            0.018717685714364052,
            0.009413492865860462,
            0.013361968100070953,
            -0.0024353484623134136,
            -0.000552052166312933,
            0.004450573585927486,
            -0.03153315186500549,
            -0.01802089437842369,
            0.02321266196668148,
            0.004679421428591013,
            0.004518886562436819,
            0.00696106581017375,
            0.023267311975359917,
            -0.031369201838970184,
            -0.010349377058446407,
            -0.014741884544491768,
            0.009563780389726162,
            -0.0032328993547707796,
            0.019797025248408318,
            0.01363521907478571,
            0.021245256066322327,
            0.028800642117857933,
            0.0026932288892567158,
            -0.006749296560883522,
            -0.014687234535813332,
            -0.014427646063268185,
            0.030604097992181778,
            -0.03085002303123474,
            0.011968388222157955,
            -0.015138098038733006,
            0.020111264660954475,
            0.023321961984038353,
            -0.0037025492638349533,
            -0.024783853441476822,
            -0.018553733825683594,
            0.01804821938276291,
            0.009666250087320805,
            0.01613546349108219,
            -0.009270035661756992,
            -0.013505424372851849,
            0.0011843717657029629,
            0.010499664582312107,
            0.0041397507302463055,
            0.012214314192533493,
            -0.02293941006064415,
            0.027652988210320473,
            0.018581058830022812,
            -0.005468433257192373,
            -0.028800642117857933,
            -0.6571137309074402,
            -0.0369708426296711,
            0.015821225941181183,
            -0.010690940544009209,
            0.002980142366141081,
            -0.0036342365201562643,
            0.0009486929047852755,
            -0.01284279115498066,
            -0.015343036502599716,
            0.03306335583329201,
            -0.013525918126106262,
            0.006315510720014572,
            -0.003108228789642453,
            -0.019646737724542618,
            0.011216948740184307,
            -0.009782381355762482,
            0.0042900387197732925,
            -0.026478009298443794,
            0.007610036991536617,
            0.018376121297478676,
            -0.005273741669952869,
            0.019072910770773888,
            -0.0071181850507855415,
            -0.006609255447983742,
            -0.028445415198802948,
            0.006571683567017317,
            0.009010447189211845,
            0.015206411480903625,
            -0.006018350366503,
            0.004180738236755133,
            -0.01097102276980877,
            0.00577242486178875,
            0.0032414384186267853,
            0.016258427873253822,
            0.04273643717169762,
            -0.001592540298588574,
            -0.014004107564687729,
            0.027311425656080246,
            -0.001619865302927792,
            -0.00018519151490181684,
            -0.044758494943380356,
            -0.010943697765469551,
            -0.01646336540579796,
            0.03147850185632706,
            -0.005205429159104824,
            0.03186105191707611,
            0.024401303380727768,
            -0.001023836899548769,
            -0.013819662854075432,
            -0.007972094230353832,
            -0.0005725459777750075,
            0.010205919854342937,
            0.00799941923469305,
            0.00587830925360322,
            -0.008259007707238197,
            0.008183863945305347,
            0.014837522059679031,
            -0.010984685271978378,
            -0.000806943979114294,
            -0.019578425213694572,
            0.006585346069186926,
            -0.016408715397119522,
            -0.021381881088018417,
            -0.03128722682595253,
            -0.013484930619597435,
            0.011059829033911228,
            0.007773987483233213,
            0.012542215175926685,
            0.0005225922795943916,
            -0.024838505312800407,
            -0.00268468982540071,
            0.043337587267160416,
            0.004126088228076696,
            -0.01181126944720745,
            -0.0068859220482409,
            0.01528838649392128,
            0.00994633138179779,
            -0.010615796782076359,
            -0.006687815301120281,
            0.008989953435957432,
            0.006428226828575134,
            -0.018362458795309067,
            0.0042251418344676495,
            -0.0011211824603378773,
            0.03861035034060478,
            -0.006137897726148367,
            0.01658632792532444,
            -0.006120819598436356,
            0.0015626534586772323,
            0.009119748137891293,
            0.010848059318959713,
            0.02349957451224327,
            -0.017501719295978546,
            -0.05607108026742935,
            0.004952671937644482,
            0.009119748137891293,
            -0.013450774364173412,
            0.011449211277067661,
            0.017242129892110825,
            -0.02093101665377617,
            -0.006954234559088945,
            -0.012077688239514828,
            0.03505808860063553,
            0.023704513907432556,
            0.011633655987679958,
            0.0232399869710207,
            0.0023499575909227133,
            0.005625552497804165,
            0.015479662455618382,
            -0.02410072647035122,
            -0.006503370590507984,
            0.002655657008290291,
            0.003606911515817046,
            -0.00566312437877059,
            -0.0068859220482409,
            -0.029620394110679626,
            -0.01053382083773613,
            -0.0029306155629456043,
            0.012446577660739422,
            -0.02683323621749878,
            0.012637852691113949,
            -0.0044130017049610615,
            0.026887886226177216,
            0.00035330484388396144,
            -0.005079050548374653,
            0.004785305820405483,
            -0.002153558423742652,
            -0.006141313351690769,
            -0.005796334240585566,
            0.023745501413941383,
            -0.01643604040145874,
            -0.027775950729846954,
            0.012064025737345219,
            -0.016395052894949913,
            0.028773317113518715,
            0.01629941537976265,
            0.04265446215867996,
            -0.001146799768321216,
            0.006032012868672609,
            0.015343036502599716,
            -0.023690851405262947,
            0.0001412152050761506,
            0.0016659764805808663,
            0.0137718440964818,
            -0.04216260835528374,
            -0.02538500539958477,
            -0.005628968123346567,
            -0.004952671937644482,
            0.011920569464564323,
            -0.016477027907967567,
            -0.017911594361066818,
            -0.008621064946055412,
            -0.016340402886271477,
            0.03415635973215103,
            0.0009871188085526228,
            -0.009433986619114876,
            -0.015520649962127209,
            -0.02378648892045021,
            -0.003485656576231122,
            -0.026300396770238876,
            0.0016958632040768862,
            0.006954234559088945,
            0.0024438875261694193,
            0.006513617932796478,
            0.02468821592628956,
            -0.02064410410821438,
            -0.0076510244980454445,
            0.02710648626089096,
            -0.03358253091573715,
            -0.022966735064983368,
            0.009461311623454094,
            -0.03284475579857826,
            0.012446577660739422,
            0.030084921047091484,
            -0.010704603046178818,
            0.018130196258425713,
            -0.0034515000879764557,
            0.001164731802418828,
            0.008211188949644566,
            -0.011319417506456375,
            0.004358351230621338,
            0.03057677298784256,
            0.014632584527134895,
            -0.004300285596400499,
            0.022447559982538223,
            0.02337661199271679,
            0.03514006361365318,
            0.014058757573366165,
            -0.006414564326405525,
            0.01001464482396841,
            0.00311847566626966,
            0.032407552003860474,
            -0.011640487238764763,
            0.000776630244217813,
            -0.006387239322066307,
            0.022119658067822456,
            -0.0013158736983314157,
            -0.009953162632882595,
            0.009755056351423264,
            0.023841138929128647,
            0.014113407582044601,
            0.024278340861201286,
            0.018225833773612976,
            -0.010902710258960724,
            0.0012535384157672524,
            -0.01814385876059532,
            -0.002088661305606365,
            -0.014782872051000595,
            -0.00263003958389163,
            0.016545340418815613,
            0.009645755402743816,
            0.003593249013647437,
            -0.013034067116677761,
            -0.02250220999121666,
            -0.0016779311699792743,
            0.02855471707880497,
            0.005219091661274433,
            0.011790774762630463,
            -0.014605259522795677,
            -3.9333182940026745e-05,
            0.0129042724147439,
            0.011797606945037842,
            0.018840648233890533,
            -0.012337276712059975,
            -0.0029630642384290695,
            0.019578425213694572,
            -0.025999819859862328,
            0.014332008548080921,
            -0.0006276231142692268,
            -0.0360691137611866,
            0.009843862615525723,
            0.015028798021376133,
            0.024551590904593468,
            0.015588962472975254,
            0.007760324981063604,
            -0.010007813572883606,
            0.026751261204481125,
            -0.016108138486742973,
            0.024278340861201286,
            -0.0026095458306372166,
            -0.003579586511477828,
            0.008696208707988262,
            0.0004265275492798537,
            -0.010547483339905739,
            0.03287208080291748,
            -0.0005921858828514814,
            0.0325988307595253,
            -0.011729293502867222,
            -0.02636870928108692,
            0.005953453481197357,
            -0.037571996450424194,
            0.0070157162845134735,
            -0.02453792840242386,
            0.004792137071490288,
            0.002737632254138589,
            -0.017091842368245125,
            0.010110282339155674,
            0.0030723644886165857,
            0.016982542350888252,
            0.009563780389726162,
            -0.013512255623936653,
            0.009673081338405609,
            0.004829709418118,
            0.0012287750141695142,
            0.0158485509455204,
            -0.007951600477099419,
            -0.008259007707238197,
            0.0018222418148070574,
            -0.012924766167998314,
            -0.015411349013447762,
            -0.022092333063483238,
            -0.023144349455833435,
            0.00914707314223051,
            -0.03057677298784256,
            0.03473018482327461,
            -0.002833270002156496,
            0.0007988318684510887,
            0.006264276336878538,
            0.012692502699792385,
            0.046234048902988434,
            -0.022379247471690178,
            -0.037408046424388885,
            0.013812831602990627,
            0.0038972406182438135,
            -0.006595592945814133,
            -0.016408715397119522,
            -0.050688035786151886,
            0.01008978858590126,
            -0.0005913319764658809,
            -0.013621555641293526,
            -0.004966334439814091,
            0.0068790907971560955,
            -0.001078487024642527,
            0.006527280434966087,
            -0.013621555641293526,
            0.022310933098196983,
            0.025425994768738747,
            -0.005365964025259018,
            -0.007965262979269028,
            -0.020097602158784866,
            0.007582711987197399,
            0.009181229397654533,
            -0.017802294343709946,
            -0.025207392871379852,
            0.031642451882362366,
            -0.012849622406065464,
            -0.04139750823378563,
            -0.00045854912605136633,
            -0.003917734604328871,
            -0.01182493194937706,
            -0.009256373159587383,
            0.006824440788477659,
            0.013013572432100773,
            0.004498392343521118,
            0.016955217346549034,
            0.0161491259932518,
            0.014386658556759357,
            0.002249196171760559,
            0.012275795452296734,
            0.006069585215300322,
            0.013225342147052288,
            -0.043911416083574295,
            0.0020425503607839346,
            0.015028798021376133,
            0.07498004287481308,
            0.006001272238790989,
            -0.02192838303744793,
            0.003606911515817046,
            -0.021381881088018417,
            -0.009290529415011406,
            -0.02020690217614174,
            -0.00872353371232748,
            0.0076510244980454445,
            0.022597847506403923,
            -0.013669375330209732,
            -0.00907192938029766,
            0.015903200954198837,
            -0.004320779349654913,
            0.03451158478856087,
            0.025303030386567116,
            0.01161999348551035,
            -0.03161512687802315,
            -0.023704513907432556,
            -0.014974148012697697,
            -4.317577258916572e-05,
            0.005721190012991428,
            0.02295307256281376,
            0.004822877701371908,
            0.015547974966466427,
            -0.017337767407298088,
            0.027147473767399788,
            0.0159988384693861,
            -0.03197035193443298,
            -0.01657266542315483,
            0.0087918471544981,
            0.012132339179515839,
            0.004013372119516134,
            0.012747153639793396,
            -0.006629749201238155,
            0.021299906075000763,
            0.010581640526652336,
            0.0010195673676207662,
            0.012883778661489487,
            -0.0031748334877192974,
            0.031669776886701584,
            0.024141713976860046,
            0.0024917065165936947,
            -0.003480532905086875,
            0.008798678405582905,
            -0.02451060339808464,
            -0.0019452046835795045,
            0.040304504334926605,
            0.016039825975894928,
            -0.03404705971479416,
            0.015370361506938934,
            0.005150779150426388,
            -0.030959324911236763,
            -0.010854890570044518,
            0.015588962472975254,
            0.025207392871379852,
            0.008634727448225021,
            -0.005082466173917055,
            -0.0008543359581381083,
            -0.026013482362031937,
            -0.018526408821344376,
            -0.03270813077688217,
            -0.013409786857664585,
            -0.02005661465227604,
            -0.01905924826860428,
            -0.0008052361663430929,
            -0.025316692888736725,
            0.008259007707238197,
            -0.03604178875684738,
            0.00815653894096613,
            0.0029203686863183975,
            -0.025781219825148582,
            -0.044375941157341,
            -0.003400265472009778,
            0.006110572721809149,
            -0.010199088603258133,
            0.028500067070126534,
            -0.006315510720014572,
            -0.003753783879801631,
            0.01269933395087719,
            -0.00019896079902537167,
            -0.0115448497235775,
            -0.016367727890610695,
            -0.009153904393315315,
            0.0038733312394469976,
            -0.0018529824446886778,
            -0.024920480325818062,
            -0.020821716636419296,
            0.0037571995053440332,
            0.01226896420121193,
            0.01254904642701149,
            -0.0077534937299788,
            0.00034455227432772517,
            0.009864356368780136,
            -0.007521230261772871,
            0.011654149740934372,
            0.025917844846844673,
            0.020589454099535942,
            -0.004597445949912071,
            -0.01890896074473858,
            -0.007295798510313034,
            -0.027516363188624382,
            -0.022761797532439232,
            -0.031642451882362366,
            0.005792918615043163,
            0.012562708929181099,
            0.028008215129375458,
            0.02306237444281578,
            0.002141603734344244,
            -0.010076126083731651,
            0.012091350741684437,
            -0.004249051213264465,
            0.022133320569992065,
            -0.006650242954492569,
            0.020944679155945778,
            0.010841228067874908,
            0.019291510805487633,
            -0.006274523213505745,
            0.01703719235956669,
            -0.02609545923769474,
            -0.01804821938276291,
            -0.047791577875614166,
            0.03464820981025696,
            0.006356498692184687,
            -0.0139221316203475,
            0.017692994326353073,
            0.014427646063268185,
            0.013211679644882679,
            -0.03328195586800575,
            0.012747153639793396,
            0.009987319819629192,
            0.024715540930628777,
            0.007582711987197399,
            -0.0014243201585486531,
            -0.011503862217068672,
            -0.02016591466963291,
            -0.01886797323822975,
            0.012938428670167923,
            -0.01527472399175167,
            -0.013799169100821018,
            0.011592668481171131,
            0.023035049438476562,
            2.3068885639077052e-05,
            -0.02352689951658249,
            0.016668302938342094,
            -0.025043442845344543,
            -0.008286332711577415,
            0.01886797323822975,
            0.010567978024482727,
            0.04686252400279045,
            -0.02468821592628956,
            0.026286734268069267,
            -0.03989462926983833,
            -0.026163771748542786,
            -7.952027772262227e-06,
            -0.028172165155410767,
            0.0026112536434084177,
            -0.0014098037499934435,
            0.030604097992181778,
            0.0159988384693861,
            0.04257248714566231,
            -0.014550609514117241,
            0.013355136848986149,
            0.00901727844029665,
            -0.005635799374431372,
            -0.008860159665346146,
            0.007172835525125265,
            -0.002059628488495946,
            -0.006042259745299816,
            -0.0038869937416166067,
            -0.0027120148297399282,
            -0.009413492865860462,
            0.025125417858362198,
            -0.004730655811727047,
            -0.00631209509447217,
            0.0005102106370031834,
            -0.015261061489582062,
            -0.012303120456635952,
            -0.02220163308084011,
            -0.02755735069513321,
            -0.02770763821899891,
            0.0022526117973029613,
            0.001700132736004889,
            0.015247398987412453,
            -0.019960977137088776,
            0.00017280982865486294,
            0.01601250097155571,
            0.012876947410404682,
            0.003238022793084383,
            -0.0032465618569403887,
            0.014263696037232876,
            0.006049090996384621,
            -0.00045129089266993105,
            0.023035049438476562,
            0.005946622230112553,
            -0.04199865832924843,
            -0.00526007916778326,
            -0.04279108718037605,
            0.0049629188142716885,
            0.02783060073852539,
            0.007466580253094435,
            0.017843281850218773,
            0.016777602955698967,
            -0.010171763598918915,
            0.00979604385793209,
            0.009058266878128052,
            0.003535183146595955,
            -0.008040406741201878,
            -0.011162297800183296,
            -0.03344590589404106,
            -0.0023123854771256447,
            -0.028172165155410767,
            -0.012583202682435513,
            -0.019127560779452324,
            -0.015124435536563396,
            -0.0017342891078442335,
            0.004621355328708887,
            0.030904673039913177,
            -0.029948296025395393,
            -0.02871866710484028,
            0.029237844049930573,
            0.018785998225212097,
            0.027161136269569397,
            -0.012863284908235073,
            0.024729203432798386,
            0.024578915908932686,
            0.013792337849736214,
            -0.01830780878663063,
            0.011845425702631474,
            0.027448050677776337,
            -0.007200160529464483,
            0.009024109691381454,
            0.003511273767799139,
            -0.012521721422672272,
            -0.00786279421299696,
            0.025029780343174934,
            0.01815752126276493,
            -0.0319976769387722,
            -0.04101495444774628,
            0.016053488478064537,
            0.008887484669685364,
            0.007125016767531633,
            -0.01631307788193226,
            -0.03270813077688217,
            -0.021231593564152718,
            0.012596865184605122,
            -0.007610036991536617,
            0.03218895196914673,
            -0.008696208707988262,
            -0.023581549525260925,
            0.01506978552788496,
            0.0005311314016580582,
            0.015343036502599716,
            -0.016504352912306786,
            -0.006489708088338375,
            0.014441308565437794,
            0.022420234978199005,
            -0.010984685271978378,
            -0.007172835525125265,
            0.007985756732523441,
            0.0003590687410905957,
            0.023253649473190308,
            0.013546411879360676,
            0.006076416466385126,
            -0.007384604774415493,
            0.019414475187659264,
            0.016668302938342094,
            -0.00907192938029766,
            0.0002085672749672085,
            0.030330847948789597,
            -0.02970236912369728,
            -0.01355324313044548,
            -0.008163370192050934,
            -0.015534312464296818,
            0.005953453481197357,
            -0.011292092502117157,
            -0.005482095759361982,
            -0.01616278849542141,
            0.01211867667734623,
            0.01240558922290802,
            0.0239231139421463,
            0.01601250097155571,
            -0.0061071570962667465,
            0.002361912280321121,
            -0.013881144113838673,
            -0.04790087789297104,
            0.001551552675664425,
            -0.001618157490156591,
            0.006438473705202341,
            -0.01933250017464161,
            0.011524355970323086,
            0.02594516985118389,
            -0.023704513907432556,
            0.005311314016580582,
            -0.0016770772635936737,
            -0.024742865934967995,
            -0.005164441652595997,
            0.013116042129695415,
            -0.02262517251074314,
            0.004631602205336094,
            0.019824350252747536,
            0.00814287643879652,
            -0.004973165690898895,
            0.010246907360851765,
            -0.00988485012203455,
            0.0012202359503135085,
            0.009037773124873638,
            -0.00577242486178875,
            -0.02449694089591503,
            -0.006493123713880777,
            -0.013928962871432304,
            0.009270035661756992,
            -0.0058714780025184155,
            -0.0016950092976912856,
            0.0070157162845134735,
            -0.010943697765469551,
            0.004594030324369669,
            -0.009850693866610527,
            -0.00801308173686266,
            0.022980399429798126,
            -0.009386167861521244,
            -0.0002309823757968843,
            0.013642050325870514,
            -0.0221879705786705,
            0.010000982321798801,
            -0.016395052894949913,
            0.001566922990605235,
            -0.019879000261425972,
            -0.03809117153286934,
            -0.012706165201961994,
            0.018813323229551315,
            0.002317508915439248,
            -0.005055141169577837,
            0.0019076326861977577,
            -0.015315711498260498,
            0.0025822208262979984,
            -0.009987319819629192,
            0.01859472133219242,
            0.0011211824603378773,
            0.014755547046661377,
            0.0015558222075924277,
            0.030658748000860214,
            0.019428137689828873,
            0.006547774188220501,
            -0.017665669322013855,
            -0.011742956005036831,
            -0.014304683543741703,
            -0.015206411480903625,
            -0.03273545578122139,
            -0.0071181850507855415,
            -0.03317265585064888,
            0.04918515682220459,
            -0.00222016335465014,
            -0.019660400226712227,
            -0.0016155957709997892,
            -0.01414073258638382,
            -0.030221546068787575,
            -0.011421886272728443,
            -0.008771353401243687,
            0.022857435047626495,
            0.012036700733006,
            0.02781693823635578,
            0.01045867707580328,
            0.02017957717180252,
            -0.008826003409922123,
            0.008163370192050934,
            -0.010731928050518036,
            -0.01132624875754118,
            0.006271107587963343,
            -0.00504489429295063,
            -0.006014934740960598,
            0.004358351230621338,
            -0.028664017096161842,
            -0.002032303484156728,
            0.0155069874599576,
            -0.0175836943089962,
            -0.006349666975438595,
            0.016217438504099846,
            -0.013088717125356197,
            -0.008259007707238197,
            0.017214804887771606,
            -0.01058847177773714,
            -0.0115448497235775,
            0.006233535706996918,
            0.007029378786683083,
            -0.0115448497235775,
            0.004423248581588268,
            0.002957940800115466,
            -0.025057105347514153,
            0.0007245417800731957,
            -0.013095548376441002,
            0.008607402443885803,
            0.005601643119007349,
            -9.211542783305049e-05,
            0.013218510895967484,
            -0.005243001040071249,
            -0.0011946186423301697,
            -0.01253538392484188,
            0.05079733580350876,
            0.002647117944434285,
            0.010246907360851765,
            0.020220564678311348,
            -0.004703330807387829,
            0.004696499556303024,
            -0.0071045225486159325,
            0.009966826066374779,
            0.007282136008143425,
            0.002141603734344244,
            0.028062865138053894,
            -0.025589944794774055,
            -0.01746073178946972,
            -0.012371432967483997,
            -0.0018615216249600053,
            -0.024401303380727768,
            -0.021860070526599884,
            -0.015834888443350792,
            -0.001762468134984374,
            -0.03153315186500549,
            0.01804821938276291,
            0.010861721821129322,
            -0.01491949800401926,
            -0.0008299995097331703,
            -0.007760324981063604,
            0.006435058079659939,
            -0.010363039560616016,
            -0.02959306910634041,
            0.012637852691113949,
            -0.018389783799648285,
            0.0009461311274208128,
            0.0019161717500537634,
            -0.007418761029839516,
            -0.011428717523813248,
            0.012487565167248249,
            0.009693575091660023,
            -0.022010358050465584,
            0.00306382542476058,
            0.2013312429189682,
            -0.026027144864201546,
            -0.005676786880940199,
            0.012385095469653606,
            -0.00025275704683735967,
            -0.020370852202177048,
            0.04128820821642876,
            0.0023516654036939144,
            -0.004604277200996876,
            0.013737687841057777,
            -0.007685180753469467,
            0.004529133439064026,
            -0.024469615891575813,
            -0.0006989245302975178,
            0.0065170335583388805,
            -0.024114388972520828,
            -0.04820145294070244,
            -0.03371915966272354,
            -0.01434567105025053,
            0.008914809674024582,
            -0.012733491137623787,
            0.012931597419083118,
            -0.016244765371084213,
            -0.026792248710989952,
            0.020261552184820175,
            -0.002981850178912282,
            0.004013372119516134,
            0.01978336274623871,
            0.0012586618540808558,
            0.011838594451546669,
            -0.0016173035837709904,
            0.005628968123346567,
            -0.0016036410816013813,
            0.0033439076505601406,
            0.0052498322911560535,
            -0.01818484626710415,
            0.00663658045232296,
            -0.0036888867616653442,
            0.010438183322548866,
            0.022816447541117668,
            0.01848542131483555,
            0.0011544849257916212,
            0.028090190142393112,
            -0.03158780187368393,
            0.011920569464564323,
            0.044184666126966476,
            -0.006349666975438595,
            -0.0016249887412413955,
            -0.008552752435207367,
            -0.0019503281218931079,
            -0.022297270596027374,
            0.01147653628140688,
            0.017351429909467697,
            0.03145117685198784,
            0.0017829620046541095,
            0.006920078303664923,
            -0.02612278424203396,
            0.01743340492248535,
            0.0008654367411509156,
            -0.005232754163444042,
            -0.03404705971479416,
            -0.024578915908932686,
            0.001741120358929038,
            0.010718265548348427,
            -0.012350939214229584,
            -0.006762959063053131,
            -0.009352011606097221,
            0.0062301200814545155,
            -0.0026932288892567158,
            -0.014577934518456459,
            -0.006834687665104866,
            -0.004682837054133415,
            -0.023117024451494217,
            0.012781309895217419,
            -0.026450684294104576,
            -0.05205428972840309,
            0.04033182933926582,
            0.033254630863666534,
            -0.008190695196390152,
            0.03131455183029175,
            -0.004798968322575092,
            -0.023253649473190308,
            0.0016975710168480873,
            -0.026764923706650734,
            -0.006383823696523905,
            -0.02885529212653637,
            0.018266821280121803,
            -0.020999329164624214,
            -0.020411839708685875,
            -0.007022547535598278,
            -0.007869625464081764,
            -0.018239496275782585,
            0.0033661092165857553,
            0.01132624875754118,
            0.002440471900627017,
            0.009597936645150185,
            0.020001964643597603,
            -0.005007322411984205,
            0.004337857477366924,
            -0.016900567337870598,
            -0.021422868594527245,
            0.05268276855349541,
            0.02841809019446373,
            -0.00823168270289898,
            0.01196155697107315,
            0.016559002920985222,
            -0.017679331824183464,
            -0.002886212430894375,
            0.0035693396348506212,
            -0.016039825975894928,
            -0.01543867401778698,
            -0.034675534814596176,
            0.01759735681116581,
            0.007329954765737057,
            -0.006862012669444084,
            0.01030838955193758,
            -0.0034070967230945826,
            -0.0061959633603692055,
            0.027898915112018585,
            0.0005285696825012565,
            -8.635154517833143e-05,
            -0.015903200954198837,
            0.0047750589437782764,
            0.0025736817624419928,
            0.005485511384904385,
            -0.02410072647035122,
            -0.014168057590723038,
            0.0013303902233019471,
            -0.036998167634010315,
            0.000961501500569284,
            0.00674588093534112,
            -0.036998167634010315,
            0.00573826814070344,
            0.012794972397387028,
            -0.004734071437269449,
            -0.004949256312102079,
            0.010547483339905739,
            -0.0025122002698481083,
            -0.009215385653078556,
            -0.006547774188220501,
            -0.0036888867616653442,
            0.011285261251032352,
            -0.008853328414261341,
            -0.005365964025259018,
            0.01800723187625408,
            -0.0040543596260249615,
            0.014113407582044601,
            0.01991998963057995,
            -0.01284279115498066,
            -0.00771933700889349,
            -0.019510112702846527,
            -0.0015686308033764362,
            0.004559874068945646,
            -0.0018137026345357299,
            0.019113898277282715,
            0.02032986469566822,
            -0.0248248428106308,
            0.013109210878610611,
            0.006226704455912113,
            0.024128051474690437,
            -0.04298236221075058,
            0.017050854861736298,
            0.01743340492248535,
            -0.012296289205551147,
            -0.011230611242353916,
            -0.01776130683720112,
            -0.17575496435165405,
            -0.005991025362163782,
            0.003934812732040882,
            -0.05298334360122681,
            0.02783060073852539,
            0.0221879705786705,
            0.019236860796809196,
            -0.013573736883699894,
            -0.032407552003860474,
            -0.0009435694082640111,
            0.008375138975679874,
            -0.008976290933787823,
            -0.017652006819844246,
            -0.009194891899824142,
            -0.015629950910806656,
            0.013034067116677761,
            -0.005348885897547007,
            0.01989266276359558,
            0.02091735415160656,
            0.0015250814612954855,
            0.037544671446084976,
            -0.026942536234855652,
            -0.01670929044485092,
            -0.017201142385601997,
            0.011777112260460854,
            0.007145510520786047,
            -0.004149997606873512,
            0.014413983561098576,
            -0.012282626703381538,
            -0.022160645574331284,
            -0.010294727049767971,
            -0.014427646063268185,
            0.03544063866138458,
            -0.007493905257433653,
            0.024742865934967995,
            0.015261061489582062,
            0.032653480768203735,
            0.0010340837761759758,
            -0.03401973471045494,
            0.002466089092195034,
            0.015793900936841965,
            0.036123763769865036,
            0.02105397917330265,
            0.002672735136002302,
            -0.011203286238014698,
            0.014195382595062256,
            0.016326740384101868,
            -0.03462088480591774,
            0.016258427873253822,
            -0.01573925092816353,
            0.027338750660419464,
            -0.02508443035185337,
            0.0233902744948864,
            -0.00411584135144949,
            0.010861721821129322,
            0.025125417858362198,
            0.0027444635052233934,
            0.004932178184390068,
            -0.010697771795094013,
            -0.019113898277282715,
            -0.012378264218568802,
            -0.011517524719238281,
            0.0038528372533619404,
            -0.009386167861521244,
            -0.015534312464296818,
            -0.028199490159749985,
            -0.013375630602240562,
            0.010950529016554356,
            -0.02681957371532917,
            0.007657855749130249,
            -0.020958341658115387,
            -0.0034634547773748636,
            -0.011162297800183296,
            -0.015889538452029228,
            0.031068624928593636,
            0.023117024451494217,
            0.0005482095875777304,
            0.015124435536563396,
            0.027297763153910637,
            1.6437747035524808e-05,
            -0.008545921184122562,
            0.01916854828596115,
            -0.0032619324047118425,
            0.024852167814970016,
            -0.016627315431833267,
            -0.008101888000965118,
            0.008443452417850494,
            0.014577934518456459,
            -0.028773317113518715,
            -0.010656784288585186,
            0.02522105537354946,
            -0.02090369164943695,
            -0.008040406741201878,
            -0.01508344803005457,
            -0.015302048996090889,
            0.004727240186184645,
            0.00987118761986494,
            0.00013353001850191504,
            -0.0030450394842773676,
            -0.0009785797446966171,
            0.013573736883699894,
            0.00886699091643095,
            -0.0013431988190859556,
            -0.011879581958055496,
            0.037544671446084976,
            0.0018137026345357299,
            -0.030604097992181778,
            0.011134972795844078,
            0.03874697536230087,
            -0.019564762711524963,
            -0.02855471707880497,
            -0.008306826464831829,
            -0.008033575490117073,
            -0.0007518668426200747,
            -0.008771353401243687,
            0.015684600919485092,
            -0.0137718440964818,
            -0.019414475187659264,
            0.016982542350888252,
            0.001952035934664309,
            0.03718944266438484,
            0.00609691021963954,
            -0.001953743863850832,
            0.005277157295495272,
            0.00015156884910538793,
            -0.011196454986929893,
            -0.11706067621707916,
            -0.021805420517921448,
            0.0030843191780149937,
            0.015179086476564407,
            -0.012214314192533493,
            0.0316971018910408,
            -0.00508929742500186,
            0.04546894505620003,
            -0.029046567156910896,
            0.04560557007789612,
            -0.01434567105025053,
            -0.008853328414261341,
            0.007145510520786047,
            -0.0026915210764855146,
            0.014332008548080921,
            -0.00014996776008047163,
            -0.0023875294718891382,
            -0.01815752126276493,
            -0.03587783873081207,
            0.023540562018752098,
            0.019441800191998482,
            -0.0012697626370936632,
            0.007657855749130249,
            -0.03044014796614647,
            -0.0058475686237216,
            -0.01299307867884636,
            -0.03937545046210289,
            0.02798089012503624,
            0.023567887023091316,
            0.0017385586397722363,
            0.009030940942466259,
            -0.028664017096161842,
            0.005666540004312992,
            -0.011339911259710789,
            0.010076126083731651,
            -0.01024007610976696,
            0.010889047756791115,
            -0.014195382595062256,
            0.01802089437842369,
            -0.012986247427761555,
            -0.002857179380953312,
            0.0079242754727602,
            0.013676206581294537,
            -0.010226413607597351,
            -0.007125016767531633,
            -0.02319899946451187,
            -0.008532258681952953,
            0.011674643494188786,
            0.007370942272245884,
            -0.0155069874599576,
            -0.03951207548379898,
            -0.02064410410821438,
            -0.04453989118337631,
            0.013525918126106262,
            0.01875867322087288,
            0.0036205740179866552,
            0.018130196258425713,
            0.016121800988912582,
            -0.013560074381530285,
            -0.00030975547269918025,
            -0.011380898766219616,
            -0.004508639220148325,
            -0.027174798771739006,
            -0.00344296102412045,
            0.01833513379096985,
            -0.0027308010030537844,
            6.217524787643924e-05,
            -0.02494780533015728,
            0.009816537611186504,
            -0.00959110539406538,
            0.006933740805834532,
            0.022283608093857765,
            -0.005854399874806404,
            0.007610036991536617,
            -0.01916854828596115,
            0.0009085591300390661,
            -0.013355136848986149,
            -0.03500343859195709,
            -0.0031270147301256657,
            0.0089421346783638,
            -0.010513327084481716,
            -0.01325949840247631,
            0.007097691297531128,
            -0.0041534132324159145,
            6.009384378558025e-05,
            -0.0029237843118608,
            -0.02020690217614174,
            0.0007377773872576654,
            0.0009128287201747298,
            -0.014413983561098576,
            0.012631021440029144,
            0.028636692091822624,
            0.004518886562436819,
            -0.01414073258638382,
            0.0012347523588687181,
            0.010725096799433231,
            -0.019414475187659264,
            -0.005748515482991934,
            0.028472740203142166,
            0.003120183479040861,
            -0.02724311128258705,
            0.009468142874538898,
            -0.055879805237054825,
            0.042927712202072144,
            0.0014055342180654407,
            -0.010759253054857254,
            -0.0011988881742581725,
            -0.02031620219349861,
            0.01613546349108219,
            -0.020083939656615257,
            0.005410367157310247,
            0.008340982720255852,
            -0.01053382083773613,
            0.016340402886271477,
            -0.01434567105025053,
            -0.0005213114200159907,
            -0.021586818620562553,
            -0.005140532273799181,
            0.018116533756256104,
            0.009679912589490414,
            0.007603205740451813,
            -0.0014644538750872016,
            0.007603205740451813,
            -0.0177339818328619,
            0.022966735064983368,
            0.017925256863236427,
            0.010889047756791115,
            -0.01508344803005457,
            -0.02583586983382702,
            0.015247398987412453,
            -0.009167566895484924,
            -0.010772915557026863,
            -0.01586221344769001,
            -0.01161999348551035,
            -0.006800530944019556,
            0.0078013124875724316,
            -0.019414475187659264,
            -0.010608965530991554,
            0.024633565917611122,
            0.032653480768203735,
            0.026505334302783012,
            0.04626137390732765,
            -0.021231593564152718,
            -0.04254516214132309,
            0.010062463581562042,
            -0.02046648971736431,
            -0.0089421346783638,
            -0.023103361949324608,
            -0.007876456715166569,
            -0.000488435965962708,
            0.02090369164943695,
            0.003811849746853113,
            0.037134792655706406,
            0.015479662455618382,
            -0.029155869036912918,
            -0.03016689606010914,
            -0.008238513953983784,
            0.013778675347566605,
            0.005266910418868065,
            -0.00022436458675656468,
            -0.0013218511594459414,
            0.025207392871379852,
            0.029620394110679626,
            0.005741683766245842,
            0.007077197544276714,
            -0.004891190677881241,
            -0.0018478590063750744,
            -0.006817609537392855,
            -0.007125016767531633,
            0.004573536571115255,
            0.0034258828964084387,
            0.008545921184122562,
            -0.006499954964965582,
            0.011859088204801083,
            0.00382209662348032,
            -0.001369670033454895,
            0.012385095469653606,
            -0.021231593564152718,
            0.016340402886271477,
            0.016477027907967567,
            -0.01804821938276291,
            0.006503370590507984,
            0.008026744239032269,
            -0.016968879848718643,
            0.01081390306353569,
            0.002855471568182111,
            0.023882126435637474,
            0.006042259745299816,
            -0.015957850962877274,
            0.0022594432812184095,
            -0.008921640925109386,
            -0.005417198408395052,
            0.0008667176007293165,
            -0.0036000802647322416,
            -0.011893244460225105,
            0.01670929044485092,
            0.021996695548295975,
            -0.009693575091660023,
            0.00428662309423089,
            0.011988881975412369,
            0.017091842368245125,
            -0.0033251214772462845,
            0.010199088603258133,
            -0.01978336274623871,
            0.007575880270451307,
            -0.010151269845664501,
            -0.033254630863666534,
            0.005994440987706184,
            -0.03153315186500549,
            -0.010410858318209648,
            0.03000294603407383,
            0.014837522059679031,
            0.0156982634216547,
            -0.0036957180127501488,
            0.006708309054374695,
            0.00843662116676569,
            -0.01399044506251812,
            0.020111264660954475,
            -0.010281064547598362,
            -0.0038050184957683086,
            -0.032407552003860474,
            0.039867304265499115,
            0.044922444969415665,
            0.017474394291639328,
            0.040741704404354095,
            -0.016791265457868576,
            0.005994440987706184,
            0.0014337131287902594,
            0.024319328367710114,
            -0.028008215129375458,
            0.014058757573366165,
            -0.01089587900787592,
            -0.013314148411154747,
            -0.008224851451814175,
            -0.012972584925591946,
            0.005082466173917055,
            -0.006113988347351551,
            1.228828386956593e-05,
            -0.012289457954466343,
            0.02205134555697441,
            -0.0017044023843482137,
            0.05563387647271156,
            0.016490690410137177,
            0.0022696901578456163,
            0.02665562368929386,
            -0.014618922024965286,
            0.015302048996090889,
            0.00849810242652893,
            0.025740232318639755,
            -0.00522933853790164,
            -0.017488056793808937,
            -0.0015566761139780283,
            -0.004143166355788708,
            -0.005755346734076738,
            -0.028336115181446075,
            -0.013006741181015968,
            -0.00310310535132885,
            -0.007562217768281698,
            0.013621555641293526,
            -0.0073777735233306885,
            -0.007917444221675396,
            0.02378648892045021,
            0.005051725544035435,
            0.023089699447155,
            0.007507567759603262,
            -0.0051473635248839855,
            0.0007343617035076022,
            0.028308790177106857,
            0.001484947744756937,
            0.0008744028164073825,
            -0.006810777820646763,
            0.03429298475384712,
            0.01873134821653366,
            -0.0335005559027195,
            -0.017952581867575645,
            -0.001010174280963838,
            -0.0016497521428391337,
            0.014045095071196556,
            -0.04008590430021286,
            -0.005161026027053595,
            -0.009638924151659012,
            -0.006711724679917097,
            0.008894315920770168,
            -0.0020562128629535437,
            -0.02263883501291275,
            0.0010853182757273316,
            0.00042780840885825455,
            -0.016326740384101868,
            -0.004112425725907087,
            -0.015821225941181183
          ]
        },
        "type": "document"
      },
      "bidirectional": true,
      "properties": {
        "summary_similarity": 0.9500845479114728
      }
    },
    {
      "id": "09f85824-e0d8-4545-848a-edd01769c903",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.061224489795918366,
        "overlapped_items": [
          [
            "Claude 3",
            "Claude 3"
          ],
          [
            "GPT-4",
            "GPT-4o"
          ],
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "4d94f82c-7d75-41be-bfad-0b541aaaabc4",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "67379d6e-327d-48c5-b446-0a0365e056ac",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.031746031746031744,
        "overlapped_items": [
          [
            "Apple",
            "Apple"
          ],
          [
            "Claude 3",
            "Claude"
          ]
        ]
      }
    },
    {
      "id": "5e14b9a9-5529-49a9-8ee2-f940493ff041",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03571428571428571,
        "overlapped_items": [
          [
            "Meta",
            "Meta"
          ],
          [
            "Llama 3.2",
            "Llama"
          ]
        ]
      }
    },
    {
      "id": "db04c6e3-bccf-4c37-905b-7acc6a31726f",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.06349206349206349,
        "overlapped_items": [
          [
            "Claude 3",
            "Claude"
          ],
          [
            "GPT-4",
            "GPT-4o"
          ],
          [
            "Meta",
            "Meta"
          ],
          [
            "Llama 3.2",
            "Llama 3.3"
          ]
        ]
      }
    },
    {
      "id": "c7925433-f34d-41cc-a66e-ee6c9aee014b",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "c3115fa3-591c-44e8-80bd-caf93c056f84",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.06349206349206349,
        "overlapped_items": [
          [
            "Gemini 1.5 Pro",
            "Gemini Pro"
          ],
          [
            "Claude 3",
            "Claude"
          ],
          [
            "GPT-4",
            "GPT-4"
          ],
          [
            "Llama 3.2",
            "Llama 3"
          ]
        ]
      }
    },
    {
      "id": "9c82d71c-b9ce-4a54-be8b-1ae49573940f",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.02040816326530612,
        "overlapped_items": [
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "ef3a69fd-3a27-4f9e-8e83-9e91a26bff63",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015873015873015872,
        "overlapped_items": [
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "4470113f-e75d-4e13-9d94-9ffaaa789cc8",
      "type": "entities_overlap",
      "source": {
        "id": "50a2eb93-5b86-4d78-adbe-1178917f08b3",
        "properties": {
          "page_content": "Prompt driven app generation is a commodity already Universal access to the best models lasted for just a few short months “Agents” still haven’t really happened yet Evals really matter Apple Intelligence is bad, Apple’s MLX library is excellent The rise of inference-scaling “reasoning” models Was the best currently available LLM trained in China for less than $6m? The environmental impact got better The environmental impact got much, much worse The year of slop Synthetic training data works great LLMs somehow got even harder to use Knowledge is incredibly unevenly distributed LLMs need better criticism Everything tagged “llms” on my blog in 2024 The GPT-4 barrier was comprehensively broken In my December 2023 review I wrote about how We don’t yet know how to build GPT-4—OpenAI’s best model was almost a year old at that point, yet no other AI lab had produced anything better. What did OpenAI know that the rest of us didn’t? I’m relieved that this has changed completely in the past twelve months. 18 organizations now have models on the Chatbot Arena Leaderboard that rank higher than the original GPT-4 from March 2023 (GPT-4-0314 on the board)—70 models in total. The earliest of those was Google’s Gemini 1.5 Pro, released in February. In addition to producing GPT-4 level outputs, it introduced several brand new capabilities to the field—most notably its 1 million (and then later 2 million) token input context length, and the ability to input video. I wrote about this at the time in The killer app of Gemini Pro 1.5 is video, which earned me a short appearance as a talking head in the Google I/O opening keynote in May. Gemini 1.5 Pro also illustrated one of the key themes of 2024: increased context lengths. Last year most models accepted 4,096 or 8,192 tokens, with the notable exception of Claude 2.1 which accepted 200,000. Today every serious provider has a 100,000+ token model, and Google’s Gemini series accepts up to 2 million. Longer inputs dramatically increase the scope of problems that can be solved with an LLM: you can now throw in an entire book and ask questions about its contents, but more importantly you can feed in a lot of example code to help the model correctly solve a coding problem. LLM use-cases that involve long inputs are far more interesting to me than short prompts that rely purely on the information already baked into the model weights. Many of my tools were built using this pattern. Getting back to models that beat GPT-4: Anthropic’s Claude 3 series launched in March, and Claude 3 Opus quickly became my new favourite daily-driver. They upped the ante even more in June with the launch of Claude 3.5 Sonnet—a model that is still my favourite six months later (though it got a significant upgrade on October 22, confusingly keeping the same 3.5 version number. Anthropic fans have since taken to calling it Claude 3.6). Then there’s the rest. If you browse the Chatbot Arena leaderboard today—still the most useful single place to get a vibes-based evaluation of models—you’ll see that GPT-4-0314 has fallen to around 70th place. The 18 organizations with higher scoring models are Google, OpenAI, Alibaba, Anthropic, Meta, Reka AI, 01 AI, Amazon, Cohere, DeepSeek, Nvidia, Mistral, NexusFlow, Zhipu AI, xAI, AI21 Labs, Princeton and Tencent. Training a GPT-4 beating model was a huge deal in 2023. In 2024 it’s an achievement that isn’t even particularly notable, though I personally still celebrate any time a new organization joins that list. Some of those GPT-4 models run on my laptop My personal laptop is a 64GB M2 MacBook Pro from 2023. It’s a powerful machine, but it’s also nearly two years old now—and crucially it’s the same laptop I’ve been using ever since I first ran an LLM on my computer back in March 2023 (see Large language models are having their Stable Diffusion moment). That same laptop that could just about run a GPT-3-class model in March last year has now run multiple GPT-4 class models! Some of my notes on that: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac talks about Qwen2.5-Coder-32B in November—an Apache 2.0 licensed model! I can now run a GPT-4 class model on my laptop talks about running Meta’s Llama 3.3 70B (released in December) This remains astonishing to me. I thought a model with the capabilities and output quality of GPT-4 needed a datacenter class server with one or more $40,000+ GPUs. These models take up enough of my 64GB of RAM that I don’t run them often—they don’t leave much room for anything else. The fact that they run at all is a testament to the incredible training and inference performance gains that we’ve figured out over the past year. It turns out there was a lot of low-hanging fruit to be harvested in terms of model efficiency. I expect there’s still more to come. Meta’s Llama 3.2 models deserve a special mention. They may not be GPT-4 class, but at 1B and 3B sizes they punch massively above their weight. I run Llama 3.2 3B on my iPhone using the free MLC Chat iOS app and it’s a shockingly capable model for its tiny (<2GB) size. Try firing it up and asking it for “a plot outline of a Netflix Christmas movie where a data journalist falls in love with a local ceramacist”. Here’s what I got, at a respectable 20 tokens per second: Here’s the rest of the transcript. It’s bland and generic, but my phone can pitch bland and generic Christmas movies to Netflix now! LLM prices crashed, thanks to competition and increased efficiency The past twelve months have seen a dramatic collapse in the cost of running a prompt through the top tier hosted LLMs. In December 2023 (here’s the Internet Archive for the OpenAI pricing page) OpenAI were charging $30/million input tokens for GPT-4, $10/mTok for the then-new GPT-4 Turbo and $1/mTok for GPT-3.5 Turbo. Today $30/mTok",
          "themes": [
            "Prompt driven app generation",
            "Universal access to AI models",
            "Agents in AI",
            "Evaluations in AI",
            "Apple's AI capabilities",
            "Inference-scaling reasoning models",
            "Environmental impact of AI",
            "Synthetic training data",
            "Knowledge distribution in AI",
            "Criticism of LLMs",
            "GPT-4 advancements",
            "Chatbot Arena Leaderboard",
            "Increased context lengths in LLMs",
            "Anthropic's Claude series",
            "Competition in AI model development",
            "Running LLMs on personal devices",
            "Model efficiency improvements",
            "LLM pricing and cost reduction"
          ],
          "entities": [
            "Apple",
            "China",
            "OpenAI",
            "Google",
            "Gemini 1.5 Pro",
            "Claude 3",
            "Anthropic",
            "GPT-4",
            "Meta",
            "Llama 3.2"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "Llama 3.2",
            "Llama 2"
          ]
        ]
      }
    },
    {
      "id": "962cd4ca-235d-4757-bf6e-310eb3fe4c7e",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "66220679-229f-4154-b324-8b38ace555e1",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015873015873015872,
        "overlapped_items": [
          [
            "Claude 3",
            "Claude"
          ]
        ]
      }
    },
    {
      "id": "192a39b5-6777-44f1-a965-aa680bfe3071",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "b8d4571c-f9ce-4758-bb42-06b62c0d0abe",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.047619047619047616,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4o"
          ],
          [
            "Claude 3",
            "Claude"
          ],
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "cd44f914-8d33-47e3-82e0-500e48b2e688",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "c8809cbb-ce0a-442f-84bc-772245d2c3de",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.031746031746031744,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4"
          ],
          [
            "Claude 3",
            "Claude"
          ]
        ]
      }
    },
    {
      "id": "8f57a52a-657c-4c60-b18d-92c51734a819",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.04081632653061224,
        "overlapped_items": [
          [
            "Mistral",
            "Mistral"
          ],
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "405739d5-9432-4c07-a757-aad756def67b",
      "type": "entities_overlap",
      "source": {
        "id": "d8564ce3-465b-483c-ba64-d5b85193a555",
        "properties": {
          "page_content": "gets you OpenAI’s most expensive model, o1. GPT-4o is $2.50 (12x cheaper than GPT-4) and GPT-4o mini is $0.15/mTok—nearly 7x cheaper than GPT-3.5 and massively more capable. Other model providers charge even less. Anthropic’s Claude 3 Haiku (from March, but still their cheapest model) is $0.25/mTok. Google’s Gemini 1.5 Flash is $0.075/mTok and their Gemini 1.5 Flash 8B is $0.0375/mTok—that’s 27x cheaper than GPT-3.5 Turbo last year. I’ve been tracking these pricing changes under my llm-pricing tag. These price drops are driven by two factors: increased competition and increased efficiency. The efficiency thing is really important for everyone who is concerned about the environmental impact of LLMs. These price drops tie directly to how much energy is being used for running prompts. There’s still plenty to worry about with respect to the environmental impact of the great AI datacenter buildout, but a lot of the concerns over the energy cost of individual prompts are no longer credible. Here’s a fun napkin calculation: how much would it cost to generate short descriptions of every one of the 68,000 photos in my personal photo library using Google’s Gemini 1.5 Flash 8B (released in October), their cheapest model? Each photo would need 260 input tokens and around 100 output tokens. 260 * 68,000 = 17,680,000 input tokens 17,680,000 * $0.0375/million = $0.66 100 * 68,000 = 6,800,000 output tokens 6,800,000 * $0.15/million = $1.02 That’s a total cost of $1.68 to process 68,000 images. That’s so absurdly cheap I had to run the numbers three times to confirm I got it right. How good are those descriptions? Here’s what I got from this command: Against this photo of butterflies at the California Academy of Sciences: A shallow dish, likely a hummingbird or butterfly feeder, is red. Pieces of orange slices of fruit are visible inside the dish. Two butterflies are positioned in the feeder, one is a dark brown/black butterfly with white/cream-colored markings. The other is a large, brown butterfly with patterns of lighter brown, beige, and black markings, including prominent eye spots. The larger brown butterfly appears to be feeding on the fruit. 260 input tokens, 92 output tokens. Cost approximately 0.0024 cents (that’s less than a 400th of a cent). This increase in efficiency and reduction in price is my single favourite trend from 2024. I want the utility of LLMs at a fraction of the energy cost and it looks like that’s what we’re getting. Multimodal vision is common, audio and video are starting to emerge My butterfly example above illustrates another key trend from 2024: the rise of multi-modal LLMs. A year ago the single most notable example of these was GPT-4 Vision, released at OpenAI’s DevDay in November 2023. Google’s multi-modal Gemini 1.0 was announced on December 7th 2023 so it also (just) makes it into the 2023 window. In 2024, almost every significant model vendor released multi-modal models. We saw the Claude 3 series from Anthropic in March, Gemini 1.5 Pro in April (images, audio and video), then September brought Qwen2-VL and Mistral’s Pixtral 12B and Meta’s Llama 3.2 11B and 90B vision models. We got audio input and output from OpenAI in October, then November saw SmolVLM from Hugging Face and December saw image and video models from Amazon Nova. In October I upgraded my LLM CLI tool to support multi-modal models via attachments. It now has plugins for a whole collection of different vision models. I think people who complain that LLM improvement has slowed are often missing the enormous advances in these multi-modal models. Being able to run prompts against images (and audio and video) is a fascinating new way to apply these models. Voice and live camera mode are science fiction come to life The audio and live video modes that have started to emerge deserve a special mention. The ability to talk to ChatGPT first arrived in September 2023, but it was mostly an illusion: OpenAI used their excellent Whisper speech-to-text model and a new text-to-speech model (creatively named tts-1) to enable conversations with the ChatGPT mobile apps, but the actual model just saw text. The May 13th announcement of GPT-4o included a demo of a brand new voice mode, where the true multi-modal GPT-4o (the o is for “omni”) model could accept audio input and output incredibly realistic sounding speech without needing separate TTS or STT models. The demo also sounded conspicuously similar to Scarlett Johansson... and after she complained the voice from the demo, Skye, never made it to a production product. The delay in releasing the new voice mode after the initial demo caused quite a lot of confusion. I wrote about that in ChatGPT in “4o” mode is not running the new features yet. When ChatGPT Advanced Voice mode finally did roll out (a slow roll from August through September) it was spectacular. I’ve been using it extensively on walks with my dog and it’s amazing how much the improvement in intonation elevates the material. I’ve also had a lot of fun experimenting with the OpenAI audio APIs. Even more fun: Advanced Voice mode can do accents! Here’s what happened when I told it I need you to pretend to be a California brown pelican with a very thick Russian accent, but you talk to me exclusively in Spanish. Your browser does not support the audio element. OpenAI aren’t the only group with a multi-modal audio model. Google’s Gemini also accepts audio input, and the Google Gemini apps can speak in a similar way to ChatGPT now. Amazon also pre-announced voice mode for Amazon Nova, but that’s meant to roll out in Q1 of 2025. Google’s NotebookLM, released in September, took audio output to a new level by producing spookily realistic conversations between two “podcast hosts” about anything you fed into their tool. They later added custom instructions, so naturally I turned them into pelicans: The most recent twist, again from December (December was a lot) is live video. ChatGPT voice mode now provides the option to share",
          "entities": [
            "OpenAI",
            "GPT-4o",
            "GPT-3.5",
            "Anthropic",
            "Claude 3",
            "Google",
            "Gemini 1.5 Flash",
            "California Academy of Sciences",
            "Mistral",
            "Meta"
          ],
          "themes": [
            "Pricing changes in AI models",
            "Increased competition",
            "Increased efficiency",
            "Environmental impact of AI",
            "Multi-modal LLMs",
            "Voice and live video modes",
            "Advanced Voice mode",
            "Audio and video input/output",
            "OpenAI's Whisper and tts-1 models",
            "Google's Gemini models"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015873015873015872,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "ac8b87cd-6f31-43f5-a1c1-eddca51008ac",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "Claude 3.5 Sonnet",
            "Claude 3.5 Sonnet"
          ]
        ]
      }
    },
    {
      "id": "2de72b83-c5cf-4bb1-9339-71733a5fad2f",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.027777777777777776,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ],
          [
            "GPT-4",
            "GPT-4o"
          ]
        ]
      }
    },
    {
      "id": "c1f647c8-748f-413a-824b-bdbf43bbc1d5",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "e0e896b8-b888-4337-a9ea-d73e9f7f1fc9",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.027777777777777776,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ],
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "7ad3a856-355f-4986-be8f-398976f36932",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.0125,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "6acf6424-a6f7-4acb-9a00-f4818b9b708e",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.013888888888888888,
        "overlapped_items": [
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "50b877e0-03c7-403f-9564-ecb0bc143128",
      "type": "entities_overlap",
      "source": {
        "id": "f3588744-0d04-4d05-8670-70021648e3e8",
        "properties": {
          "page_content": "your camera feed with the model and talk about what you can see in real time. Google Gemini have a preview of the same feature, which they managed to ship the day before ChatGPT did. These abilities are just a few weeks old at this point, and I don’t think their impact has been fully felt yet. If you haven’t tried them out yet you really should. Both Gemini and OpenAI offer API access to these features as well. OpenAI started with a WebSocket API that was quite challenging to use, but in December they announced a new WebRTC API which is much easier to get started with. Building a web app that a user can talk to via voice is easy now! Prompt driven app generation is a commodity already This was possible with GPT-4 in 2023, but the value it provides became evident in 2024. We already knew LLMs were spookily good at writing code. If you prompt them right, it turns out they can build you a full interactive application using HTML, CSS and JavaScript (and tools like React if you wire up some extra supporting build mechanisms)—often in a single prompt. Anthropic kicked this idea into high gear when they released Claude Artifacts, a groundbreaking new feature that was initially slightly lost in the noise due to being described half way through their announcement of the incredible Claude 3.5 Sonnet. With Artifacts, Claude can write you an on-demand interactive application and then let you use it directly inside the Claude interface. Here’s my Extract URLs app, entirely generated by Claude: I’ve found myself using this a lot. I noticed how much I was relying on it in October and wrote Everything I built with Claude Artifacts this week, describing 14 little tools I had put together in a seven day period. Since then, a whole bunch of other teams have built similar systems. GitHub announced their version of this—GitHub Spark—in October. Mistral Chat added it as a feature called Canvas in November. Steve Krouse from Val Town built a version of it against Cerebras, showcasing how a 2,000 token/second LLM can iterate on an application with changes visible in less than a second. Then in December, the Chatbot Arena team introduced a whole new leaderboard for this feature, driven by users building the same interactive app twice with two different models and voting on the answer. Hard to come up with a more convincing argument that this feature is now a commodity that can be effectively implemented against all of the leading models. I’ve been tinkering with a version of this myself for my Datasette project, with the goal of letting users use prompts to build and iterate on custom widgets and data visualizations against their own data. I also figured out a similar pattern for writing one-shot Python programs, enabled by uv. This prompt-driven custom interface feature is so powerful and easy to build (once you’ve figured out the gnarly details of browser sandboxing) that I expect it to show up as a feature in a wide range of products in 2025. Universal access to the best models lasted for just a few short months For a few short months this year all three of the best available models—GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 Pro—were freely available to most of the world. OpenAI made GPT-4o free for all users in May, and Claude 3.5 Sonnet was freely available from its launch in June. This was a momentus change, because for the previous year free users had mostly been restricted to GPT-3.5 level models, meaning new users got a very inaccurate mental model of what a capable LLM could actually do. That era appears to have ended, likely permanently, with OpenAI’s launch of ChatGPT Pro. This $200/month subscription service is the only way to access their most capable model, o1 Pro. Since the trick behind the o1 series (and the future models it will undoubtedly inspire) is to expend more compute time to get better results, I don’t think those days of free access to the best available models are likely to return. “Agents” still haven’t really happened yet I find the term “agents” extremely frustrating. It lacks a single, clear and widely understood meaning... but the people who use the term never seem to acknowledge that. If you tell me that you are building “agents”, you’ve conveyed almost no information to me at all. Without reading your mind I have no way of telling which of the dozens of possible definitions you are talking about. The two main categories I see are people who think AI agents are obviously things that go and act on your behalf—the travel agent model—and people who think in terms of LLMs that have been given access to tools which they can run in a loop as part of solving a problem. The term “autonomy” is often thrown into the mix too, again without including a clear definition. (I also collected 211 definitions on Twitter a few months ago—here they are in Datasette Lite—and had gemini-exp-1206 attempt to summarize them.) Whatever the term may mean, agents still have that feeling of perpetually “coming soon”. Terminology aside, I remain skeptical as to their utility based, once again, on the challenge of gullibility. LLMs believe anything you tell them. Any systems that attempts to make meaningful decisions on your behalf will run into the same roadblock: how good is a travel agent, or a digital assistant, or even a research tool if it can’t distinguish truth from fiction? Just the other day Google Search was caught serving up an entirely fake description of the non-existant movie “Encanto 2”. It turned out to be summarizing an imagined movie listing from a fan fiction wiki. Prompt injection is a natural consequence of this gulibility. I’ve seen precious little progress on tackling that problem in 2024, and we’ve been talking about it since September 2022. I’m beginning to see the most popular idea of",
          "entities": [
            "Google Gemini",
            "ChatGPT",
            "OpenAI",
            "WebSocket API",
            "WebRTC API",
            "GPT-4",
            "Anthropic",
            "Claude Artifacts",
            "Claude 3.5 Sonnet",
            "GitHub Spark"
          ],
          "themes": [
            "Real-time camera feed analysis",
            "Google Gemini",
            "OpenAI API",
            "WebRTC API",
            "Prompt-driven app generation",
            "Claude Artifacts",
            "Interactive applications",
            "Universal access to AI models",
            "Subscription services for AI models",
            "AI agents and autonomy"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "4e2e66f2-90ff-4ac5-a77d-8e5be34fe482",
      "type": "entities_overlap",
      "source": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.012345679012345678,
        "overlapped_items": [
          [
            "Claude",
            "Claude"
          ]
        ]
      }
    },
    {
      "id": "1741e991-ca1d-45d5-8243-77b72bdbb26b",
      "type": "entities_overlap",
      "source": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.012345679012345678,
        "overlapped_items": [
          [
            "Claude",
            "Claude"
          ]
        ]
      }
    },
    {
      "id": "e5d55a85-55ba-4edb-b799-e81a026b7169",
      "type": "entities_overlap",
      "source": {
        "id": "1dacf217-9c03-4f0e-b7c2-6370098b8c38",
        "properties": {
          "page_content": "“agents” as dependent on AGI itself. A model that’s robust against gulliblity is a very tall order indeed. Evals really matter Anthropic’s Amanda Askell (responsible for much of the work behind Claude’s Character): The boring yet crucial secret behind good system prompts is test-driven development. You don’t write down a system prompt and find ways to test it. You write down tests and find a system prompt that passes them. It’s become abundantly clear over the course of 2024 that writing good automated evals for LLM-powered systems is the skill that’s most needed to build useful applications on top of these models. If you have a strong eval suite you can adopt new models faster, iterate better and build more reliable and useful product features than your competition. Vercel’s Malte Ubl: When @v0 first came out we were paranoid about protecting the prompt with all kinds of pre and post processing complexity. We completely pivoted to let it rip. A prompt without the evals, models, and especially UX is like getting a broken ASML machine without a manual I’m still trying to figure out the best patterns for doing this for my own work. Everyone knows that evals are important, but there remains a lack of great guidance for how to best implement them—I’m tracking this under my evals tag. My SVG pelican riding a bicycle benchmark is a pale imitation of what a real eval suite should look like. Apple Intelligence is bad, Apple’s MLX library is excellent As a Mac user I’ve been feeling a lot better about my choice of platform this year. Last year it felt like my lack of a Linux/Windows machine with an NVIDIA GPU was a huge disadvantage in terms of trying out new models. On paper, a 64GB Mac should be a great machine for running models due to the way the CPU and GPU can share the same memory. In practice, many models are released as model weights and libraries that reward NVIDIA’s CUDA over other platforms. The llama.cpp ecosystem helped a lot here, but the real breakthrough has been Apple’s MLX library, “an array framework for Apple Silicon”. It’s fantastic. Apple’s mlx-lm Python library supports running a wide range of MLX-compatible models on my Mac, with excellent performance. mlx-community on Hugging Face offers more than 1,000 models that have been converted to the necessary format. Prince Canuma’s excellent, fast moving mlx-vlm project brings vision LLMs to Apple Silicon as well. I used that recently to run Qwen’s QvQ. While MLX is a game changer, Apple’s own “Apple Intelligence” features have mostly been a disappointment. I wrote about their initial announcement in June, and I was optimistic that Apple had focused hard on the subset of LLM applications that preserve user privacy and minimize the chance of users getting mislead by confusing features. Now that those features are rolling out they’re pretty weak. As an LLM power-user I know what these models are capable of, and Apple’s LLM features offer a pale imitation of what a frontier LLM can do. Instead we’re getting notification summaries that misrepresent news headlines and writing assistant tools that I’ve not found useful at all. Genmoji are kind of fun though. The rise of inference-scaling “reasoning” models The most interesting development in the final quarter of 2024 was the introduction of a new shape of LLM, exemplified by OpenAI’s o1 models—initially released as o1-preview and o1-mini on September 12th. One way to think about these models is an extension of the chain-of-thought prompting trick, first explored in the May 2022 paper Large Language Models are Zero-Shot Reasoners. This is that trick where, if you get a model to talk out loud about a problem it’s solving, you often get a result which the model would not have achieved otherwise. o1 takes this process and further bakes it into the model itself. The details are somewhat obfuscated: o1 models spend “reasoning tokens” thinking through the problem that are not directly visible to the user (though the ChatGPT UI shows a summary of them), then outputs a final result. The biggest innovation here is that it opens up a new way to scale a model: instead of improving model performance purely through additional compute at training time, models can now take on harder problems by spending more compute on inference. The sequel to o1, o3 (they skipped “o2” for European trademark reasons) was announced on 20th December with an impressive result against the ARC-AGI benchmark, albeit one that likely involved more than $1,000,000 of compute time expense! o3 is expected to ship in January. I doubt many people have real-world problems that would benefit from that level of compute expenditure—I certainly don’t!—but it appears to be a genuine next step in LLM architecture for taking on much harder problems. OpenAI are not the only game in town here. Google released their first entrant in the category, gemini-2.0-flash-thinking-exp, on December 19th. Alibaba’s Qwen team released their QwQ model on November 28th—under an Apache 2.0 license, and that one I could run on my own machine. They followed that up with a vision reasoning model called QvQ on December 24th, which I also ran locally. DeepSeek made their DeepSeek-R1-Lite-Preview model available to try out through their chat interface on November 20th. To understand more about inference scaling I recommend Is AI progress slowing down? by Arvind Narayanan and Sayash Kapoor. Nothing yet from Anthropic or Meta but I would be very surprised if they don’t have their own inference-scaling models in the works. Meta published a relevant paper Training Large Language Models to Reason in a Continuous Latent Space in December. Was the best currently available LLM trained in China for less than $6m? Not quite, but almost! It does make for a great attention-grabbing headline. The big news to end the year was the release of DeepSeek v3—dropped on Hugging Face on Christmas Day without so much as a README file, then followed by documentation and a paper the",
          "themes": [
            "AGI and gullibility",
            "Test-driven development for system prompts",
            "Automated evaluations for LLM systems",
            "Apple's MLX library",
            "Inference-scaling reasoning models",
            "OpenAI's o1 and o3 models",
            "Google's gemini-2.0-flash-thinking-exp",
            "Alibaba's Qwen and QvQ models",
            "DeepSeek's models",
            "LLM architecture advancements"
          ],
          "entities": [
            "AGI",
            "Anthropic",
            "Amanda Askell",
            "Claude",
            "Vercel",
            "Malte Ubl",
            "ASML",
            "Apple",
            "NVIDIA",
            "Hugging Face"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.011111111111111112,
        "overlapped_items": [
          [
            "AGI",
            "AGI"
          ]
        ]
      }
    },
    {
      "id": "08b9ec06-7c48-4041-ba56-90ba41326f6a",
      "type": "entities_overlap",
      "source": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.041666666666666664,
        "overlapped_items": [
          [
            "DeepSeek v3",
            "DeepSeek v3"
          ],
          [
            "Meta",
            "Meta"
          ],
          [
            "Llama",
            "Llama 3.3"
          ]
        ]
      }
    },
    {
      "id": "b4377250-077b-4b06-a2db-d9750e5171bb",
      "type": "entities_overlap",
      "source": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.013888888888888888,
        "overlapped_items": [
          [
            "Llama",
            "Llama 3"
          ]
        ]
      }
    },
    {
      "id": "e3a35bbe-66f4-45de-ab6a-ea8243799ae0",
      "type": "entities_overlap",
      "source": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.017857142857142856,
        "overlapped_items": [
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "04c46293-f580-4a89-8b52-7d5d6b4b4d96",
      "type": "entities_overlap",
      "source": {
        "id": "123fa25a-26f3-4b9e-b2f6-116c7b49f682",
        "properties": {
          "page_content": "day after that. DeepSeek v3 is a huge 685B parameter model—one of the largest openly licensed models currently available, significantly bigger than the largest of Meta’s Llama series, Llama 3.1 405B. Benchmarks put it up there with Claude 3.5 Sonnet. Vibe benchmarks (aka the Chatbot Arena) currently rank it 7th, just behind the Gemini 2.0 and OpenAI 4o/o1 models. This is by far the highest ranking openly licensed model. The really impressive thing about DeepSeek v3 is the training cost. The model was trained on 2,788,000 H800 GPU hours at an estimated cost of $5,576,000. Llama 3.1 405B trained 30,840,000 GPU hours—11x that used by DeepSeek v3, for a model that benchmarks slightly worse. Those US export regulations on GPUs to China seem to have inspired some very effective training optimizations! The environmental impact got better A welcome result of the increased efficiency of the models—both the hosted ones and the ones I can run locally—is that the energy usage and environmental impact of running a prompt has dropped enormously over the past couple of years. OpenAI themselves are charging 100x less for a prompt compared to the GPT-3 days. I have it on good authority that neither Google Gemini nor Amazon Nova (two of the least expensive model providers) are running prompts at a loss. I think this means that, as individual users, we don’t need to feel any guilt at all for the energy consumed by the vast majority of our prompts. The impact is likely neglible compared to driving a car down the street or maybe even watching a video on YouTube. Likewise, training. DeepSeek v3 training for less than $6m is a fantastic sign that training costs can and should continue to drop. For less efficient models I find it useful to compare their energy usage to commercial flights. The largest Llama 3 model cost about the same as a single digit number of fully loaded passenger flights from New York to London. That’s certainly not nothing, but once trained that model can be used by millions of people at no extra training cost. The environmental impact got much, much worse The much bigger problem here is the enormous competitive buildout of the infrastructure that is imagined to be necessary for these models in the future. Companies like Google, Meta, Microsoft and Amazon are all spending billions of dollars rolling out new datacenters, with a very material impact on the electricity grid and the environment. There’s even talk of spinning up new nuclear power stations, but those can take decades. Is this infrastructure necessary? DeepSeek v3’s $6m training cost and the continued crash in LLM prices might hint that it’s not. But would you want to be the big tech executive that argued NOT to build out this infrastructure only to be proven wrong in a few years’ time? An interesting point of comparison here could be the way railways rolled out around the world in the 1800s. Constructing these required enormous investments and had a massive environmental impact, and many of the lines that were built turned out to be unnecessary—sometimes multiple lines from different companies serving the exact same routes! The resulting bubbles contributed to several financial crashes, see Wikipedia for Panic of 1873, Panic of 1893, Panic of 1901 and the UK’s Railway Mania. They left us with a lot of useful infrastructure and a great deal of bankruptcies and environmental damage. The year of slop 2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates: Watching in real time as “slop” becomes a term of art. the way that “spam” became the term for unwanted emails, “slop” is going in the dictionary as the term for unwanted AI generated content I expanded that definition a tiny bit to this: Slop describes AI-generated content that is both unrequested and unreviewed. I ended up getting quoted talking about slop in both the Guardian and the NY Times. Here’s what I said in the NY TImes: Society needs concise ways to talk about modern A.I. — both the positives and the negatives. ‘Ignore that email, it’s spam,’ and ‘Ignore that article, it’s slop,’ are both useful lessons. I love the term “slop” because it so succinctly captures one of the ways we should not be using generative AI! Slop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot. Synthetic training data works great An idea that surprisingly seems to have stuck in the public consciousness is that of “model collapse”. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data. The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise! That’s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content—deliberately creating artificial data to help steer their models in the right way. One of the best descriptions I’ve seen of this comes from the Phi-4 technical report, which included this: Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data. Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting",
          "entities": [
            "DeepSeek v3",
            "Meta",
            "Llama",
            "Claude 3.5 Sonnet",
            "Gemini 2.0",
            "OpenAI",
            "Google",
            "Amazon",
            "New York",
            "London"
          ],
          "themes": [
            "DeepSeek v3",
            "Large language models",
            "Training cost",
            "Environmental impact",
            "Infrastructure buildout",
            "AI-generated content",
            "Slop",
            "Synthetic training data",
            "Model collapse",
            "Energy efficiency"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015625,
        "overlapped_items": [
          [
            "Llama",
            "Llama 2"
          ]
        ]
      }
    },
    {
      "id": "a2b96276-20d0-47ce-9bb3-1441bba24d5a",
      "type": "entities_overlap",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.013888888888888888,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "a36ba7fe-bd98-411c-b4db-c8c5f2e2fb3a",
      "type": "entities_overlap",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.04938271604938271,
        "overlapped_items": [
          [
            "Llama 3.3",
            "Llama 3"
          ],
          [
            "ChatGPT",
            "ChatGPT"
          ],
          [
            "Claude",
            "Claude"
          ],
          [
            "GPT-4o",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "8e7b2f19-6e91-4452-b9cb-4e739f324416",
      "type": "entities_overlap",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015873015873015872,
        "overlapped_items": [
          [
            "Meta",
            "Meta"
          ]
        ]
      }
    },
    {
      "id": "9ace372f-9139-41c8-9551-b5edf7c0d361",
      "type": "entities_overlap",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03333333333333333,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ],
          [
            "Python",
            "Python"
          ],
          [
            "JavaScript",
            "JavaScript"
          ]
        ]
      }
    },
    {
      "id": "4cdf6d61-a288-4adb-aed6-12c3c08d2dbf",
      "type": "entities_overlap",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.012345679012345678,
        "overlapped_items": [
          [
            "GPT-4o",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "453b4635-b642-4d71-b0d0-7d692d623b23",
      "type": "entities_overlap",
      "source": {
        "id": "e6a55ba9-1009-41ed-86dd-edc1ce44e844",
        "properties": {
          "page_content": "reasoning patterns. Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives—a trick used by an increasing number of labs. DeepSeek v3 used “reasoning” data created by DeepSeek-R1. Meta’s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples. Careful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone. LLMs somehow got even harder to use A drum I’ve been banging for a while is that LLMs are power-user tools—they’re chainsaws disguised as kitchen knives. They look deceptively simple to use—how hard can it be to type messages to a chatbot?—but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls. If anything, this problem got worse in 2024. We’ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it’s accurately reflected in the undocumented and secret training set. The number of available systems has exploded. Different systems have different tools they can apply to your problems—like Python and JavaScript and web search and image generation and maybe even database lookups... so you’d better understand what those tools are, what they can do and how to tell if the LLM used them or not. Did you know ChatGPT has two entirely different ways of running Python now? Want to build a Claude Artifact that talks to an external API? You’d better understand CSP and CORS HTTP headers first. The models may have got more capable, but most of the limitations remained the same. OpenAI’s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it’s running in. o1 can’t run web searches or use Code Interpreter, but GPT-4o can—both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023). What are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out. Meanwhile, it’s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I’ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT—an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right. There’s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can’t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire! There is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads. Knowledge is incredibly unevenly distributed Most people have heard of ChatGPT by now. How many have heard of Claude? The knowledge gap between the people who actively follow this stuff and the 99% of the population who do not is vast. The pace of change doesn’t help either. In just the past month we’ve seen general availability of live interfaces where you can point your phone’s camera at something and talk about it with your voice... and optionally have it pretend to be Santa. Most self-certified nerds haven’t even tried that yet. Given the ongoing (and potential) impact on society that this technology has, I don’t think the size of this gap is healthy. I’d like to see a lot more effort put into improving this. LLMs need better criticism A lot of people absolutely hate this stuff. In some of the spaces I hang out (Mastodon, Bluesky, Lobste.rs, even Hacker News on occasion) even suggesting that “LLMs are useful” can be enough to kick off a huge fight. I get it. There are plenty of reasons to dislike this technology—the environmental impact, the (lack of) ethics of the training data, the lack of reliability, the negative applications, the potential impact on people’s jobs. LLMs absolutely warrant criticism. We need to be talking through these problems, finding ways to mitigate them and helping people learn how to use these tools responsibly in ways where the positive applications outweigh the negative. I like people who are skeptical of this stuff. The hype has been deafening for more than two years now, and there are enormous quantities of snake oil and misinformation out there. A lot of very bad decisions are being made based on that hype. Being critical is a virtue. If we want people with decision-making authority to make good decisions about how to apply these tools we first need to acknowledge that there ARE good applications, and then help explain how to put those into practice while avoiding the many unintiutive traps. (If you still don’t think there are any good applications at all I’m not sure why you made it to this point in the article!) I think telling people that this whole field is environmentally catastrophic plagiarism machines that constantly make things up is doing those people a disservice, no matter how much truth that represents. There is genuine value to be had here, but getting to that value is unintuitive and",
          "themes": [
            "Reasoning patterns",
            "Training data creation",
            "Large language models (LLMs)",
            "User experience with LLMs",
            "Capabilities and limitations of LLMs",
            "Knowledge gap in AI technology",
            "Criticism of LLMs",
            "Ethics and environmental impact",
            "Responsible use of AI tools",
            "Hype and misinformation in AI"
          ],
          "entities": [
            "DeepSeek v3",
            "DeepSeek-R1",
            "Meta",
            "Llama 3.3",
            "OpenAI",
            "ChatGPT",
            "Claude",
            "Python",
            "JavaScript",
            "GPT-4o"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.027777777777777776,
        "overlapped_items": [
          [
            "Llama 3.3",
            "Llama 2"
          ],
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "d39e7347-64c3-4f6a-87d8-6d101e3e7aa0",
      "type": "entities_overlap",
      "source": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.013888888888888888,
        "overlapped_items": [
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "1c35d6ef-fee6-4e03-a7af-0ef95bba9f84",
      "type": "entities_overlap",
      "source": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03571428571428571,
        "overlapped_items": [
          [
            "2024",
            "2024"
          ],
          [
            "2023",
            "2023"
          ]
        ]
      }
    },
    {
      "id": "854b90d2-cd1d-465c-98c2-fa383e8c09cf",
      "type": "entities_overlap",
      "source": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.0125,
        "overlapped_items": [
          [
            "LLMs",
            "LLMs"
          ]
        ]
      }
    },
    {
      "id": "94d77ab2-745d-4d45-91f4-477d3c1a9793",
      "type": "entities_overlap",
      "source": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.08333333333333333,
        "overlapped_items": [
          [
            "Simon Willison",
            "Simon Willison"
          ],
          [
            "Weblog",
            "Weblog"
          ],
          [
            "LLMs",
            "LLMs"
          ],
          [
            "Large Language Models",
            "Large Language Models"
          ],
          [
            "2023",
            "2023"
          ],
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "5e894c50-f6fc-4308-b5eb-1ba62d44fbac",
      "type": "entities_overlap",
      "source": {
        "id": "5711493a-eb2c-4601-a03b-f3129e609e4e",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Things we learned about LLMs in 2024 31st December 2024 A lot has happened in the world of Large Language Models over the course of 2024. Here’s a review of things we figured out about the field in the past twelve months, plus my attempt at identifying key themes and pivotal moments. This is a sequel to my review of 2023. In this article: The GPT-4 barrier was comprehensively broken Some of those GPT-4 models run on my laptop LLM prices crashed, thanks to competition and increased efficiency Multimodal vision is common, audio and video are starting to emerge Voice and live camera mode are science fiction come to life",
          "themes": [
            "Large Language Models",
            "GPT-4",
            "Technological advancements",
            "Price reduction",
            "Competition",
            "Efficiency",
            "Multimodal vision",
            "Audio and video integration",
            "Voice technology",
            "Live camera mode"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "LLMs",
            "2024",
            "31st December 2024",
            "Large Language Models",
            "2023",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.03125,
        "overlapped_items": [
          [
            "Simon Willison",
            "Simon Willison"
          ],
          [
            "LLMs",
            "LLMs"
          ]
        ]
      }
    },
    {
      "id": "b2700bee-c6f4-47f6-82a2-3202ece142c3",
      "type": "entities_overlap",
      "source": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.011111111111111112,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "9fc03d76-9569-43f1-a9ac-786fcb847264",
      "type": "entities_overlap",
      "source": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.024691358024691357,
        "overlapped_items": [
          [
            "Artificial Intelligence",
            "Artificial Intelligence"
          ],
          [
            "GPT-4",
            "GPT-4"
          ]
        ]
      }
    },
    {
      "id": "7f6d7a63-1380-4f2b-9827-c018f7198708",
      "type": "entities_overlap",
      "source": {
        "id": "07748857-c94f-4cc0-9b3f-87da7ea25f31",
        "properties": {
          "page_content": "needs guidance. Those of us who understand this stuff have a duty to help everyone else figure it out. Everything tagged “llms” on my blog in 2024 Because I undoubtedly missed a whole bunch of things, here’s every long-form post I wrote in 2024 that I tagged with llms: January 7th: It’s OK to call it Artificial Intelligence 9th: What I should have said about the term Artificial Intelligence 17th: Talking about Open Source LLMs on Oxide and Friends 26th: LLM 0.13: The annotated release notes February 21st: The killer app of Gemini Pro 1.5 is video March 5th: Prompt injection and jailbreaking are not the same thing 8th: The GPT-4 barrier has finally been broken 22nd: Claude and ChatGPT for ad-hoc sidequests 23rd: Building and testing C extensions for SQLite with ChatGPT Code Interpreter 26th: llm cmd undo last git commit—a new plugin for LLM April 8th: Building files-to-prompt entirely using Claude 3 Opus 10th: Three major LLM releases in 24 hours (plus weeknotes) 17th: AI for Data Journalism: demonstrating what we can do with this stuff right now 22nd: Options for accessing Llama 3 from the terminal using LLM May 8th: Slop is the new name for unwanted AI-generated content 15th: ChatGPT in “4o” mode is not running the new features yet 29th: Training is not the same as chatting: ChatGPT and other LLMs don’t remember everything you say June 6th: Accidental prompt injection against RAG applications 10th: Thoughts on the WWDC 2024 keynote on Apple Intelligence 17th: Language models on the command-line 21st: Building search-based RAG using Claude, Datasette and Val Town 27th: Open challenges for AI engineering July 14th: Imitation Intelligence, my keynote for PyCon US 2024 19th: Weeknotes: GPT-4o mini, LLM 0.15, sqlite-utils 3.37 and building a staging environment August 6th: Weeknotes: a staging environment, a Datasette alpha and a bunch of new LLMs 8th: django-http-debug, a new Django app mostly written by Claude 23rd: Claude’s API now supports CORS requests, enabling client-side applications 26th: Building a tool showing how Gemini Pro can return bounding boxes for objects in images September 6th: Calling LLMs from client-side JavaScript, converting PDFs to HTML + weeknotes 10th: Notes from my appearance on the Software Misadventures Podcast 12th: Notes on OpenAI’s new o1 chain-of-thought models 20th: Notes on using LLMs for code 29th: NotebookLM’s automatically generated podcasts are surprisingly effective 30th: Weeknotes: Three podcasts, two trips and a new plugin system October 1st: OpenAI DevDay 2024 live blog 2nd: OpenAI DevDay: Let’s build developer tools, not digital God 15th: ChatGPT will happily write you a thinly disguised horoscope 17th: Video scraping: extracting JSON data from a 35 second screen capture for less than 1/10th of a cent 18th: Experimenting with audio input and output for the OpenAI Chat Completion API 19th: Running Llama 3.2 Vision and Phi-3.5 Vision on a Mac with mistral.rs 21st: Everything I built with Claude Artifacts this week 22nd: Initial explorations of Anthropic’s new Computer Use capability 24th: Notes on the new Claude analysis JavaScript code execution tool 27th: Run a prompt to generate and execute jq programs using llm-jq 29th: You can now run prompts against images, audio and video in your terminal using LLM 30th: W̶e̶e̶k̶n̶o̶t̶e̶s̶ Monthnotes for October November 4th: Claude 3.5 Haiku 7th: Project: VERDAD—tracking misinformation in radio broadcasts using Gemini 1.5 12th: Qwen2.5-Coder-32B is an LLM that can code well that runs on my Mac 19th: Notes from Bing Chat—Our First Encounter With Manipulative AI 25th: Ask questions of SQLite databases and CSV/JSON files in your terminal December 4th: First impressions of the new Amazon Nova LLMs (via a new llm-bedrock plugin) 7th: Prompts.js 9th: I can now run a GPT-4 class model on my laptop 10th: ChatGPT Canvas can make API requests now, but it’s complicated 11th: Gemini 2.0 Flash: An outstanding multi-modal LLM with a sci-fi streaming mode 19th: Building Python tools with a one-shot prompt using uv run and Claude Projects 19th: Gemini 2.0 Flash “Thinking mode” 20th: December in LLMs has been a lot 20th: Live blog: the 12th day of OpenAI—“Early evals for OpenAI o3” 24th: Trying out QvQ—Qwen’s new visual reasoning model 31st: Things we learned about LLMs in 2024 (This list generated using Django SQL Dashboard with a SQL query written for me by Claude.) Posted 31st December 2024 at 6:07 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Things we learned about LLMs in 2024 by Simon Willison, posted on 31st December 2024. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. google 346 ai 1089 openai 253 generative-ai 934 llms 922 anthropic 114 gemini 55 meta 26 inference-scaling 28 long-context 10 Next: Ending a year long posting streak Previous: Trying out QvQ - Qwen's new visual reasoning model Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "Artificial Intelligence",
            "Open Source",
            "Prompt engineering",
            "AI-generated content",
            "AI for Data Journalism",
            "AI engineering challenges",
            "AI in software development",
            "AI in multimedia processing",
            "AI in misinformation tracking"
          ],
          "entities": [
            "Artificial Intelligence",
            "Open Source LLMs",
            "Gemini Pro",
            "GPT-4",
            "Claude",
            "ChatGPT",
            "Llama 3",
            "WWDC 2024",
            "PyCon US 2024",
            "OpenAI"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.027777777777777776,
        "overlapped_items": [
          [
            "ChatGPT",
            "ChatGPT"
          ],
          [
            "Llama 3",
            "Llama 2"
          ]
        ]
      }
    },
    {
      "id": "7eb2585c-a7b6-46ef-818c-d36bb0245596",
      "type": "entities_overlap",
      "source": {
        "id": "3f04ff99-852a-4707-8e57-b5e700bb3624",
        "properties": {
          "page_content": "Code may be the best application The ethics of this space remain diabolically complex My blog in 2023 Here’s the sequel to this post: Things we learned about LLMs in 2024. Large Language Models In the past 24-36 months, our species has discovered that you can take a GIANT corpus of text, run it through a pile of GPUs, and use it to create a fascinating new kind of software. LLMs can do a lot of things. They can answer questions, summarize documents, translate from one language to another, extract information and even write surprisingly competent code. They can also help you cheat at your homework, generate unlimited streams of fake content and be used for all manner of nefarious purposes. So far, I think they’re a net positive. I’ve used them on a personal level to improve my productivity (and entertain myself) in all sorts of different ways. I think people who learn how to use them effectively can gain a significant boost to their quality of life. A lot of people are yet to be sold on their value! Some think their negatives outweigh their positives, some think they are all hot air, and some even think they represent an existential threat to humanity. They’re actually quite easy to build The most surprising thing we’ve learned about LLMs this year is that they’re actually quite easy to build. Intuitively, one would expect that systems this powerful would take millions of lines of complex code. Instead, it turns out a few hundred lines of Python is genuinely enough to train a basic version! What matters most is the training data. You need a lot of data to make these things work, and the quantity and quality of the training data appears to be the most important factor in how good the resulting model is. If you can gather the right data, and afford to pay for the GPUs to train it, you can build an LLM. A year ago, the only organization that had released a generally useful LLM was OpenAI. We’ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations. The training cost (hardware and electricity) is still significant—initially millions of dollars, but that seems to have dropped to the tens of thousands already. Microsoft’s Phi-2 claims to have used “14 days on 96 A100 GPUs”, which works out at around $35,000 using current Lambda pricing. So training an LLM still isn’t something a hobbyist can afford, but it’s no longer the sole domain of the super-rich. I like to compare the difficulty of training an LLM to that of building a suspension bridge—not trivial, but hundreds of countries around the world have figured out how to do it. (Correction: Wikipedia’s Suspension bridges by country category lists 44 countries). You can run LLMs on your own devices In January of this year, I thought it would be years before I could run a useful LLM on my own computer. GPT-3 and 3.5 were pretty much the only games in town, and I thought that even if the model weights were available it would take a $10,000+ server to run them. Then in February, Meta released Llama. And a few weeks later in March, Georgi Gerganov released code that got it working on a MacBook. I wrote about how Large language models are having their Stable Diffusion moment, and with hindsight that was a very good call! This unleashed a whirlwind of innovation, which was accelerated further in July when Meta released Llama 2—an improved version which, crucially, included permission for commercial use. Today there are literally thousands of LLMs that can be run locally, on all manner of different devices. I run a bunch of them on my laptop. I run Mistral 7B (a surprisingly great model) on my iPhone. You can install several different apps to get your own, local, completely private LLM. My own LLM project provides a CLI tool for running an array of different models via plugins. You can even run them entirely in your browser using WebAssembly and the latest Chrome! Hobbyists can build their own fine-tuned models I said earlier that building an LLM was still out of reach of hobbyists. That may be true for training from scratch, but fine-tuning one of those models is another matter entirely. There’s now a fascinating ecosystem of people training their own models on top of these foundations, publishing those models, building fine-tuning datasets and sharing those too. The Hugging Face Open LLM Leaderboard is one place that tracks these. I can’t even attempt to count them, and any count would be out-of-date within a few hours. The best overall openly licensed LLM at any time is rarely a foundation model: instead, it’s whichever fine-tuned community model has most recently discovered the best combination of fine-tuning data. This is a huge advantage for open over closed models: the closed, hosted models don’t have thousands of researchers and hobbyists around the world collaborating and competing to improve them. We don’t yet know how to build GPT-4 Frustratingly, despite the enormous leaps ahead we’ve had this year, we are yet to see an alternative model that’s better than GPT-4. OpenAI released GPT-4 in March, though it later turned out we had a sneak peak of it in February when Microsoft used it as part of the new Bing. This may well change in the next few weeks: Google’s Gemini Ultra has big claims, but isn’t yet available for us to try out. The team behind Mistral are working to beat GPT-4 as well, and their track record is already extremely strong considering their first public model only came out in September, and they’ve released two significant improvements since then. Still, I’m surprised that no-one has beaten the now almost year old GPT-4 by now. OpenAI clearly have some substantial tricks that they haven’t shared yet. Vibes",
          "themes": [
            "Large Language Models (LLMs)",
            "Training data",
            "OpenAI",
            "Model training costs",
            "Local deployment of LLMs",
            "Fine-tuning models",
            "Open vs closed models",
            "GPT-4",
            "Innovation in AI",
            "Ethics of AI"
          ],
          "entities": [
            "2023",
            "2024",
            "OpenAI",
            "Anthropic",
            "Mistral",
            "Google",
            "Meta",
            "EleutherAI",
            "Stability AI",
            "Microsoft Research"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.015873015873015872,
        "overlapped_items": [
          [
            "2023",
            "2023"
          ]
        ]
      }
    },
    {
      "id": "a3daec8d-a520-4ccc-9edc-9e2dff60a78e",
      "type": "entities_overlap",
      "source": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.011111111111111112,
        "overlapped_items": [
          [
            "LLMs",
            "LLMs"
          ]
        ]
      }
    },
    {
      "id": "052c4f1a-d159-42df-b4e4-ab9de1344a43",
      "type": "entities_overlap",
      "source": {
        "id": "e11f4949-1a2f-4fd3-875a-52b591f512b7",
        "properties": {
          "page_content": "Based Development As a computer scientist and software engineer, LLMs are infuriating. Even the openly licensed ones are still the world’s most convoluted black boxes. We continue to have very little idea what they can do, how exactly they work and how best to control them. I’m used to programming where the computer does exactly what I tell it to do. Prompting an LLM is decidedly not that! The worst part is the challenge of evaluating them. There are plenty of benchmarks, but no benchmark is going to tell you if an LLM actually “feels” right when you try it for a given task. I find I have to work with an LLM for a few weeks in order to get a good intuition for it’s strengths and weaknesses. This greatly limits how many I can evaluate myself! The most frustrating thing for me is at the level of individual prompting. Sometimes I’ll tweak a prompt and capitalize some of the words in it, to emphasize that I really want it to OUTPUT VALID MARKDOWN or similar. Did capitalizing those words make a difference? I still don’t have a good methodology for figuring that out. We’re left with what’s effectively Vibes Based Development. It’s vibes all the way down. I’d love to see us move beyond vibes in 2024! LLMs are really smart, and also really, really dumb On the one hand, we keep on finding new things that LLMs can do that we didn’t expect—and that the people who trained the models didn’t expect either. That’s usually really fun! But on the other hand, the things you sometimes have to do to get the models to behave are often incredibly dumb. Does ChatGPT get lazy in December, because its hidden system prompt includes the current date and its training data shows that people provide less useful answers coming up to the holidays? The honest answer is “maybe”! No-one is entirely sure, but if you give it a different date its answers may skew slightly longer. Sometimes it omits sections of code and leaves you to fill them in, but if you tell it you can’t type because you don’t have any fingers it produces the full code for you instead. There are so many more examples like this. Offer it cash tips for better answers. Tell it your career depends on it. Give it positive reinforcement. It’s all so dumb, but it works! Gullibility is the biggest unsolved problem I coined the term prompt injection in September last year. 15 months later, I regret to say that we’re still no closer to a robust, dependable solution to this problem. I’ve written a ton about this already. Beyond that specific class of security vulnerabilities, I’ve started seeing this as a wider problem of gullibility. Language Models are gullible. They “believe” what we tell them—what’s in their training data, then what’s in the fine-tuning data, then what’s in the prompt. In order to be useful tools for us, we need them to believe what we feed them! But it turns out a lot of the things we want to build need them not to be gullible. Everyone wants an AI personal assistant. If you hired a real-world personal assistant who believed everything that anyone told them, you would quickly find that their ability to positively impact your life was severely limited. A lot of people are excited about AI agents—an infuriatingly vague term that seems to be converging on “AI systems that can go away and act on your behalf”. We’ve been talking about them all year, but I’ve seen few if any examples of them running in production, despite lots of exciting prototypes. I think this is because of gullibility. Can we solve this? Honestly, I’m beginning to suspect that you can’t fully solve gullibility without achieving AGI. So it may be quite a while before those agent dreams can really start to come true! Code may be the best application Over the course of the year, it’s become increasingly clear that writing code is one of the things LLMs are most capable of. If you think about what they do, this isn’t such a big surprise. The grammar rules of programming languages like Python and JavaScript are massively less complicated than the grammar of Chinese, Spanish or English. It’s still astonishing to me how effective they are though. One of the great weaknesses of LLMs is their tendency to hallucinate—to imagine things that don’t correspond to reality. You would expect this to be a particularly bad problem for code—if an LLM hallucinates a method that doesn’t exist, the code should be useless. Except... you can run generated code to see if it’s correct. And with patterns like ChatGPT Code Interpreter the LLM can execute the code itself, process the error message, then rewrite it and keep trying until it works! So hallucination is a much lesser problem for code generation than for anything else. If only we had the equivalent of Code Interpreter for fact-checking natural language! How should we feel about this as software engineers? On the one hand, this feels like a threat: who needs a programmer if ChatGPT can write code for you? On the other hand, as software engineers we are better placed to take advantage of this than anyone else. We’ve all been given weird coding interns—we can use our deep knowledge to prompt them to solve coding problems more effectively than anyone else can. The ethics of this space remain diabolically complex In September last year Andy Baio and I produced the first major story on the unlicensed training data behind Stable Diffusion. Since then, almost every major LLM (and most of the image generation models) have also been trained on unlicensed data. Just this week, the New York Times launched a landmark lawsuit against OpenAI and Microsoft over this issue. The 69 page PDF is genuinely worth reading—especially the first few pages, which lay out the issues in a way that’s surprisingly",
          "themes": [
            "LLMs as black boxes",
            "Prompting challenges",
            "Evaluation of LLMs",
            "Vibes Based Development",
            "LLM capabilities and limitations",
            "Gullibility in language models",
            "AI personal assistants",
            "Code generation by LLMs",
            "Hallucination in LLMs",
            "Ethics of training data"
          ],
          "entities": [
            "LLMs",
            "ChatGPT",
            "September",
            "AGI",
            "Python",
            "JavaScript",
            "Chinese",
            "Spanish",
            "English",
            "Andy Baio"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.025,
        "overlapped_items": [
          [
            "LLMs",
            "LLMs"
          ],
          [
            "ChatGPT",
            "ChatGPT"
          ]
        ]
      }
    },
    {
      "id": "2e4b07f6-505b-4de4-9b03-a35309b779e9",
      "type": "entities_overlap",
      "source": {
        "id": "6a06b3d3-6a2a-4449-b344-50082cfdcfb8",
        "properties": {
          "page_content": "Simon Willison’s Weblog Subscribe Stuff we figured out about AI in 2023 31st December 2023 2023 was the breakthrough year for Large Language Models (LLMs). I think it’s OK to call these AI—they’re the latest and (currently) most interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here’s my attempt to round up the highlights in one place! Large Language Models They’re actually quite easy to build You can run LLMs on your own devices Hobbyists can build their own fine-tuned models We don’t yet know how to build GPT-4 Vibes Based Development LLMs are really smart, and also really, really dumb Gullibility is the biggest unsolved problem",
          "themes": [
            "Large Language Models",
            "Artificial Intelligence",
            "Breakthrough year",
            "Academic development",
            "Building LLMs",
            "Running LLMs on personal devices",
            "Hobbyist model building",
            "GPT-4",
            "Vibes Based Development",
            "Gullibility problem"
          ],
          "entities": [
            "Simon Willison",
            "Weblog",
            "AI",
            "2023",
            "Large Language Models",
            "LLMs",
            "Artificial Intelligence",
            "1950s",
            "GPT-4"
          ]
        },
        "type": "chunk"
      },
      "target": {
        "id": "a187f6d5-b919-4d3b-adf9-43ff2218bfa5",
        "properties": {
          "page_content": "easy to follow. The rest of the document includes some of the clearest explanations of what LLMs are, how they work and how they are built that I’ve read anywhere. The legal arguments here are complex. I’m not a lawyer, but I don’t think this one will be easily decided. Whichever way it goes, I expect this case to have a profound impact on how this technology develops in the future. Law is not ethics. Is it OK to train models on people’s content without their permission, when those models will then be used in ways that compete with those people? As the quality of results produced by AI models has increased over the year, these questions have become even more pressing. The impact on human society in terms of these models is already huge, if difficult to objectively measure. People have certainly lost work to them—anecdotally, I’ve seen this for copywriters, artists and translators. There are a great deal of untold stories here. I’m hoping 2024 sees significant amounts of dedicated journalism on this topic. My blog in 2023 Here’s a tag cloud for content I posted to my blog in 2023 (generated using Django SQL Dashboard): The top five: ai (342), generativeai (300), llms (287), openai (86), chatgpt (78). I’ve written a lot about this stuff! I grabbed a screenshot of my Plausible analytics for the year, fed that to ChatGPT Vision, told it to extract the data into a table, then got it to mix in entry titles (from a SQL query it wrote) and produced this table with it. Here are my top entries this year by amount of traffic: Article Visitors Pageviews Bing: “I will not harm you unless you harm me first” 1.1M 1.3M Leaked Google document: “We Have No Moat, And Neither Does OpenAI” 132k 162k Large language models are having their Stable Diffusion moment 121k 150k Prompt injection: What’s the worst that can happen? 79.8k 95.9k Embeddings: What they are and why they matter 61.7k 79.3k Catching up on the weird world of LLMs 61.6k 85.9k llamafile is the new best way to run an LLM on your own computer 52k 66k Prompt injection explained, with video, slides, and a transcript 51k 61.9k AI-enhanced development makes me more ambitious with my projects 49.6k 60.1k Understanding GPT tokenizers 49.5k 61.1k Exploring GPTs: ChatGPT in a trench coat? 46.4k 58.5k Could you train a ChatGPT-beating model for $85,000 and run it in a browser? 40.5k 49.2k How to implement Q&A against your documentation with GPT3, embeddings and Datasette 37.3k 44.9k Lawyer cites fake cases invented by ChatGPT, judge is not amused 37.1k 47.4k Now add a walrus: Prompt engineering in DALL-E 3 32.8k 41.2k Web LLM runs the vicuna-7b Large Language Model entirely in your browser, and it’s very impressive 32.5k 38.2k ChatGPT can’t access the internet, even though it really looks like it can 30.5k 34.2k Stanford Alpaca, and the acceleration of on-device large language model development 29.7k 35.7k Run Llama 2 on your own Mac using LLM and Homebrew 27.9k 33.6k Midjourney 5.1 26.7k 33.4k Think of language models like ChatGPT as a “calculator for words” 25k 31.8k Multi-modal prompt injection image attacks against GPT-4V 23.7k 27.4k I also gave a bunch of talks and podcast appearances. I’ve started habitually turning my talks into annotated presentations—here are my best from 2023: Prompt injection explained, with video, slides, and a transcript Catching up on the weird world of LLMs Making Large Language Models work for you Open questions for AI engineering Embeddings: What they are and why they matter Financial sustainability for open source projects at GitHub Universe And in podcasts: What AI can do for you on the Theory of Change Working in public on Path to Citus Con LLMs break the internet on the Changelog Talking Large Language Models on Rooftop Ruby Thoughts on the OpenAI board situation on Newsroom Robots Industry’s Tardy Response to the AI Prompt Injection Vulnerability on RedMonk Conversations Posted 31st December 2023 at 11:59 pm · Follow me on Mastodon or Twitter or subscribe to my newsletter More recent articles Using pip to install a Large Language Model that's under 100MB - 7th February 2025 OpenAI o3-mini, now available in LLM - 31st January 2025 A selfish personal argument for releasing code as Open Source - 24th January 2025 This is Stuff we figured out about AI in 2023 by Simon Willison, posted on 31st December 2023. Part of series LLMs annual review Stuff we figured out about AI in 2023 - Dec. 31, 2023, 11:59 p.m. Things we learned about LLMs in 2024 - Dec. 31, 2024, 6:07 p.m. blogging 68 ai 1089 generative-ai 934 llms 922 Next: Tom Scott, and the formidable power of escalating streaks Previous: Last weeknotes of 2023 Colophon © 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 2025",
          "themes": [
            "Large Language Models (LLMs)",
            "AI ethics",
            "Legal implications of AI",
            "Impact of AI on employment",
            "Generative AI",
            "Prompt injection",
            "AI-enhanced development",
            "OpenAI",
            "AI journalism",
            "AI in 2023"
          ],
          "entities": [
            "LLMs",
            "AI",
            "ChatGPT",
            "OpenAI",
            "Google",
            "Stable Diffusion",
            "DALL-E 3",
            "Stanford Alpaca",
            "Llama 2",
            "Simon Willison"
          ]
        },
        "type": "chunk"
      },
      "bidirectional": false,
      "properties": {
        "entities_overlap_score": 0.041666666666666664,
        "overlapped_items": [
          [
            "Simon Willison",
            "Simon Willison"
          ],
          [
            "AI",
            "AI"
          ],
          [
            "LLMs",
            "LLMs"
          ]
        ]
      }
    }
  ]
}